\chapter{Performance Impact of Self-Hosting on a Meta-Tracing JIT}

	\label{chapter:problem}

	\begin{chaptersynopsis}
		\footnotesize
		Self-hosting on a meta-tracing JIT compiler introduces several performance issues.

		Sections:
		\begin{itemize}
			\item \ref{section:pycket-performance-characteristics} Pycket's Performance Characteristics

				Pycket is generally fast, but self-hosting exposes performance issues.
			\item \ref{section:module-and-language-loading} Module and Language Loading

				Self-hosting introduces a "boot and language expansion" overhead, which Pycket didn't have before.
			\item \ref{section:nature-of-the-beast} The Nature of the Beast: Tracing Data-Dependent Computation

				Data-dependent, interpreter-style, branch-heavy code produces overspecialized, non-reusable traces. Computations that are critical to self-hosting—such as program expansion—seem poorly suited to meta-tracing because they contain no hot loops for the tracer to capture.
			\item \ref{section:memory} Memory Issues with Self-Hosting

				Long continuation chains, and big and old heap objects cause GC pressure.
			\item \ref{section:cross-benchmarks} Performance of Fully Expanded Programs

				Pycket didn't lose too much fully-expanded-program performance by adding self-hosting.
		\end{itemize}
	\end{chaptersynopsis}

	\paragraph{}%
	 	Self-hosting introduces performance issues on a language runtime in several ways. Data-dependent, interpreter-style, branch-heavy code produces overspecialized, non-reusable traces; self-hosting raises memory-usage concerns; and computations that are critical to self-hosting—such as program expansion—seem to be poorly suited to meta-tracing because they contain no hot loops for the tracer to capture. Although the exact symptoms may vary with the design of a given runtime and the hosted language, the underlying issues remain the same on every meta-tracing system.

	\paragraph{}%
	 	In this chapter we study in detail and demonstrate the impact of these performance issues on a concrete system, namely \emph{Racket on Pycket}. Recall from \chapterRef{chapter:pycket} that the original Pycket front-end invoked the stand-alone Racket executable to load core libraries, as well as the -\verb|#lang|-language, and fully expand the user program before handing the expanded \verb|#%kernel| form to its CEK back-end. The new self-hosting front-end instead runs the expander entirely inside Pycket: it evaluates the bootstrapping linklets that implement the expander, uses them to expand the user program, and then evaluates the resulting core program—all on the CEK back-end from the outset. This architectural shift brings clear benefits, but it also introduces fresh costs.

	\paragraph{}%
		Note that, Pycket remains a tracing JIT compiler whose primary objective is to identify and trace hot loops in \emph{user} code. But with the new front-end, part of every \smartQL user program\smartQR is now the language program that expands the user code. After expansion, the fully expanded \verb|#%kernel| program is identical to what the Racket stand-alone binary would generate. Thus, the meta-traced CEK interpreter ultimately executes the same core program in both configurations—the difference lies only in where and how that program is obtained.

	\paragraph{}%
		In the remainder of this chapter, we proceed in the same order a program travels through the system. We first cover Pycket's baseline performance profile and review those RPython back-end optimizations most relevant to our study (\S\ref{section:pycket-performance-characteristics}). Then we examine how the runtime cooperates with Racket's module system to improve the loading of core libraries (\S\ref{section:module-and-language-loading}). Afterward, we analyze the cost of program expansion itself—a step now executed inside Pycket (\S\ref{section:nature-of-the-beast}). Finally, we measure how self-hosting affects the performance of the \emph{expanded} user program (\S\ref{section:cross-benchmarks}).

	\paragraph{}%
		In Chapter~\ref{chapter:solution}, we propose concrete remedies for each challenge we expose and, supported by preliminary evidence, argues that these directions warrant deeper investigation as a general strategy for improving the performance of self-hosting on meta-tracing systems.

	\section{Pycket's Performance Characteristics}
	\label{section:pycket-performance-characteristics}

	\begin{sectionpoint}
		Pycket is generally fast, but self-hosting exposes performance issues.
	\end{sectionpoint}

	\paragraph{}%
		With an extensive suite of micro- and macro-benchmarks plus larger real-world experiments, the existing Pycket implementation consistently demonstrates competitive performance when executing Racket code. It benefits from the generic optimizations provided by the RPython framework—including common-subexpression elimination, copy propagation, constant folding, loop-invariant code motion, malloc removal, and the inlining that naturally arises from tracing \cite{loop-aware:12,hotpath:06,malloc-removal:11}. It also benefits from interpreter-specific improvements such as environment pruning, data-structure specialization, strategy objects, and gradual-typing features like hidden classes. Nevertheless, earlier studies revealed workloads on which Pycket lags behind every other system—namely “almost exclusively recursive programs with data-dependent control flow in the style of an interpreter over an AST” \cite{pycketmain,pycketmain2}. Investigating these cases launched the present line of research.

	\paragraph{}%
		Pycket's baseline shows that when programs feature tight, well-behaved loops—numeric kernels, tail-recursive traversals, or higher-order combinators—the tracer quickly identifies the hot paths and specialization pays off. Across the Larceny and R6RS suites, for example, Pycket typically matches or exceeds Chez Scheme's throughput while starting up in a fraction of the time required by ahead-of-time native compilers; inside a trace, most primitive operations collapse to a handful of low-level nodes and even polymorphic arithmetic becomes monomorphic.

	\paragraph{}%
		The picture changes when control flow branches unpredictably or when large interpreter-like dispatch loops dominate execution. Here Pycket must juggle deep continuation structures, environment chains, and dynamically typed tag checks, all of which inflate trace size and lengthen warm-up. Garbage-collector telemetry (in \sectionRef{section:memory}) further shows that such workloads allocate an order of magnitude more temporary objects per unit time than steady-state numeric code, stressing the nursery and triggering full collections. These observations motivate the detailed analysis that follows.

	\paragraph{}%
		To develop the core issue we first recall from \ref{chapter:pycket} how Pycket currently detects loops. While for the low-level languages such as in a byte-code interpreter a program counter is used to detect loops, in Pycket the focus is on function applications, since Pycket works on program ASTs and the only AST that may create a loop is an application. Whether it is a basic counter or a compound state, the idea is that a loop is registered as soon as a program state transitions to one of the previous states. As reported in the previous studies, Pycket utilizes two techniques, namely the two-state tracking and the use of a dynamic call-graph. In both techniques the idea is to eliminate the “false-loops” among all the observed trace headers (i.e. potential start of a loop). The two-state tracking encodes the trace headers as a pair of a lambda body and its call-site, and the call-graph method detects cycles on a call-graph that's dynamically generated within the interpreter to handle extra levels of indirection. These approaches together are proven to be very effective in tracing code with a heavy use of shared control-flow indirections, such as the contract system \cite{pycketmain,pycketmain2}.

	\paragraph{}%
		A second crucial component is the relevant RPython back-end optimizations. For example, loop-invariant code motion (LICM) performed by the JIT \cite{loop-aware:12}. LICM hoists interpreter-state destructuring into a preamble, leaving a compact peeled iteration that forms the loop body. If a side trace (a \emph{bridge}) can jump directly into this peeled iteration, the runtime avoids re-executing hoisted operations and gains a substantial speed-up.

	\paragraph{}%
		Such a jump, however, is legal only when the program state—heap-allocated environment frames, continuation objects, virtualized temporaries, and range variables—matches exactly the state expected at the end of the destination trace's preamble. In Pycket a major part of the interpreter state consists of the environment and the continuation, which are all heap allocated objects. If the shape differs when entering a trace of a loop, for instance because a non-tail call inserted an extra frame to the continuation, the bridge must fall back to the preamble or to interpretation, negating any benefits from LICM, as well as tracing in general.

	\paragraph{}%
		Earlier versions of Pycket mitigated some of this mismatch by allowing the JIT to allocate just enough additional data to reconcile the states. Escape analysis and malloc-removal often virtualize those allocations \cite{malloc-removal:11,loop-aware:12}. A small heuristic therefore permits the JIT to materialize objects that would otherwise stay virtual, trading a bit of space for a jump into the peeled iteration; thus yielding roughly a 4 \% speed-up for gradually typed programs with no measurable overhead \cite{pycketmain2}.

	\paragraph{}%
		Any functionality that Pycket now imports as Racket code\footnote{i.e., bootstrapping linklets} would, in a handwritten RPython interpreter, exploit meta-interpreter hints-such as \verb|@jit.unroll_safe|-to guide the JIT back-end. Because the imported modules arrive as opaque ASTs, Pycket cannot inject such hints, so the tracer sees long interpreter-style dispatch loops without guidance. Moreover, introducing the linklet layer lets Pycket execute large Racket programs—including the 2642-function expander—entirely inside the JIT. Bootstrapping the \emph{racket/base} language alone instantiates more than a hundred modules before user code starts. Although micro-benchmarks are unaffected (the back-end is unchanged), these large workloads exacerbate all the aforementioned issues. In short, the very extensibility that makes self-hosting attractive also deprives the meta-tracer of inside information, limiting trace quality and prolonging warm-up.

	\section{Module and Language Loading}
	\label{section:module-and-language-loading}

		\begin{sectionpoint}
			Self-hosting introduces a "boot and language expansion" overhead, which Pycket didn't have before.
		\end{sectionpoint}

		\paragraph{}%
			A Racket runtime has to \emph{expand} the language of the user program—most prominently the language declared on the \#lang line—\emph{before} it can even begin to expand the user's source; the expander should be able to resolve every identifier that the program may reference, and doing so requires loading, and instantiating the language's own modules so that their bindings are available.


		\paragraph{}%
			Consequently, the complete transitive closure of the language's module graph has to be loaded and expanded ahead of the user program. For a language such as \texttt{\#lang racket/base} that closure already contains roughly 90 leaf modules; larger domain-specific languages may exceed that number. In the original, non-self-hosting Pycket this preparatory step was executed by the external \texttt{racket} binary, so its cost was invisible to the JIT. Now it is part of the measured runtime.

		\paragraph{}%
			\tableRef{table:boot-no-compiled} quantifies that cost of a cold boot when \emph{no} pre-compiled (\texttt{.zo}) artifacts are present, loading the \texttt{\#lang racket/base}. The loading of the bootstrapping linklets, their compilation into linklet objects and their instantiation, as well as instantiating all the Racket modules for the \texttt{\#lang racket/base} (\emph{init-lib}) together dominate the 2.7s wall-clock time. Note that the compilation of a linklet involves constructing AST nodes for every object that was in the s-expression, and the A-normalization and assignment conversion passes. Moreover, because these steps are internal to the interpreter and not part of the user code, these milliseconds are almost entirely spent in the interpreter.

		\begin{table}[ht]
		\centering
		\begin{tabular}{@{}lrrr@{}}
			\toprule
			Phase & Time (ms) & GC (ms) & Calls \\ \midrule
			boot                           &     16 &    0 & -- \\
			instantiate-linklet            &    868 &   33 & 7\,875 \\
			make-instance                  &     39 &    5 & 13\,405 \\
			startup                        &    156 &   53 & 1 \\
			\quad expander-linklet         &    135 &   53 & 1 \\
			\quad regexp-linklet                 &     37 &    8 & 1 \\
			\quad thread-linklet                 &     17 &    6 & 1 \\
			\quad pycket-boot-linklet            &      0 &    0 & 1 \\
			\quad fasl-linklet             &      2 &    0 & 1 \\
			\quad set-params               &     18 &    0 & 1 \\
			init-lib					   &	128408 &   8477 & 1 \\ \midrule
			compile                        &  1\,603 &  298 & -- \\
			\quad compile-linklet          &    106 &   38 & 680 \\
			\quad compile-sexp-to-ast      &    773 &  125 & 685 \\
			\quad compile-normalize        &    211 &   27 & 2\,026 \\
			\quad compile-assign-convert   &    512 &  108 & 5\,533 \\ \midrule
			\textbf{total}                 &  2\,740 &  403 & -- \\
			\bottomrule
		\end{tabular}
		\caption{Boot without using any compiled code}
		\label{table:boot-no-compiled}
		\end{table}

		\paragraph{}%
			Before self-hosting, none of that overhead appeared: Pycket received a fully-expanded core program, and the JIT could concentrate on the hot loops of \emph{user} code. The new architecture gives Pycket full visibility into the macro expander, the module system, and the reader, and more, but that transparency comes with an up-front cost.

		\paragraph{}%
			One obvious advantage of this transparency is to reuse compiled code. By setting \racketcode{use-compiled-file-paths} and \racketcode{read-accept-compiled} (Racket-level) parameters, Pycket instructs the expander to prefer \texttt{.zo} files over \texttt{.rkt} sources. Doing so eliminates the most expensive compile-time passes (cf.\ the disappearance of \texttt{compile-linklet}, \texttt{assign-convert}, and \texttt{normalize} in \tableRef{table:boot-compiled}), yet it also shifts work: module bodies must be deserialized (\racketcode{fasl->s-exp}) and validated, and the resulting syntax objects still have to be parsed into ASTs suitable for Pycket's evaluator.

		\begin{table}[ht]
		\centering
		\begin{tabular}{@{}lrrr@{}}
			\toprule
			Phase & Time (ms) & GC (ms) & Calls \\ \midrule
			boot                           &     15 &    0 & -- \\
			instantiate-linklet            &    903 &   40 & 684 \\
			make-instance                  &      2 &    0 & 845 \\
			startup                        &    129 &   38 & 1 \\
			\quad expander-linklet         &    109 &   38 & 1 \\
			\quad regexp-linklet                 &     20 &    4 & 1 \\
			\quad thread-linklet                 &     12 &    4 & 1 \\
			\quad pycket-boot-linklet            &      0 &    0 & 1 \\
			\quad fasl-linklet             &      1 &    0 & 1 \\
			\quad set-params               &     18 &    0 & 1 \\
			init-lib					   &	3387 &   361 & 1 \\ \midrule
			read                           &    146 &   29 & -- \\
			\quad fasl$\rightarrow$s-exp   &    144 &   29 & 74 \\
			\quad s-exp$\rightarrow$ast    &      2 &    0 & 74 \\
			\quad assign-convert-deser.    &      0 &    0 & -- \\ \midrule
			compile                        &  1\,439 &  313 & -- \\
			\quad compile-linklet          &      0 &    0 & 9 \\
			\quad compile-sexp-to-ast      &    836 &  128 & 764 \\
			\quad compile-normalize        &      0 &    0 & 28 \\
			\quad compile-assign-convert   &    603 &  185 & 11\,763 \\ \midrule
			\textbf{total}                 &  2\,673 &  428 & -- \\
			\bottomrule
		\end{tabular}
		\caption{Boot using compiled code}
		\label{table:boot-compiled}
		\end{table}

		\paragraph{}%
			As expected, using pre-compiled \texttt{.zo} artifacts for Racket modules makes the \emph{compiled-code} path dramatically faster: the \texttt{init-lib} phase's wall-clock time falls from about 127s when loading source to roughly 3.4s with compiled code, while avoiding compilation saves large heap allocations, therefore the GC time drops from 8.5s to a few hundred milliseconds.

		\paragraph{}%
			Regardless of the source of each module, every macro expansion of user code ultimately depends on the performance of the underlying expander. Pycket's ability to inline into, and sometimes across, expander helpers opens new optimization opportunities, but it also means that deficiencies in the expander's own performance now directly affect application start-up. The next sections discuss those interactions in detail.

	\section{The Nature of the Beast: Tracing Data-Dependent Computation}
	\label{section:nature-of-the-beast}

		\begin{sectionpoint}
			Data-dependent, interpreter-style, branch-heavy code produces overspecialized, non-reusable traces.

			Computations that are critical to self-hosting—such as program expansion—seem poorly suited to meta-tracing because they contain no hot loops for the tracer to capture.
		\end{sectionpoint}

		\paragraph{}%
			The \emph{Racket macro expander}, loaded inside Pycket as the \emph{expander linklet}, now runs in the same process as every user program; its performance therefore lies on the critical path, because (i) expansion itself contributes directly to end-to-end latency, as detailed in \chapterRef{chapter:pycket}, and (ii) the entire language hierarchy must be loaded and initialized first, as shown in \sectionRef{section:module-and-language-loading}.

		\paragraph{}%
			Conceptually the expander is a tree-walking interpreter from higher level Racket languages to the \racketcode{\#\%kerrnel} language. Its control flow walks a rich lattice of macros and compile-time bindings, yielding deeply branch-heavy control flow whose exact shape depends on the user's program and the libraries it imports. [CITE] Pycket's meta-tracing JIT therefore meets high-level Racket functions whose behavior cannot be trivially predicted at trace time; unlike the low-level CEK interpreter these helpers cannot be decorated with hints such as \texttt{@jit.unroll\_safe}, and their extensive allocation and branching patterns frustrate the usual trace-specialization heuristics. Therefore the traces generated for these high-level abstractions are substantially larger; the JIT spends significant time tracing and optimizing these paths only to encounter unpredictable branches, bail out, and fall back to interpretation—negating much of the anticipated speed-up.

		\paragraph{}%
			In interpreter-style, data-dependent recursion the control flow fans out across many branches driven by the program's input, yet a tracing JIT captures only the \emph{single} linear path taken while tracing and specializes the resulting code to that exact sequence of decisions. When new input steers execution down a different branch the JIT must abort, start another trace, and specialize again. Over time this produces a combinatorial explosion of narrow, redundant traces—each optimal for the data that birthed it but useless for most other paths—inflating compilation time, thrashing the trace cache, and frequently forcing the runtime to fall back on interpretation.

		\paragraph{}%
			In \chapterRef{chapter:solution} we propose a lightweight meta-hint mechanism that aims to help with this by helping to detect hot branches during interpretation and guiding the tracer away from branch-heavy code paths, mitigating the trace compilation churn and the explosion of overspecialized traces. In the remainder of this section, we examine in detail how the meta-tracer in Pycket behaves on branch-heavy computations.

		\subsection{Tracing Branch-Heavy Computation}
			\begin{mainpoint}
				Data-dependent, interpreter-style, branch-heavy code produces overspecialized, non-reusable traces.
			\end{mainpoint}
				\label{section:branchy}

			\paragraph{}%
				As detailed in \chapterRef{chapter:pycket}, the Racket expander--containing the macro expander, module system, and more--is notably large and intricate. Investigating tracing JIT behavior directly within such a sizable and complex program can be impractical. Thus, we analyze the tracing behavior using smaller, controlled examples to isolate and clearly observe specific issues.\footnote{Full code and alternative definitions of Branchy is available at \url{https://github.com/cderici/pycket-performance/blob/master/branchy/private/branchy.rkt}.}

			\inputSub{problem}{branchy-source}

			\paragraph{}%
				\figref{fig:branchy-introduce-code} introduces \emph{Branchy}, a synthetic benchmark intentionally designed to exhibit interpreter-style, branch-heavy, and data-dependent behavior. Each iteration processes an element from an input list through nested conditional branches resembling a decision tree. Certain inputs produce consistently deep paths through this branching structure, while others immediately exit via shallow paths. For instance, an input list such as \racketcode{'(0 0 0 ...)} repeatedly traverses the longer path sequence \emph{18-8-3-1}, whereas \racketcode{'(20 20 20 ...)} consistently follows the shorter path labeled \emph{18}.

			\paragraph{}%
				It is important to note that describing static source code itself as "branch-heavy" can be misleading. The critical notion here is the data-dependent nature of branch decisions during runtime. A program could possess deeply nested conditional structures yet exhibit minimal runtime branching if the input consistently chooses shallow branches. Therefore, the "branchiness" of a computation is an empirical property, characterized by the frequency and complexity of branching decisions made during an actual evaluation over an input.

			\paragraph{}%
				This data-dependency manifests explicitly through the guards placed by a tracing JIT. During tracing, each conditional encountered generates a corresponding guard, asserting assumptions made about the input data at trace time. Consequently, more complex branching decisions lead directly to more numerous and more specialized guards. A trace containing multiple guards becomes highly specific to the particular input data observed during tracing, thereby limiting its reusability.

			\begin{figure}[h] % let LaTeX decide where it fits best
				\centering
				\includegraphics[width=0.5\linewidth]{\inputFigPath{problem}{branchy-long-path-trace-guards.png}}
				\caption{RPython trace for Branchy running all-zero input taking a long 18-8-3-1 path.}
				\label{fig:branchy-long-path-trace}
			\end{figure}

			\paragraph{}%
				\figref{fig:branchy-long-path-trace} presents an excerpt from a trace of Branchy executed with an all-zero input, repeatedly following the deep path sequence \emph{18-8-3-1}. Each decision along this path generates a guard that must hold true at runtime; failure of any guard causes the trace to exit prematurely and fallback to the interpreter or trigger another trace. The resulting trace, while optimized and compiled, is excessively specialized, limiting its utility exclusively to identical inputs.

			\paragraph{}%
				Thus, the internal structure of the input data significantly influences tracing outcomes. Repetitive input patterns, such as \racketcode{'(3 3 3 3 ...)}, allow trace reuse, thereby amortizing the overhead of tracing and compiling. However, highly repetitive inputs like these are uncommon in realistic workloads. Conversely, more varied or random inputs, such as \racketcode{'(1 3 5 1 3 5 ...)}, significantly reduce trace reuse. This phenomenon of internal "loops" within the input data will be revisited in greater detail in \sectionRef{section:there-is-no-loop} when we discuss the nature of loops the tracer attempts to capture.

			\begin{figure}[h] % let LaTeX decide where it fits best
				\centering
				\includegraphics[width=0.7\linewidth]{\inputFigPath{problem}{branchy-trace-graphs.png}}
				\caption{Tracing branch-heavy computation with arbitrary branches results in a combinatorial explosion of recorded loops and bridges.}
				\label{fig:branchy-trace-graphs}
			\end{figure}

			\paragraph{}%
				\figref{fig:branchy-trace-graphs} and \tableRef{table:branchy-backend-summaries} illustrate this effect quantitatively. For inputs with highly repetitive branching decisions (e.g., all 20s), the meta-tracer records only two loops and three bridges, requiring a negligible fraction of runtime spent on trace compilation. However, even these traces remain highly specialized to the particular repetitive path taken, significantly limiting their reusability in more general cases where input variation occurs. In contrast, random inputs, more representative of realistic macro expander behavior, induce a combinatorial explosion: 39 loops, 248 bridges, and substantially more traced operations, resulting in significant overhead in trace compilation and optimization.

			\begin{table}[!h]
			\centering
			\small
			\begin{tabular}{@{}lcc@{}}
				\toprule
				& \textbf{Repeating Branches} & \textbf{Random Branches} \\
				\midrule
				\textbf{Tracing (s)}                 & \textbf{0.004566} & \textbf{0.153118} \\
				Backend (s)                 & 0.001239 & 0.044965 \\
				TOTAL (s)                            & 1.185808 & 2.242329 \\ \\
				ops                                   & 2130 & 147895 \\
				heapcached ops                        & 572 & 41615 \\
				\textbf{recorded ops}                & \textbf{462} & \textbf{31202} \\
				\quad calls                                 & 12 & 380 \\
				guards                                & 128 & 7391 \\
				opt ops                               & 243 & 11448 \\
				opt guards                            & 75 & 3768 \\
				opt guards shared                     & 46 & 2553 \\
				forcings                              & 0 & 0 \\
				abort: trace too long                 & 0 & 0 \\
				abort: compiling                      & 0 & 0 \\
				abort: vable escape                   & 0 & 0 \\
				abort: bad loop                       & 0 & 0 \\
				abort: force quasi-immut              & 0 & 0 \\
				abort: segmenting trace               & 0 & 0 \\
				virtualizables forced                 & 0 & 0 \\
				nvirtuals                    & 74 & 1982 \\
				nvholes                               & 8 & 126 \\
				nvreused                              & 36 & 907 \\
				vecopt tried                          & 0 & 0 \\
				vecopt success                        & 0 & 0 \\
				\textbf{Total \# loops}               & \textbf{2} & \textbf{39} \\
				\textbf{Total \# bridges}             & \textbf{3} & \textbf{248} \\
				Freed \# loops                        & 0 & 0 \\
				Freed \# bridges                      & 0 & 0 \\
				\bottomrule
			\end{tabular}
			\caption{JIT back-end summaries for branch-heavy computations taking repeating vs random branches.}
			\label{table:branchy-backend-summaries}
			\end{table}


			\paragraph{}%
				This observation naturally raises the question of what constitutes an ideal tracing strategy for interpreter-style, branch-heavy computations. Should a tracing JIT generate one highly specialized trace per unique branching pattern, incurring substantial memory and compilation overhead but achieving maximum runtime efficiency for those exact inputs? Or is it preferable to construct broader, less specialized traces containing multiple branches, potentially reducing memory footprint but introducing runtime overhead from unnecessary paths? Consider a repeated branching pattern such as \emph{18-8-3-5-4-18-8-3-5-4-...}; capturing the entire pattern into one trace may yield high specialization, whereas identifying a shorter common prefix (e.g., \emph{18-8-3-5}) might increase trace reuse at the cost of less specialization. However, for our specific scenario regarding self-hosting, neither tracing strategy sufficiently justifies the resources expended by meta-tracing during runtime. As we will elaborate in the next section, this inefficiency arises because program expansion occurs at compile-time, where the control flow lacks meaningful loops to capture—each computation path is effectively static and resembles the linear decision-making of a regular-expression matcher.


			\paragraph{}%
				This fundamental tension explains the limited adoption of tracing JITs in practice in the last decade [CITE HERE]. Highly specialized traces yield considerable speed-ups only for workloads closely matching traced paths, while deviating workloads incur substantial tracing overhead without corresponding benefits. Consequently, the average-case performance may significantly degrade, exemplified by the observation that a tracing JIT that is ten times faster on half of a benchmark suite but ten times slower on the other half is, overall, more than five times slower in practice~\cite{mozblog}.

		\subsection{Performance of Program Expansion}
			\label{section:there-is-no-loop}
			% regexp stuff

			\begin{sectionpoint}
				Computations that are critical to self-hosting—such as program expansion—seem poorly suited to meta-tracing because they contain no hot loops for the tracer to capture. There is no loop.
			\end{sectionpoint}

			\begin{paragraph-here}
				Moving on to a more real-world program: regexp. Getting deeper into the nature of the problem itself.
			\end{paragraph-here}

			\begin{paragraph-here}
				Working on regexp is great for us, extrapolating which reveals what happens in the expander at larger scale. Because It's an interpreter-style data-driven branch-heavy code.
			\end{paragraph-here}

			\begin{paragraph-here}
				Pycket already has an efficient regexp implementation in RPython to compare against, implementing the same thing with what we have in the regexp linklet, so 1:1 comparison yay.

				Even regexp linklet is very complex to distill the nature of the issue concretely.
			\end{paragraph-here}

			\begin{paragraph-here}
				As a running example, we can take a look at the performance differences between the regexp linklet and the RPython implementation over a simple example of matching $\mathtt{\#}$\racketcode{rx"defg"} against a large string containing \racketcode{"aaa...defg...aaa"} in the middle.
			\end{paragraph-here}

			\inputSub{problem}{regexp-overall-runtime}

			\begin{paragraph-here}
				Introduce a synthetic regexp implementation (Figure 17 in proposal)
			\end{paragraph-here}

			\inputSub{problem}{reg-match-figure}

			\paragraph{}%
			 As a simple experiment
			to demonstrate the issue, we run both regexp implementations (the one
			in the \figref{fig:regexp} and the RPython implementation) to match
			 and measure the time
			performances.


			\figref{fig:regexp-trace} shows both the trace we get
			from the RPython implementation and the program we run. To run the
			simple implementation in \figref{fig:regexp}, we modify this program
			to use the \racketcode{reg-match} in instead of the
			\racketcode{regexp-match}. When we run both with the same inputs, we
			observe 2x slowdown on the simple implementation. Part of the reason
			is the use of clever techniques that are specific to the regexp
			implementation on RPython such as caching and using contexts etc. The
			bigger part of the problem, however, lies in the difference between
			the traces that are generated for this computation.

			\paragraph{}%
			The gist of this computation is the literal search in the string for
			the \racketcode{"defg"} pattern. As can be seen in
			\figref{fig:regexp-trace}, this is captured in a tight loop in the
			main trace for the RPython implementation. The RPython regexp
			implementation utilizes interpreter hints to control the unrolling and
			encourages the allocation removal optimizations through type
			specializations. On the other hand, tThe trace we get for the Racket
			implementation for the same program is quite large due to aggressive
			inlining comes with tracing and includes large amounts of
			allocation/deallocation. For space concerns in this document we defer
			presenting the large traces to the dissertation.

			\paragraph{}%
			Tracing interpreter-style programs with complex control-flow paths is
			a known weakness of JIT compilers. The large number of indirections
			not only cripple the JIT optimizations but also causes the loops to be
			segmented into many highly data driven traces. For
			example, consider the following program in \figref{fig:regexp}, which
			is a very simple regular expression matcher. It is highly simplified
			and some of the rules are removed for space.

			\paragraph{}%
			Tracing this program running with an input regexp, say
			$\mathtt{\#}$\racketcode{rx"defg"}, trying to match it against an
			input string produces a trace that follows the control-flow path of
			the program for that input, making tracing quite wasteful because for
			any other input regexp, say $\mathtt{\#}$\racketcode{rx"a*"} which
			follows an entirely different path on the program, the JIT produces,
			compiles and optimizes yet another trace for that input, unable to use
			the previously generated trace. This problem not only increases the
			warmup time but also produces traces that are unlikely to be
			frequently re-used.

			\begin{show-experiment}
				Similar experiments with some data making it go super fast and some super slow.
			\end{show-experiment}

			\begin{paragraph-here}
				Interleaving paragraphs to talk about various experiments with regexp, showing traces.
			\end{paragraph-here}

			\begin{paragraph-here}
				Maybe a paragraph to tie up this discussion with relating to the real Racket expander.
			\end{paragraph-here}

			(We move here into the very nature of the self-hosting on a meta-tracing JIT)

			\begin{paragraph-here}
				Let's recall what problem meta-tracing solved.
			\end{paragraph-here}

			\begin{paragraph-here}
				Self-hosting is a different problem from what meta-tracing solves. (Find figure I made before-or make it again).
			\end{paragraph-here}

			\begin{paragraph-here}
				Tie it up with data dependency and punch it: this is not suitable for meta-tracing.
			\end{paragraph-here}

			\begin{paragraph-here}
				This problem requires a significant amount of research to solve.
			\end{paragraph-here}


	\section{Memory Issues with Self-Hosting}
	\label{section:memory}

		\begin{mainpoint}
			Long continuation chains, and big and old heap objects cause GC pressure.
		\end{mainpoint}

		\begin{paragraph-here}
			user program is effected by everything before including expansion.
		\end{paragraph-here}

		\begin{paragraph-here}
			RPython GC minimark assumes short-lived small objects.
		\end{paragraph-here}

		\begin{show-experiment}
				Demonstrate the issues concretely, nursery overflows/promotions, long-lived old objects, GC pause time eaten by majors, real-world cost
				Maybe also deeper insights, like "`W\_StructDescriptor` accounts for 68\% of promoted bytes".
		\end{show-experiment}

		\begin{paragraph-here}
			But Racket's objects are gigantic and long-lived.	structs, parameters
		\end{paragraph-here}

		\begin{paragraph-here}
			Various paragraphs discussing experiments.
		\end{paragraph-here}

	\section{Performance of Fully Expanded Programs}
	\label{section:cross-benchmarks}

		\begin{mainpoint}
				Pycket didn't lose too much fully-expanded-program performance by adding self-hosting.
		\end{mainpoint}

		\begin{paragraph-here}
			We're not talking about Pycket vs Racket, we're talking about Pycket with \& without self-hosting.
		\end{paragraph-here}

		\begin{paragraph-here}
			There’s no reason Pycket with self-hosting should be faster than the original Pycket.

			Both OP/NP using the same expander generates same kernel code.
		\end{paragraph-here}

		\begin{show-experiment}
			CrossBenchmark experiments.
		\end{show-experiment}

		\begin{paragraph-here}
			There are rare cases where self-hosting Pycket is faster.
		\end{paragraph-here}

		\begin{paragraph-here}
			 Often we're slower, for reasons.
		\end{paragraph-here}

		\begin{paragraph-here}
			Warmup is worse
		\end{paragraph-here}

		\begin{show-experiment}
			Warmup breakdowns
		\end{show-experiment}

		\inputSub{problem}{sample-trace}

	% Runtime Performance of Tracing Data-Dependent Branchy Code


