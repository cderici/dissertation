\chapter[\texorpdfstring{PERFORMANCE EVALUATION OF SELF-HOSTING ON META-TRACING}
                          {6. Performance Evaluation}]{PERFORMANCE EVALUATION OF SELF-HOSTING ON META-TRACING}

	\label{chapter:problem}

	\begin{chaptersynopsis}[Chapter Synopsis - \emph{Chapter Content: 70\%}]
		\footnotesize
		Self-hosting on a meta-tracing JIT compiler introduces several performance issues.

		Sections:
		\begin{itemize}
			\item \ref{section:pycket-performance-characteristics} Pycket's Performance Characteristics

				Pycket is generally fast, but self-hosting exposes performance issues.
			\item \ref{section:module-and-language-loading} Module and Language Loading

				Self-hosting introduces a "boot and language expansion" overhead, which Pycket didn't have before.
			\item \ref{section:cross-benchmarks} Cost of Self-hosting on Back-end \& Front-end

				Pycket's back-end performance for fully-expanded code is nearly the same after adding self-hosting (i.e. by including expansion time to the run-time). However, the front-end got a big hit, the evaluation of the expander on the meta-tracing JIT needs work for self-hosting to be fully practical.
			\item \ref{section:nature-of-the-beast} The Nature of the Beast: Tracing Data-Dependent Computation

				Data-dependent, interpreter-style, branch-heavy code produces overspecialized, non-reusable traces. Computations that are critical to self-hosting—such as program expansion—seem poorly suited to meta-tracing because they contain no hot loops for the tracer to capture.
			\item \ref{section:memory} Memory Issues with Self-Hosting

				Long continuation chains, and big and old heap objects cause GC pressure.
		\end{itemize}
	\end{chaptersynopsis}

	\paragraph{}%
	 	Self-hosting introduces performance issues on a language runtime in several ways. Data-dependent, interpreter-style, branch-heavy code produces overspecialized, non-reusable traces; self-hosting raises memory-usage concerns; and computations that are critical to self-hosting —such as program expansion—seem to be poorly suited to meta-tracing because they contain no hot loops for the tracer to capture. Although the exact symptoms may vary with the design of a given runtime and the hosted language, the underlying issues remain the same on every meta-tracing system.

	\paragraph{}%
	 	In this chapter we study in detail and demonstrate the impact of these performance issues on a concrete system, namely \emph{Racket on Pycket}. Recall from \chapterRef{chapter:pycket} that the original Pycket front-end invoked the stand-alone Racket executable to load core libraries, as well as the -\verb|#lang|-language, and fully expand the user program before handing the expanded \verb|#%kernel| form to its CEK back-end. The new self-hosting front-end instead runs the expander entirely inside Pycket: it evaluates the bootstrapping linklets that implement the expander, uses them to expand the user program, and then evaluates the resulting core program—all on the CEK back-end from the outset. This architectural shift brings clear benefits, but it also introduces fresh costs.

	\paragraph{}%
		Note that, Pycket remains a tracing JIT compiler whose primary objective is to identify and trace hot loops in \emph{user} code. But with the new front-end, part of every \smartQL user program\smartQR is now the language program that expands the user code. After expansion, the fully expanded \verb|#%kernel| program is identical to what the Racket stand-alone binary would generate. Thus, the meta-traced CEK interpreter ultimately executes the same core program in both configurations—the difference lies only in where and how that program is obtained.

	\paragraph{}%
		In the remainder of this chapter, we proceed in a similar order a user program travels through the system. We first cover Pycket's baseline performance profile and review those RPython back-end optimizations most relevant to our study (\S\ref{section:pycket-performance-characteristics}). Then we examine how the runtime cooperates with Racket's module system to improve the loading of core libraries (\S\ref{section:module-and-language-loading}). Afterward, we analyze the cost of program expansion itself—a step now executed inside Pycket (\S\ref{section:nature-of-the-beast}). We also study the overall memory overhead induced by self-hosting (\S\ref{section:memory}). Finally, we measure how self-hosting affects the performance of a \emph{fully-expanded} user program (\S\ref{section:cross-benchmarks}).

	\paragraph{}%
		In Chapter~\ref{chapter:solution}, we propose concrete remedies for each challenge we expose and, supported by preliminary evidence, argues that these directions warrant deeper investigation as a general strategy for improving the performance of self-hosting on meta-tracing systems.

	\section[\texorpdfstring{Pycket's Performance Characteristics}{Pycket's Performance}]{Pycket's Performance Characteristics}
	\label{section:pycket-performance-characteristics}

	% \begin{sectionpoint}
	% 	Pycket is generally fast, but self-hosting exposes performance issues.
	% \end{sectionpoint}

	\paragraph{}%
		With an extensive suite of micro- and macro-benchmarks plus larger real-world experiments, the existing Pycket implementation consistently demonstrates competitive performance when executing Racket code. It benefits from the generic optimizations provided by the RPython framework—including common-subexpression elimination, copy propagation, constant folding, loop-invariant code motion, malloc removal, and the inlining that naturally arises from tracing \cite{loop-aware:12,hotpath:06,malloc-removal:11}. It also benefits from interpreter-specific improvements such as environment pruning, data-structure specialization, strategy objects, and more dynamic features like hidden classes. Nevertheless, earlier studies revealed workloads on which Pycket lags behind every other system—namely “almost exclusively recursive programs with data-dependent control flow in the style of an interpreter over an AST” \cite{pycketmain,pycketmain2}. Investigating these cases launched the present line of research.

	\paragraph{}%
		Pycket's baseline shows that when programs feature tight, well-behaved loops—numeric kernels, tail-recursive traversals, or higher-order combinators—the tracer quickly identifies the hot paths and specialization pays off. Across the Larceny and R6RS suites, for example, Pycket typically matches or exceeds Chez Scheme's throughput while starting up in a fraction of the time required by ahead-of-time native compilers; inside a trace, most primitive operations collapse to a handful of low-level nodes and even polymorphic arithmetic becomes monomorphic \cite{pycketmain2}.

	\paragraph{}%
		The picture changes when control flow branches unpredictably or when large interpreter-like dispatch loops dominate execution. Here Pycket must juggle deep continuation structures, environment chains, and dynamically typed tag checks, all of which inflate trace size and lengthen warm-up as well. Garbage-collector telemetry (in \sectionRef{section:memory}) further shows that such workloads allocate an order of magnitude more temporary objects per unit time than steady-state numeric code, stressing the nursery and triggering full collections. These observations motivate the detailed analysis that follows.

	\paragraph{}%
		To develop the core issue we first recall from \chapterRef{chapter:pycket} how Pycket currently detects loops. While for the low-level languages such as in a byte-code interpreter a program counter is used to detect loops, in Pycket the focus is on function applications, since Pycket works on program ASTs and the only AST that may create a loop is an application. Whether it is a basic counter or a compound state, the idea is that a loop is registered as soon as a program state transitions to one of the previous states. As reported in the previous studies, Pycket utilizes two techniques, namely the two-state tracking and the use of a dynamic call-graph. In both techniques the idea is to eliminate the “false-loops” among all the observed trace headers (i.e. potential start of a loop). The two-state tracking encodes the trace headers as a pair of a lambda body and its call-site, and the call-graph method detects cycles on a call-graph that's dynamically generated within the interpreter to handle extra levels of indirection. These approaches together are proven to be very effective in tracing code with a heavy use of shared control-flow indirections, such as the contract system \cite{pycketmain,pycketmain2}.

	\paragraph{}%
		A second crucial component is the relevant RPython back-end optimizations. For example, loop-invariant code motion (LICM) performed by the JIT \cite{loop-aware:12}. LICM hoists interpreter-state destructuring into a preamble, leaving a compact peeled iteration that forms the loop body. If a side trace (a \emph{bridge}) can jump directly into this peeled iteration, the runtime avoids re-executing hoisted operations and gains a substantial speed-up.

	\paragraph{}%
		Such a jump, however, is legal only when the program state—heap-allocated environment frames, continuation objects, virtualized temporaries, and range variables—matches exactly the state expected at the end of the destination trace's preamble. In Pycket a major part of the interpreter state consists of the environment and the continuation, which are all heap allocated objects. If the shape differs when entering a trace of a loop, for instance because a non-tail call inserted an extra frame to the continuation, the bridge must fall back to the preamble or to interpretation, negating any benefits from LICM, as well as tracing in general.

	\paragraph{}%
		Earlier versions of Pycket mitigated some of this mismatch by allowing the JIT to allocate just enough additional data to reconcile the states. Escape analysis and malloc-removal often virtualize those allocations \cite{malloc-removal:11,loop-aware:12}. A small heuristic therefore permits the JIT to materialize objects that would otherwise stay virtual, trading a bit of space for a jump into the peeled iteration; thus yielding roughly a 4 \% speed-up for gradually typed programs with no measurable overhead \cite{pycketmain2}.

	\paragraph{}%
		Any functionality that Pycket now imports as Racket code\footnote{via bootstrapping linklets} would, in a handwritten RPython interpreter, exploit meta-interpreter hints-such as \verb|@jit.unroll_safe|-to guide the JIT back-end. Because the imported modules arrive as opaque ASTs, Pycket cannot inject such hints, so the tracer sees long interpreter-style dispatch loops without guidance. Moreover, the runtime feedback that Pycket and all the interpreters on RPython utilize quite extensively (that we talked about in \chapterRef{chapter:rpython}) doesn't work. For example, we can't \emph{promote} values that are within the macro expander, or can't mark methods/functions as \emph{elidable}.

	\paragraph{}%
		Any functionality that Pycket now imports as Racket code\footnote{via bootstrapping linklets} would, in a handwritten RPython interpreter, utilize meta-interpreter hints—such as \verb|@jit.unroll_safe|—to guide the JIT backend. Because imported modules arrive as opaque ASTs, Pycket cannot insert these hints, causing the tracer to encounter extensive interpreter-style dispatch loops without guidance. Additionally, the runtime feedback extensively used by Pycket and other interpreters built on RPython (as discussed in \chapterRef{chapter:rpython}) is ineffective in this context. For instance, it is impossible to \emph{promote} values within the macro expander or mark methods and functions as \emph{elidable} without manually annotating the Racket code.

	\paragraph{}%
		Introducing the linklet layer lets Pycket execute large Racket programs—including the 2642-function expander—entirely inside the JIT. Bootstrapping the \emph{racket/base} language alone instantiates close to a hundred modules before user code starts. Although micro-benchmarks seem mostly unaffected for fully-expanded programs (the back-end is unchanged), these large workloads exacerbate all the aforementioned issues in program expansion. In short, the very extensibility that makes self-hosting attractive also deprives the meta-tracer of inside information, limiting trace quality and prolonging warm-up.

	\section[\texorpdfstring{Module and Language Loading}{Loading a Language}]{Module and Language Loading}
	\label{section:module-and-language-loading}

		% \begin{sectionpoint}
		% 	Self-hosting introduces a "boot and language expansion" overhead, which Pycket didn't have before.
		% \end{sectionpoint}

		\paragraph{}%
			A Racket runtime has to load the language of the user program—most prominently the language declared on the \#lang line—\emph{before} it can even begin to expand the user's source; the expander should be able to resolve every identifier that the program may reference, and doing so requires loading, and instantiating the language's own modules so that all the bindings are available-for all the phases.


		\paragraph{}%
			Consequently, the complete transitive closure of the language's module graph has to be loaded and expanded ahead of the user program. For a language such as \texttt{\#lang racket/base} that closure already contains roughly 90 leaf modules; larger languages may exceed that number. In the original, non-self-hosting Pycket this preparatory step was executed by the external \texttt{racket} binary, so its cost was invisible to the JIT. Now it is part of the measured runtime.

		\vspace{1.5em}

		\begin{table}[!h]
		\centering
		\small
		\begin{tabular}{@{}lrrr@{}}
			\toprule
			Phase & Time (ms) & GC (ms) & Calls \\ \midrule
			instantiate-linklet            &    868 &   33 & 7875 \\
			make-instance                  &     39 &    5 & 13405 \\
			startup                        &    156 &   53 & 1 \\
			\quad expander-linklet         &    135 &   53 & 1 \\
			\quad regexp-linklet                 &     37 &    8 & 1 \\
			\quad thread-linklet                 &     17 &    6 & 1 \\
			\quad pycket-boot-linklet            &      0 &    0 & 1 \\
			\quad fasl-linklet             &      2 &    0 & 1 \\
			\quad set-params               &     18 &    0 & 1 \\ \midrule
			compile                        &   1603 &  298 & -- \\
			\quad compile-linklet          &    106 &   38 & 680 \\
			\quad compile-sexp-to-ast      &    773 &  125 & 685 \\
			\quad compile-normalize        &    211 &   27 & 2026 \\
			\quad compile-assign-convert   &    512 &  108 & 5533 \\ \midrule
			boot            		   	   &   2740 &  403 & -- \\
			\textbf{init-lib}			   &	128408 &   8477 & -- \\
			\bottomrule
		\end{tabular}
		\caption{Loading \racketcode{\#lang racket/base} without using any compiled code}
		\label{table:boot-no-compiled}
		\end{table}

		\paragraph{}%
			\tableRef{table:boot-no-compiled} quantifies that cost of a cold boot when \emph{no} pre-compiled code is used, loading the \texttt{\#lang racket/base}. The loading of the bootstrapping linklets, their compilation into linklet objects and their instantiation, as well as instantiating all the Racket modules for the \texttt{\#lang racket/base} (\emph{init-lib}) together dominate the 2.7s wall-clock time. Note that the compilation of a linklet involves constructing AST nodes for every object that was in the s-expression, and the A-normalization and assignment conversion passes. Moreover, because these steps are internal to the interpreter and not part of the user code, these milliseconds are almost entirely spent in the interpreter.

		\paragraph{}%
			Before self-hosting, none of that overhead appeared: Pycket received a fully-expanded core program, and the JIT could concentrate on the hot loops of \emph{user} code directly. The new architecture gives Pycket full visibility into the macro expander, the module system, the reader, and more, but that transparency comes with an up-front cost; evaluating all these sub-systems to load and expand then language on the \gls{jit}.

		\paragraph{}%
			One obvious exploitation of this transparency is to reuse compiled code. By setting up some (Racket-level) parameters such as \racketcode{use-compiled-file-paths} and \racketcode{read-accept-compiled}, Pycket instructs the expander to prefer \texttt{.zo} files over \texttt{.rkt} sources whenever possible. Doing so eliminates the most expensive compile-time passes (cf.\ the disappearance of \texttt{compile-linklet}, \texttt{assign-convert}, and \texttt{normalize} in \tableRef{table:boot-compiled}), yet it also shifts work: module bodies must be deserialized (\racketcode{fasl->s-exp}) and validated, and the resulting syntax objects still have to be parsed into ASTs suitable for Pycket's evaluator.

		\paragraph{}%
			Shown in \tableRef{table:boot-compiled}, as expected, using pre-compiled \texttt{.zo} artifacts for Racket modules makes the \emph{compiled-code} path dramatically faster: the \texttt{init-lib} phase's wall-clock time falls from about 127s when loading source to roughly 3.4s with compiled code, while avoiding compilation saves large heap allocations, therefore the GC time drops from 8.5s to a few hundred milliseconds.

		\begin{table}[!h]
		\centering
		\small
		\begin{tabular}{@{}lrrr@{}}
			\toprule
			Phase & Time (ms) & GC (ms) & Calls \\ \midrule
			instantiate-linklet            &    903 &   40 & 684 \\
			make-instance                  &      2 &    0 & 845 \\
			startup                        &    129 &   38 & 1 \\
			\quad expander-linklet         &    109 &   38 & 1 \\
			\quad regexp-linklet                 &     20 &    4 & 1 \\
			\quad thread-linklet                 &     12 &    4 & 1 \\
			\quad pycket-boot-linklet            &      0 &    0 & 1 \\
			\quad fasl-linklet             &      1 &    0 & 1 \\
			\quad set-params               &     18 &    0 & 1 \\ \midrule
			read                           &    146 &   29 & -- \\
			\quad fasl$\rightarrow$s-exp   &    144 &   29 & 74 \\
			\quad s-exp$\rightarrow$ast    &      2 &    0 & 74 \\
			\quad assign-convert-deser.    &      0 &    0 & -- \\ \midrule
			compile                        &   1439 &  313 & -- \\
			\quad compile-linklet          &      0 &    0 & 9 \\
			\quad compile-sexp-to-ast      &    836 &  128 & 764 \\
			\quad compile-normalize        &      0 &    0 & 28 \\
			\quad compile-assign-convert   &    603 &  185 & 11763 \\ \midrule
			boot                 		   &   2673 &  428 & -- \\
			\textbf{init-lib}			   &	3387 &   361 & -- \\
			\bottomrule
		\end{tabular}
		\caption{Loading \racketcode{\#lang racket/base} using compiled code}
		\label{table:boot-compiled}
		\end{table}

		\paragraph{}%
			Regardless of the source of each module, every macro expansion of user code ultimately depends on the performance of evaluating the underlying expander. Pycket's ability to inline into, and sometimes across, expander helpers opens new optimization opportunities, but it also means that the performance deficiencies in the evaluation of the expander itself now directly affect application performance. The next sections discuss those interactions in detail.

	\section[\texorpdfstring{Cost of Self-hosting on Back-end \& Front-end}{Cost of Self-hosting}]{Cost of Self-hosting on Back-end \& Front-end}
	\label{section:cross-benchmarks}

		\begin{paragraph-here}% 1
			 Everything we've done with the bootstrap linklets are done at the front-end of Pycket, the only change in the CEK back-end is the addition of the linklet evaluation semantics. Therefore we don't expect too much differrence in the back-end performance with the original Pycket back-end performance.
		\end{paragraph-here}

		\begin{paragraph-here}% 2
			Note that increasing performance of the original Pycket is not one of the goals of this research. The Racket expander and all the imported functionality carries the same characteristics that make implementing dynamic languages difficult, such as late-binding, dispatching, and boxing due to dynamic typing. The self-hosting Pycket is competing in performance neither with the original Pycket nor Racket.
		\end{paragraph-here}

		\begin{paragraph-here}% 3
			Since the fully expanded Racket programs in both the original Pycket and the new Pycket with self-hosting are identical, there’s no reason Pycket with self-hosting should be faster than the original Pycket. We hypothesize that self-hosting Racket didn't hinder the JIT's back-end ability to discover the critical loops in the fully expanded user programs. In this section, we test this hypothesis and analyze the cost of self-hosting (i.e. including the expansion time in the overall run-time) on back-end and front-end separately in isolation.
		\end{paragraph-here}

		\subsection{Experiment Setup}
		\label{benchmark-setup}

			\begin{paragraph-here}% 4
				To test our hypothesis, we conducted experiments on a 10-node Kubernetes cluster, where each worker node running Ubuntu 24.04 LTS on x86-64-v2-aes CPU at 2.1 GHz with 32 MB cache and 16 GB of RAM. The cluster was virtualized on Proxmox v8.2.2. All benchmarks are run on separate worker each on different separate VMs. Each worker is set to run only a single pod at a time via podAntiAffinity rule, and each pod is set to run only a single benchmark, report the numbers and die.
			\end{paragraph-here}

			\begin{paragraph-here}% 5
				All benchmarks ran with Racket 8.18.0.1 with Pycket as of revision 00be1e3. Every benchmark was run 500 times uninterrupted at highest priority in a new process in the worker node. The execution time was measured in-system and, hence, does not include start-up; however, warm-up was not separated, so all times include JIT compilation. We show the arithmetic mean of all runs along with bootstrapped confidence intervals for a 95\% confidence level \cite{davisonBootstrapMethodsTheir2013}.
			\end{paragraph-here}

			\begin{paragraph-here}% 6
				For each benchmark, all the Kubernetes job configurations and bash scripts are automatically generated. The generation scripts, along with all the analysis and plotting scripts can be found at https://github.com/cderici/pycket-performance/ [MAKE HYPERREF].
			\end{paragraph-here}

		\subsection{Back-end}

			\inputFigure{problem}{run6-npww-opww-rel-R-chart}

			\begin{paragraph-here}% 7
				The results of original Pycket vs Pycket with the new front-end on Larceny Cross-platform Scheme Benchmark suite are summarized in \figref{fig:pycket-npww-opww-relR}. We chose this suite because these benchmarks are already prepared and used in earlier Pycket studies, where run times are adjusted to lower jitter with fast-running benchmarks, all I/O is moved out of the timed loop, etc. The only change we made to the benchmarks for this study is to have all the benchmarks to be in \racketcode{#lang racket/base}, even though some of them could be in smaller languages.
			\end{paragraph-here}

			\begin{paragraph-here}% 8
				Note that, because we don't have the start-up in these times, these times do not include the expansion of these benchmarks, thereby isolating the measurement of only the back-end performance for the fully-expanded programs (expansion time is not included). However, the traces that are captured during the evaluation of the expander and all the modules for the \racketcode{#lang racket/base} are still captured and available during the timed section of the benchmark, since it's the same process.
			\end{paragraph-here}

			\begin{paragraph-here}% 9
				We can see in the results, especially looking at the geometric mean, that the performance of both Pyckets are quite similar, validating our hypothesis, that even though the evaluation of the program expansion is included in the computation (but not the measured time), the back-end performance remains mostly unaffected. In some of the benchmarks, in particular in `earley`, `perm9` and `nboyer` we observe a slow-down in Pycket with self-hosting that we don't see for other benchmarks, and that's due to increase in GC pressure. `perm9` is a memory system benchmark, `earley` is a parser that implements a grammar, and `nboyer` is a logic programming benchmark. All of them involve large indirections and allocate and de-allocate a lot of objects in the heap within each benchmark, therefore, the memory issues that come with self-hosting are all exaggerated. We'll demonstrate these issues in detail in \secref{section:memory}.
			\end{paragraph-here}

			\begin{paragraph-here}% 10
				Our hypothesis is also confirmed by looking closely at the captured traces in the JIT, we observe that the most used traces for each benchmark are almost identical between the original Pycket and the Pycket with self-hosting, despite that Pycket with self-hosting also includes some large traces captured during the expansion phase. Because our hypothesis that "self-hosting didn't effect the back-end" is validated with these experiments, Pycket's performance analyses in the earlier studies all remain valid \cite{pycketmain, pycketmain2}.
			\end{paragraph-here}

		\subsection{Front-end}

			\begin{paragraph-here}% 11
				Now we take a look at the run times for the expansion of the same benchmarks. For this, we used the same setup that's described in the \secref{benchmark-setup}. We took each benchmark that's used for measuring the back-end performance, and turned it into a benchmark that measures the expansion performance of that program.
			\end{paragraph-here}

			\inputFigure{problem}{ack-vs-ack-expand}

			\begin{paragraph-here}% 12
				\figref{fig:ack-vs-ack-expand} shows the transformation on an example benchmark (ack). Each benchmark explicitly calls `expand` repeatedly and the time is measured in-system. Note that loading \racketcode{#lang racket/base} is still not included in the measurement, just \racketcode{expand}. Although these benchmarks might not be the ideal benchmarks for benchmarking program expansion performance per se, we still wanted to use the same suite just to give a consistent and complete analysis of front-end and back-end performance of the same system with the least amount of variables in experiments.
			\end{paragraph-here}

			\inputFigure{problem}{run7-np-vs-R-expansion-chart}

			\begin{paragraph-here}% 13
				\figref{fig:crossbenchmark-expansion-times-vs-R} shows the expansion phase performance comparison between Pycket with self-hosting and Racket on Chez. Using Racket here is the same as using the original Pycket because the original Pycket uses the Racket's own binary to expand a given program anyways.
			\end{paragraph-here}

			\begin{paragraph-here}% 14
				The results show about two orders of magnitude difference in performance between Pycket and Racket across the board. This is not surprising, as earlier studies stated that meta-tracing is bad in performance for programs with data-dependent control flow in the style of an interpreter \cite{pycketmain, bolzThesis}.
			\end{paragraph-here}

			\begin{paragraph-here}% 15
				In the next sections, we delve into the issues behind these results, detailing what the problems are and why are they fundamental to self-hosting, with some specific, targeted experiments and measurements.
			\end{paragraph-here}

		% \begin{show-experiment}
		% 	Warmup breakdowns
		% \end{show-experiment}


	\section[\texorpdfstring{The Nature of the Beast: Tracing Data-Dependent Computation}{The Nature of the Beast}]{The Nature of the Beast: Tracing Data-Dependent Computation}
	\label{section:nature-of-the-beast}

		% \begin{sectionpoint}
		% 	Data-dependent, interpreter-style, branch-heavy code produces overspecialized, non-reusable traces.

		% 	Computations that are critical to self-hosting—such as program expansion—seem poorly suited to meta-tracing because they contain no hot loops for the tracer to capture.
		% \end{sectionpoint}

		\paragraph{}%
			The \emph{Racket macro expander}, loaded inside Pycket as the \emph{expander linklet}, now runs in the same process as every user program; its performance therefore lies on the critical path, because (i) expansion itself contributes directly to end-to-end latency, as detailed in \chapterRef{chapter:pycket}, and (ii) the entire language hierarchy must be loaded and initialized first, as shown in the previous section.

		\paragraph{}%
			Conceptually the expander is a tree-walking interpreter from higher level Racket languages to the \racketcode{\#\%kernel} language. Its control flow walks a rich lattice of macros and compile-time bindings, yielding deeply branch-heavy control flow whose exact shape depends on the user's program and the libraries it imports. Pycket's meta-tracing \gls{jit} therefore meets high-level Racket functions whose behavior cannot be trivially predicted at trace time; unlike the low-level CEK interpreter these helpers cannot be decorated with hints such as \texttt{@jit.unroll\_safe}, and their extensive allocation and branching patterns frustrate the usual trace-specialization heuristics. Therefore, the traces generated for these high-level abstractions are substantially larger; the \gls{jit} spends significant time tracing and optimizing these paths only to encounter unpredictable branches, bail out, and fall back to interpretation—negating much of the anticipated speed-up.

		\paragraph{}%
			In interpreter-style, data-dependent recursion the control flow fans out across many branches driven by the program's input, yet a tracing \gls{jit} captures only the \emph{single} linear path taken while tracing and specializes the resulting code to that exact sequence of decisions. When new input steers execution down a different branch the \gls{jit} must abort, start another trace, and specialize again. Over time this produces a combinatorial explosion of narrow, redundant traces—each optimal for the data that birthed it but useless for most other paths—inflating compilation time, thrashing the trace cache, and frequently forcing the runtime to fall back on interpretation.

		\paragraph{}%
			In \chapterRef{chapter:solution} we propose a lightweight meta-hint mechanism that aims to help with this by helping to detect hot branches during interpretation and guiding the tracer away from branch-heavy code paths, mitigating the trace compilation churn and the explosion of overspecialized traces. In the remainder of this section, we examine in detail how the meta-tracer in Pycket behaves on branch-heavy computations.

		\subsection{Tracing Branch-Heavy Computation}
			% \begin{mainpoint}
			% 	Data-dependent, interpreter-style, branch-heavy code produces overspecialized, non-reusable traces.
			% \end{mainpoint}
			\label{section:branchy}

			\paragraph{}%
				As detailed in \chapterRef{chapter:pycket}, the Racket expander--containing the macro expander, module system, and more--is notably large and intricate. Investigating tracing JIT behavior directly within such a sizable and complex program can be impractical. Thus, we analyze the tracing behavior using smaller, controlled examples to isolate and clearly observe specific issues.

			\inputSub{problem}{branchy-source}

			\paragraph{}%
				\figref{fig:branchy-introduce-code} introduces \emph{Branchy}\footnote{Full code and alternative definitions of Branchy is available at https://github.com/cderici/pycket-performance/blob/master/branchy/private/branchy.rkt.}, a synthetic benchmark intentionally designed to exhibit interpreter-style, branch-heavy, and data-dependent behavior. It is synthesized by simplifying Racket's \racketcode{fasl->s-exp} function from the \gls{fasl} library, which deserializes a given byte stream into an s-expression. Each iteration processes an element from an input list through nested conditional branches resembling a decision tree. Certain inputs produce consistently deep paths through this branching structure, while others immediately exit via shallow paths. For instance, an input list such as \racketcode{'(0 0 0 ...)} repeatedly traverses the longer path sequence \emph{18-8-3-1}, via \racketcode{Exit B}, whereas \racketcode{'(20 20 20 ...)} consistently follows the shorter path labeled \emph{18}, repeatedly taking \racketcode{Exit A}.

			\paragraph{}%
				Note that describing static source code itself as "branch-heavy" can be misleading. The critical notion here is the data-dependent nature of branch decisions during runtime. A program could possess deeply nested conditional structures yet exhibit minimal runtime branching if the input consistently chooses shallow branches. Therefore, the "branchiness" of a computation is an empirical property, characterized by the frequency and complexity of branching decisions made during an actual evaluation over an input.

			\paragraph{}%
				This data-dependency explicitly manifests through guards placed by a tracing \gls{jit}. As discussed in \chapterRef{chapter:rpython}, guards serve as exit points in a trace, verifying assumptions made during tracing. Thus, each conditional encountered in the user program generates a corresponding guard in the trace to enforce assumptions about input data at trace time. An increased number of control-flow decisions therefore results in more guards within the trace. Additionally, guards carry runtime information that allows the \gls{jit} to exit to the interpreter upon guard failure. Consequently, numerous guard operations can significantly increase memory consumption \cite{schneiderEfficientHandlingGuards2012}.

			\begin{figure}[h] % let LaTeX decide where it fits best
				\centering
				\includegraphics[width=0.6\linewidth]{\inputFigPath{problem}{branchy-long-path-trace-guards.png}}
				\caption{RPython trace for Branchy running all-zero input taking a long 18-8-3-1 path. [PLACEHOLDER IMAGE]}
				\label{fig:branchy-long-path-trace}
			\end{figure}

			\paragraph{}%
				\figref{fig:branchy-long-path-trace} presents an excerpt from a trace of Branchy executed with an all-zero input, repeatedly following the deep path sequence \emph{18-8-3-1}. Note how all the guards in trace correspond to an if condition in the Branchy code. Each decision along this path generates a guard that must hold true at runtime; failure of any guard causes the trace to exit prematurely and fallback to the interpreter or trigger another trace. The resulting trace, while optimized and compiled, is excessively specialized, limiting its utility exclusively to identical inputs.

			\paragraph{}%
				Thus, the internal structure of the input data significantly influences tracing outcomes. Repetitive input patterns, such as \racketcode{'(3 3 3 3 ...)}, allow trace reuse within the pattern, thereby amortizing the overhead of tracing and compiling. However, highly repetitive inputs like these are uncommon in realistic workloads. Conversely, more varied or random inputs, such as \racketcode{'(1 3 5 1 3 5 ...)}, significantly reduce trace reuse, because of the same specialization. This phenomenon of internal "loops" within the input data will be revisited in more detail in \sectionRef{section:there-is-no-loop} when we discuss the nature of loops the tracer attempts to capture.

			\begin{figure}[h] % let LaTeX decide where it fits best
				\centering
				\includegraphics[width=0.8\linewidth]{\inputFigPath{problem}{branchy-trace-graphs.png}}
				\caption{Tracing branch-heavy computation with arbitrary branches results in a combinatorial explosion of recorded loops and bridges. [PLACEHOLDER IMAGE]}
				\label{fig:branchy-trace-graphs}
			\end{figure}

			\paragraph{}%
				\figref{fig:branchy-trace-graphs} and \tableRef{table:branchy-backend-summaries} illustrate this effect quantitatively. For inputs with highly repetitive branching decisions (e.g., all 20s), the meta-tracer records only two loops and three bridges, requiring a negligible fraction of runtime spent on trace compilation. However, even these traces remain highly specialized to the particular repetitive path taken, significantly limiting their reusability in more general cases where input variation occurs. In contrast, random inputs, more representative of realistic macro expander behavior, induce a combinatorial explosion: 39 loops, 248 bridges, and substantially more traced operations, resulting in significant overhead in trace compilation and optimization.

			\begin{table}[!h]
			\centering
			\small
			\begin{tabular}{@{}lcc@{}}
				\toprule
				& \textbf{Repeating Branches} & \textbf{Random Branches} \\
				\midrule
				\textbf{Tracing (s)}                 & \textbf{0.004566} & \textbf{0.153118} \\
				Backend (s)                 & 0.001239 & 0.044965 \\
				TOTAL (s)                            & 1.185808 & 2.242329 \\ \\
				ops                                   & 2130 & 147895 \\
				heapcached ops                        & 572 & 41615 \\
				\textbf{recorded ops}                & \textbf{462} & \textbf{31202} \\
				\quad calls                                 & 12 & 380 \\
				guards                                & 128 & 7391 \\
				opt ops                               & 243 & 11448 \\
				opt guards                            & 75 & 3768 \\
				opt guards shared                     & 46 & 2553 \\
				nvirtuals                    & 74 & 1982 \\
				nvholes                               & 8 & 126 \\
				nvreused                              & 36 & 907 \\
				\textbf{Total \# loops}               & \textbf{2} & \textbf{39} \\
				\textbf{Total \# bridges}             & \textbf{3} & \textbf{248} \\
				\bottomrule
			\end{tabular}
			\caption{JIT back-end summaries for branch-heavy computations taking repeating vs random branches. Redacted fields are all zero for both cases.}
			\label{table:branchy-backend-summaries}
			\end{table}


			\paragraph{}%
				This observation naturally raises the question of what constitutes an ideal tracing strategy for interpreter-style, branch-heavy computations. Should a tracing JIT generate one highly specialized trace per unique branching pattern, incurring substantial memory and compilation overhead but achieving maximum runtime efficiency for those exact inputs? Or is it preferable to construct broader, less specialized traces containing multiple branches, potentially reducing memory footprint but introducing runtime overhead from unnecessary paths? Consider a repeated branching pattern such as \emph{18-8-3-5-4-18-8-3-5-4-...}; capturing the entire pattern into one trace may yield high specialization, whereas identifying a shorter common prefix (e.g., \emph{18-8-3-5}) might increase trace reuse at the cost of less specialization. However, for our specific scenario regarding self-hosting, neither tracing strategy sufficiently justifies the resources expended by meta-tracing during runtime. As we will elaborate in the next section, this inefficiency arises because program expansion occurs at compile-time, where the control flow lacks meaningful loops to capture—each computation path is effectively static and resembles the linear decision-making of a regular-expression matcher.

			\paragraph{}%
				This fundamental tension explains the limited adoption of tracing JITs in practice in the last decade [CITE HERE]. Highly specialized traces yield considerable speed-ups only for workloads closely matching traced paths, while deviating workloads incur substantial tracing overhead without corresponding benefits. Consequently, the average-case performance may significantly degrade, exemplified by the observation that a tracing JIT that is ten times faster on half of a benchmark suite but ten times slower on the other half is, overall, more than five times slower in practice~\cite{mozblog}.

		\subsection{Performance of Program Expansion}
			\label{section:there-is-no-loop}
			% regexp stuff

			% \begin{sectionpoint}
			% 	Computations that are critical to self-hosting—such as program expansion—seem poorly suited to meta-tracing because they contain no hot loops for the tracer to capture. There is no loop.
			% \end{sectionpoint}

			\paragraph{}%
				Moving beyond synthetic examples, we now shift our attention to a practical, real-world use-case: regular expressions. By examining a realistic and widely used computational pattern, we deepen our understanding of the performance challenges meta-tracing encounters in more typical interpreter-style computations. Regular expression matching offers a valuable analog to program expansion, as it shares the interpreter-style, data-driven, branch-heavy computational nature inherent in real-world expansions. Through analyzing regular expressions, we can extrapolate critical insights into how Pycket's meta-tracer handles such complex branch structures when processing large-scale expansions.

			\paragraph{}%
				Fortunately, Pycket already includes a robust, efficient regular expression implementation in RPython, providing a direct benchmark. This allows us to precisely measure performance by comparing it against the identical functionality implemented in the regexp linklet, facilitating a clear, one-to-one performance analysis.


			\inputSub{problem}{regexp-example-defg}

			\paragraph{}%
				To illustrate concretely, we consider matching the simple regular expression $\mathtt{\#}$\racketcode{rx"defg"} against a large string containing the substring \racketcode{"aaa...defg...aaa"} positioned centrally, as shown in \figref{fig:regexp-example-defg}.

			\inputSub{problem}{regexp-example-trace}

			\paragraph{}%
				As shown in \figref{fig:regexp-example-trace}, the critical operation is essentially a linear literal search through the string. The RPython implementation successfully identifies and optimizes this pattern into a tight, efficient loop trace, highlighting its capability to leverage loop invariants and avoid unnecessary allocations. \figref{fig:regexp-linklet-big-trace} shows the trace for most used loop in Pycket using the regexp linklet implementation.

			\paragraph{}%
				The most notable difference is the size of the trace, with 191 operations vs 21, and significantly many parameters in the optimized loop demonstrates the generic and loose nature of the trace, as opposed to the tight loop that the RPython implementation was able to capture. Furthermore, there are numerous arguments to the trace, therefore, many objects that existed before the trace and that are passed in as arguments, therefore the partial evaluating optimizer performing escape analysis just copies all the operations that use them into the output trace.

			\paragraph{}%
				The most notable difference is the trace size, with 191 operations compared to 21. The significantly larger number of parameters in the optimized loop highlights the generic and loosely structured nature of the trace, in contrast to the tight loop effectively captured by the RPython implementation. Moreover, the large number of parameters also represent objects that were live prior to tracing and are passed into it. Consequently, the partial evaluator’s escape analysis duplicates all operations involving these arguments directly into the resulting trace, which increases size dramatically.

			\inputSub{problem}{regexp-linklet-big-trace}

			\paragraph{}%
				The comparative results, shown in \figref{fig:regexp-overall-comparison}, highlight significant performance disparities: the handwritten RPython implementation on Pycket achieves roughly a 30-fold improvement over standard Racket and approximately a 50-fold improvement compared to Pycket using the regexp linklet. This substantial advantage arises because the RPython implementation explicitly leverages interpreter hints, type specialization to optimize allocation strategies, and domain-specific optimization techniques like caching and contexts. Collectively, these optimizations allow the RPython implementation to efficiently manage control flow and memory usage.

			\inputSub{problem}{regexp-overall-runtime}

			\paragraph{}%
				The advantages inherent in the RPython implementation are further highlighted by analyzing performance \emph{without} JIT compilation. Turning off the JIT for both versions reveals insightful patterns: the RPython implementation slows down approximately six-fold, whereas Pycket using the regexp linklet suffers  two-orders of magnitude performance degradation. This difference emphasizes how JIT compilation substantially benefits the evaluation of Racket code. However, the absolute fastest runtime remains with the RPython implementation, reflecting its superior capacity to capture the computation's essence and avoid pitfalls such as mispredicted branches and overspecialized trace bailouts. This issue is evident in the extensive, allocation-heavy traces generated for the Racket-based implementation (as seen in \figref{fig:regexp-linklet-big-trace}), significantly increasing garbage collection overhead, an issue explored further in the following section.

			\paragraph{}%
				Data dependency is again prominent in our analysis. The trace behavior specifically observed for the input $\mathtt{\#}$\racketcode{rx"defg"} differs entirely when matching a pattern like $\mathtt{\#}$\racketcode{rx"a*"}, underscoring the inherent variability in program paths that meta-tracing attempts to capture. Extending this observation to the broader context of Racket's expander, we anticipate a similar scenario but exacerbated by complexity: a diverse stream of varied cases bombarding the tracer, which attempts yet fails to efficiently reuse specialized traces due to the rapidly shifting data-dependent execution paths.

			\paragraph{}%
				Recall that PyPy addresses tracing challenges by identifying and tracing loops directly in the user program rather than at the interpreter level. Essentially, PyPy elevates the abstraction level at which the tracing takes place~\cite{pypy-main}.

			\inputSub{problem}{theres-no-loop}

			\paragraph{}%
				\figref{fig:theres-no-loop} shows how Pycket, in turn, pulls that abstraction level back to where it was, by essentially turning the user program into the interpreter that meta-tracing was able to bypass. With this design, the meta-tracing interpreter evaluates the Racket macro expander that expands the user program, thus, seeking hot loops within a higher abstraction layer—where loops are fundamentally complicated, and typical programs become mere streams of heterogeneous inputs, inheriting all the problems that come with tracing branch-heavy code as we detailed in \sectionRef{section:branchy}.

			\paragraph{}%
  				Transforming or expanding a program shares fundamental characteristics with regular expression matching or \gls{fasl} parsing—each being equally data-dependent. The essence of these computations lies primarily in the ordered manipulation and parsing of symbolic structures rather than in executing the semantics within those structures. Importantly, expanding or transforming a user program that contains loops represents a fundamentally different computational pattern to a tracing JIT than simply interpreting those loops. When interpreting loops, the meta-tracing JIT can trace iterations and capture hot paths. In contrast, program expansion involves repeatedly branching and manipulating data structures, rarely exhibiting consistent iteration patterns suitable for effective tracing. Nevertheless, the loops within a fully expanded user program remain efficiently capturable by Pycket's meta-traced CEK machine core. We will discuss the implications of this multi-level tracing scenario and its resulting performance characteristics further in \sectionRef{section:cross-benchmarks}.

			\paragraph{}%
				Addressing this challenge requires further research and careful consideration. At the heart of this challenge is the fundamental violation of the second assumption underlying tracing \glspl{jit}, as previously discussed in \chapterRef{chapter:rpython}. The second assumption—that multiple iterations of the same loop are likely to traverse similar code paths—is directly contradicted by the highly data-dependent, branch-heavy nature of interpreter-style computations. This inherent violation explains why such computations, although critical for self-hosting, remain particularly ill-suited to meta-tracing and tracing \glspl{jit} in general.

	\section[\texorpdfstring{Memory Issues with Self-Hosting}{Memory Issues}]{Memory Issues with Self-Hosting}
	\label{section:memory}

		\paragraph{}% 1
			Another significant problem we identified in self-hosting functional languages on meta-tracing \glspl{jit}, such as Pycket, is increased pressure on \gls{gc}. In the original Pycket design, module expansion was delegated to Racket's standalone executable, leaving Pycket's heap usage predominantly limited to user program evaluation. However, the self-hosting design now requires Pycket to execute large-scale expansions internally—particularly for modules employing extensive macros or sophisticated language features. As described in \chapterRef{chapter:pycket}, this change substantially increases heap allocations, notably in the form of deeply nested continuation objects and large, persistent structures, such as linklet instances containing extensive sets of functions. Consequently, Pycket's heap exhibits characteristics significantly different from typical interpreter workloads, placing considerable stress on the \gls{gc}, which relies heavily on allocation behavior assumptions for its efficiency.

		\paragraph{}% 2
			To better understand the impact of these heap allocation patterns, we briefly review the garbage collector employed by the RPython framework—namely, the minimark \gls{gc}. As discussed in \chapterRef{chapter:rpython}, RPython translates interpreter-level memory operations (e.g., \texttt{malloc}) into managed allocations using its built-in garbage collector. The minimark \gls{gc} is a generational, semispace-copying collector optimized for short-lived objects by employing a nursery sized roughly to match the CPU's Level 2 cache. Objects are initially allocated in the nursery; upon nursery saturation, surviving objects are copied into an older generation to reduce full collections. The underlying assumption is that most objects die young, a property which typically holds for workloads encountered by tracing interpreters, as young, short-lived temporaries dominate their allocations. However, in the context of self-hosting expansion, Pycket generates numerous large, long-lived objects—such as continuation chains and complex linklet instances—which consistently survive minor collections and must be managed in older generations, triggering expensive full collections and undermining the generational assumptions of minimark \gls{gc}. \cite{pypy06, bolzPhDThesis, gc:16, LuaJITLanguageToolkit}

		\paragraph{}% 3
			The integration of Racket's expander into Pycket's runtime alters heap allocation patterns significantly, causing a substantial increase in old-generation heap objects and long-lived continuation chains. Consider, for example, the instantiation of the expander linklet itself, which consists of over 2600 functions and persists throughout Pycket's runtime. Similarly, macro expansions and module loading involve deeply nested continuations due to extensive control-flow indirections common in real-world Racket programs. Within RPython's minimark \gls{gc}, objects identified as large or old\footnote{The age of an object is the number of
			collections it survived.} are moved to an external generation, where they remain stationary and are collected through a serial mark-and-sweep phase. As a result, not only does Pycket fail to fully benefit from the nursery's fast collection cycles, but it also incurs additional overhead from the blocking mark-and-sweep collections of these long-lived external objects.

		\vspace{1.5em}

		\begin{table}[!h]
			\centering
			\small
			\begin{tabular}{lcccc}
			\toprule
			& Collections & Bytes copied (KiB) & Old‑gen growth (MiB) & GC wall‑time (ms)\\
			\midrule
			Old Pycket             	& 15   & 1367.1 & 19.2  & 40.6\\
			New Pycket 				& 500  &   546.4 & 148.0 & 890.6\\
			\bottomrule
			\end{tabular}
			\caption{Memory footprint of original Pycket vs Pycket hosting Racket. Nursery size: 32M}
			\label{table:memory-footprints}
		\end{table}

		\paragraph{}% 4
			The practical effects of these memory-management changes on Pycket are quantitatively evident in \tableRef{table:memory-footprints}, which contrasts the original Pycket with the current implementation integrating Racket's self-hosting architecture. All values presented in the table are averages computed over 1000 runs. Switching to the self-hosting design dramatically increases the frequency of minor garbage collections—from an average of 15 collections in the original Pycket to 500 collections in the current implementation. Although each minor collection in the current Pycket typically copies fewer bytes (approximately 546 KiB versus 1367 KiB), the cumulative impact is substantial. Specifically, the growth of the old-generation heap rises nearly eight-fold, from 19.2 MiB to 148.0 MiB. Consequently, total garbage-collection time increases significantly, from approximately 41 milliseconds in the original Pycket to about 891 milliseconds in the current Pycket, underscoring that the introduction of large, persistent, and pointer-rich Racket structures fundamentally challenges the assumptions underlying RPython's generational minimark \gls{gc}.

		\inputFigure{problem}{nursery-size-vs-gc-time-figure}

		\paragraph{}% 5
			The impact of nursery size on garbage-collection overhead in Pycket is demonstrated in \figref{figure:nursery-size-vs-gc-time}, highlighting clear trade-offs in tuning this parameter. At smaller nursery sizes (2--8\,MiB), frequent minor collections lead to substantial total GC overhead, reaching up to 1.7 seconds. Increasing the nursery size to align with typical CPU cache sizes (32--64\,MiB) significantly reduces this overhead, with GC times stabilizing around 0.5 seconds, thus identifying an optimal performance region. However, further enlarging the nursery provides diminishing returns, only slightly reducing the overhead while increasing memory demands. These results confirm that while nursery-size tuning can mitigate the pressure induced by large and persistent objects, it cannot completely address the underlying challenges posed by Racket's expansion workloads in Pycket.



	% Runtime Performance of Tracing Data-Dependent Branchy Code


