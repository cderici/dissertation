% Tracing JITs and Meta-Tracing Frameworks
% Tracing Just-in-time (JIT) Compilers and Meta-tracing
\chapter[\texorpdfstring{META-TRACING JUST-IN-TIME (JIT) COMPILERS}
                          {RPython \& Meta-tracing}]{META-TRACING JUST-IN-TIME (JIT) COMPILERS}
    \label{chapter:rpython}

    \begin{chaptersynopsis}

        We do three things in this chapter:

        1. Thesis statement refers to meta-tracing JITs, so we explain what a meta-tracing JIT compiler is.

        2. Introduce RPython as a language, and a framework that Pycket is built on.

        3. Provide context for meta-tracing-related technical discussions in the rest of the study. traces, warmup, optimizations, runtime feedback, etc.

        \vspace{2em}

        Sections:
		\begin{itemize}
			\item Meta-tracing \& RPython Framework

                What problem does PyPy solve? How do they capture loops in user programs? JIT drivers, interpreter hints, green/red variables etc.
			\item Pycket: A Rudimentary Racket Interpreter Built on RPython Framework

                Primer on Pycket's fundamentals. It's language RPython, CEK core, and how it is built.
			\item Trace Optimizations \& Runtime Feedback

                Traces are the real performance currency. Relevant optimizations and runtime feedback (e.g promote, escape analysis, warmup, etc.).
		\end{itemize}
    \end{chaptersynopsis}

    \paragraph{}%
        In this chapter, we have three main objectives. First, we explain the concept of a meta-tracing Just-in-Time (JIT) compiler, clearly connecting it to our thesis statement presented earlier, that efficient self-hosting of full-scale functional languages is achievable using meta-tracing JIT compilation. Second, we introduce the RPython framework, detailing its role as the foundational technology upon which Pycket is constructed, including aspects of translation and toolchain workflow. Lastly, we outline key concepts related to RPython and meta-tracing—such as trace formation, optimization strategies, and runtime feedback—which serve as essential context for technical discussions and evaluations relating to performance in \chapterRef{chapter:problem} and \chapterRef{chapter:solution}.

    \paragraph{}%
        A Just-in-Time (JIT) compiler dynamically compiles frequently executed parts of a program at runtime, interleaving compilation with interpretation. Unlike Ahead-of-Time (AOT) compilers, which compile the entire program beforehand, JIT compilers selectively target hot paths identified through low-overhead profiling. When the interpreter repeatedly executes a particular sequence of instructions, the JIT compiler pauses interpretation momentarily, compiles and optimizes this sequence, and subsequently executes the optimized code whenever the same instruction path recurs \cite{dynamo}.

    \paragraph{}%
        JIT compilers have proven particularly effective for dynamic language virtual machines (VMs), generally adopting either method-based or trace-based approaches. Method-based compilers optimize frequently invoked methods, whereas trace-based compilers focus on frequently executed loops \cite{survey:05,jit-history:03}. This dissertation specifically focuses on trace-based compilation. Tracing JITs generate optimized machine code by tracing and compiling execution paths under two fundamental assumptions\cite{pypy-main}:

    \begin{enumerate}
        \item Programs spend most of their execution time in loops.
        \item Iterations of the same loop often follow similar execution paths.
    \end{enumerate}

    \inputSub{rpython}{fig-trace}

    \paragraph{}%
        To exploit these assumptions, tracing JIT compilers identify certain execution paths as \emph{hot loops}. Upon detecting a hot loop, the interpreter pauses evaluation to compile this loop into a \emph{trace}, subsequently using the optimized trace whenever the same path is executed again. A trace is a linear sequence of instructions with a single entry and potentially multiple exit points. \figref{fig:trace} illustrates a simplified trace, with inputs $p0$ and $p1$, comprising a preamble (due to loop unrolling) and an inner loop that jumps back to itself. \emph{Guards} within a trace define its exit points, performing runtime checks to ensure that conditions remain consistent with the initial tracing context and handling conditions for loop termination or deviation.

    % \begin{paragraph-here}
    %     The tracer is a component of a JIT compiler that is responsible for tracing the execution of the program and generating traces.
    % \end{paragraph-here}

    % \begin{paragraph-here}
    %     Whenever a hot loop is detected, the tracer generates a trace that captures the execution of the loop.

    %     For a given program, a time that it takes for the JIT to find and trace all the hot loops is called the warmup time.
    % \end{paragraph-here}

    \section[\texorpdfstring{Tracing vs Meta-tracing in JIT Compilation}{Tracing vs Meta-tracing}]{Tracing vs Meta-tracing in JIT Compilation}
        \begin{mainpoint}
            The problem that PyPy solves:

            Rather than the loops in the interpreter being evaluated, meta-tracing manages to capture the hot loops in the user program being interpreted.
        \end{mainpoint}

        \paragraph{}%
            When a tracing JIT compiler is applied to a language interpreter, it typically ends up tracing the loops of the interpreter itself—rather than the loops in the user program that the interpreter is executing. To understand why this is a problem, we need to distinguish three roles: the language interpreter (the implementation of the language semantics), the tracing interpreter (or meta-tracer), and the user program (the input being evaluated by the language interpreter). The meta-tracer effectively executes the language interpreter step-by-step, carrying out its operations and observing their effects. Without additional guidance, it has no visibility into the boundaries or semantics of the user-level code being interpreted, and so it naturally captures hot paths in the language interpreter logic rather than in the user program.

        \paragraph{}%
            Within a language interpreter, the primary dispatch loop is typically the most significant loop. For a functional programming language, this usually manifests as a recursive descent loop, evaluating sub-expressions iteratively. However, while evaluating a user program, this loop structure in the language interpreter fundamentally violates one of the core assumptions of tracing JIT compilers—that several iterations of a hot loop tend to follow similar code paths—because it is uncommon for an interpreter to repetitively evaluate the exact same operation or expression in succession. Rather, loops in the user program often unfold as different branches taken through the language interpreter’s dispatch logic.

        \paragraph{}%
            Furthermore, by nature, tracing captures the execution paths where a program spends most of its running time. In the scenario of applying a tracing JIT to a language interpreter, however, the interpreter's dispatch loop itself is typically not the most essential computational path. Instead, the loops within the user program represent the true computational hot spots. Thus, what we seek from a tracing JIT in this context is to identify and optimize the loops of the user program, not the loops within the interpreter. This is precisely the problem addressed by \emph{meta-tracing}, as defined in PyPy: it successfully redirects the tracing JIT compiler to identify and optimize the hot loops of the user program, rather than the interpreter logic itself \cite{pypy-main}.

        \paragraph{}%
            A loop in the context of execution is essentially a backward jump to an instruction or a logical position (e.g., a previously seen program counter). The meta-tracer, lacking any intrinsic semantic knowledge of the language interpreter it is running, cannot inherently detect such loops occurring within the interpreted user program. To overcome this, meta-tracing frameworks expose an API through which the language interpreter explicitly communicates the occurrence and location of loops within the user program being interpreted. Specifically, the interpreter defines a logical \emph{program counter} composed of interpreter-specific variables. For instance, a bytecode interpreter might represent this counter by combining the current bytecode index with other execution-specific information. When the meta-tracer repeatedly observes the same logical program counter value, it infers the presence of a loop within the user program.

        \paragraph{}%
            For more fine-grained control of the traced execution paths, the language interpreter employs a set of annotations and hints to guide the meta-tracer as it executes the user program. Two of the most crucial annotations are \emph{jit\_merge\_point}, marking stable loop headers, and \emph{can\_enter\_jit}, indicating potential backward jumps. The logical program counter used for detecting loops is built from what are known as \emph{green} variables, which remain constant across multiple iterations, while other interpreter state variables that may change frequently are classified as \emph{red} variables. Both sets of variables, along with these hints, are registered with a \emph{JitDriver}, an object acting as a mediator between the interpreter and the global meta-tracer. An interpreter may define multiple JitDrivers, each with its own logical program counter, although there remains a single unified meta-tracer. We discuss this interplay further in \secref{section:hot-branches}, when addressing advanced trace optimizations.

        \paragraph{}%
            In this dissertation, we will study a concrete instance of a language interpreter built using meta-tracing: Pycket, an implementation of the Racket language built on the RPython framework. We will introduce Pycket in detail in the following section. Due to the use of meta-tracing techniques, Pycket’s tracing JIT compiler specifically captures and optimizes hot loops found within the user-level Racket code, rather than within its own interpreter logic. To understand this clearly, we will now examine concretely how an interpreter communicates loop information to a meta-tracer, as illustrated by \figref{fig:pycket-annotated-cek}.

        \paragraph{}%
            In this dissertation, we focus primarily on Pycket, an interpreter for the Racket language initially developed as a rudimentary implementation using RPython’s meta-tracing framework. Throughout the study, we progressively transform Pycket from this initial interpreter into a full-scale implementation of Racket, specifically by self-hosting Racket on top of Pycket itself. This evolution is facilitated by the meta-tracing approach, as Pycket's tracing JIT effectively identifies and optimizes hot loops in the user-level Racket code, rather than within the CEK interpreter logic. To understand concretely how such loop detection and optimization occurs, we next illustrate how the interpreter informs the meta-tracer about loops, as depicted in \figref{fig:pycket-annotated-cek}, as an example usage of meta-tracing hints.

        \inputSub{rpython}{pycket-annotated-cek}

        \paragraph{}%
            Concretely, the Pycket interpreter defines a dedicated \texttt{JitDriver}, explicitly specifying green and red variables to inform the meta-tracer about user-level loops. In Pycket’s CEK machine, the green variables consist primarily of the AST node currently being evaluated and a \texttt{came\_from} indicator, which together help encode recursive calls—the only way to make loops. Pycket’s interpreter further employs annotations such as \texttt{jit\_merge\_point} and \texttt{can\_enter\_jit} to explicitly mark stable loop entry points and potential backward jumps, respectively. These details are shown in \figref{fig:pycket-annotated-cek}, and we will further examine their operation in detail in the following section (\secref{section:pycket-primer}).

        \paragraph{}%
            Having described Pycket’s use of meta-tracing annotations, we now transition to exploring its internal core architecture. Specifically, we will discuss how its interpreter is built around the CEK abstract machine, a foundational design component that remains constant throughout our study—even as we modify and extend Pycket’s broader architecture during the transition to a self-hosted Racket implementation.

    \section[\texorpdfstring{Pycket: A Rudimentary Racket Interpreter Built on RPython Framework}{Pycket Primer}]{Pycket: A Rudimentary Racket Interpreter Built on RPython Framework}
        \label{section:pycket-primer}

        \paragraph{}%
            Pycket was initially developed in 2014 as a high-performance tracing JIT compiler for the Racket language. From the start, Pycket aimed at efficiently supporting advanced Racket features such as contracts, continuations, structures, and dynamic binding, demonstrating significant performance improvements over existing compilers at the time, including Racket's own JIT compiler~\cite{pycketmain}. It was later shown that Pycket could remove nearly 90\% of the overhead associated with sound gradual typing~\cite{pycketmain2}. In this section, we provide a primer on Pycket's core fundamentals, focusing on the RPython framework upon which it is built, its CEK-based abstract machine, and the process of running Racket programs.

        \paragraph{}%
            RPython (Restricted Python) is a statically typed, object-oriented subset of Python, originally created during the development of the PyPy project~\cite{pypy06}. By restricting Python’s dynamic features, RPython enables type inference on programs, allowing efficient compilation to lower-level languages like C. Notably, RPython disallows mixing variable types at the same program location, ensuring all types are consistent and inferable. It also prohibits runtime reflection (such as modifying classes at runtime), closures, and assumes global and class-level bindings are constants~\cite{rpython07,rpython09}.

        \inputSub{rpython}{rpython-toolchain-diagram}

        \paragraph{}%
            The RPython framework is a toolchain designed to translate interpreters written in RPython into efficient low-level languages, typically C. As illustrated in \figref{fig:rpython-toolchain}, this translation involves several phases, each with its own type system and type inference mechanism. Initially, the RPython program is loaded into a Python runtime environment, and control-flow graphs (CFGs) are generated through abstract interpretation. Next, the annotator infers types and annotates variables at each CFG node, reflecting possible Python object types. Following annotation, the RTyper (RPython typer) lowers annotations and operations into types compatible with the target language, acting as a bridge to code generation. Subsequent optional backend optimizations, such as inlining, malloc removal, and escape analysis, refine the resulting code. Because RPython programs assume automatic memory management, a garbage collector is inserted into the program before final translation into a binary executable~\cite{rpython07,pypy06,pypy08}.

        \paragraph{}%
            Pycket was built using this RPython framework. In its initial design, shown in \figref{fig:old-pycket-workflow-vertical}, Pycket utilized the Racket executable to read and expand modules into fully expanded code, after which Pycket itself converted the expanded code into its own AST representation for execution within its interpreter loop~\cite{samth:11,bolzMetatracingMakesFast2014,pycketmain}. This interpreter is based on the CEK abstract machine, defined by the state triple $\langle e,\rho,\kappa\rangle$ (where $e$ is the AST representing the control, $\rho$ is the environment mapping variables to values, and $\kappa$ is the continuation, capturing the context of the computation)~\cite{felleisen87}.

        \inputSub{rpython}{old-pycket-runtime-flow-diagram}

        \paragraph{}%
            In its original design, Pycket leverages Racket’s macro expander~\cite{flatt:2002} to preprocess macros, simplifying input programs into a small set of core forms that Pycket's runtime can directly handle~\cite{samth:11}. Initially, this macro expansion is performed externally by invoking the Racket executable, which expands modules and serializes them into JSON-encoded files for Pycket to read, convert into ASTs, and evaluate directly. As we will discuss in detail in \chapterRef{chapter:linklets}, we shift the macro expansion process into Pycket's runtime itself by importing Racket's expander linklet into Pycket's runtime. This shift is the key step that enables Pycket to become a fully self-hosted and independent execution environment for Racket.

        \paragraph{}%
            Once a module is expanded to core Racket forms, Pycket performs two transformations on its AST. First, it converts the AST into A-normal form (ANF), ensuring that all complex expressions are explicitly named, simplifying subsequent evaluation and optimization~\cite{danvy:93,flanagan:93}. Second, all mutable variables targeted by set! operations are transformed into heap-allocated cells. This approach, common in Lisp-family languages, enables environments to be immutable mappings from variables to values and allows each AST node to carry its static environment explicitly \cite{pycketmain}.

        \paragraph{}%
            Pycket implements approximately 1500 of Racket’s more than 2000 primitive operations and values. These primitives range from numeric computations, including bignums, rationals, and complex numbers, to regular expression handling, and sophisticated input/output abstractions. A substantial portion of Pycket’s implementation consists of these primitives, which are further extended by bootstrapping linklets discussed in \chapterRef{chapter:linklets}.

        \paragraph{}%
            Pycket implements approximately 1500 of Racket’s more than 2000 primitive operations and values. These primitives range from numeric computations, including bignums, rationals, and complex numbers, to sophisticated input/output abstractions and regular expression handling. Some of these primitive sets, notably input/output and regular expressions, are subsequently replaced by implementations supplied directly from Racket through bootstrapping linklets, as discussed in \chapterRef{chapter:linklets}. This approach allows Pycket to extend its runtime functionality using primitives provided as Racket code by the language itself.

        \paragraph{}%
            Additionally, Pycket employs some advanced techniques such as hidden classes, a technique borrowed from prototype-based object systems, to implement proxies efficiently. Proxies, crucial for enforcing soundness in gradual type systems and Racket’s contract systems, benefit significantly from Pycket’s approach, enhancing performance in both gradually typed and contract-heavy programs~\cite{findler-felleisen:2002,pycketmain2}. Moreover, Pycket naturally benefits from type specialization through tracing, eliminating the need for a separate type-feedback pass required by JITs such as method-based JIT compilers~\cite{typeSpecial:2009}.

        \begin{figure}[t]
        \centering
        %--------------------------------------------------------------------
        % Syntax of expressions and continuations
        \[
        \begin{array}{lcl}
            e &::=& x \mid \lambda x.\,e \mid e\,e \\[4pt]
            \kappa &::=& [\,]
                    \mid \mathsf{arg}(e,\rho)::\kappa
                    \mid \mathsf{fun}(v,\rho)::\kappa
        \end{array}
        \]

        %--------------------------------------------------------------------
        % Transition rules of the CEK machine
        \[
        \begin{array}{rcl}
            \langle x,\rho,\kappa \rangle
            &\longmapsto&
            \langle \rho(x),\rho,\kappa \rangle \\[6pt]

            \langle (e_1\,e_2),\rho,\kappa \rangle
            &\longmapsto&
            \langle e_1,\rho,\mathsf{arg}(e_2,\rho)::\kappa \rangle \\[6pt]

            \langle v,\rho,\mathsf{arg}(e,\rho')::\kappa \rangle
            &\longmapsto&
            \langle e,\rho',\mathsf{fun}(v,\rho)::\kappa \rangle \\[6pt]

            \langle v,\rho,\mathsf{fun}(\lambda x.\,e,\rho')::\kappa \rangle
            &\longmapsto&
            \langle e,\rho'[x \mapsto v],\kappa \rangle
        \end{array}
        \]
        \caption{The CEK machine for the $\lambda$‑calculus \cite{pycketmain}.}
        \label{fig:cek-formal}
        \end{figure}

        \paragraph{}%
            After the user program is fully converted into its final AST form, Pycket evaluates it using the CEK abstract machine, whose semantics are given by the transition rules in Figure~\ref{fig:cek-formal}. A CEK state is represented as $\langle e,\rho,\kappa\rangle$, where $e$ is the expression currently being evaluated (the control), $\rho$ maps variable identifiers to their associated values (the environment), and $\kappa$ holds the computation context (the continuation). The continuation consists of frames that track pending evaluations, which come in two forms: an $\mathsf{arg}(e,\rho)$ frame captures an argument that is awaiting evaluation, and a $\mathsf{fun}(v,\rho)$ frame represents a function that is ready to receive its argument. Evaluating a variable involves simply looking up its value in the environment. Applying a function involves two steps—first evaluating the function itself and then evaluating its argument. The final step applies the evaluated function to its evaluated argument. Importantly, the CEK machine creates no additional continuation frames for tail calls, thus ensuring proper tail-calls \cite{pycketmain}.

        \paragraph{}%
            The corresponding RPython code implementing the CEK loop (\figref{fig:pycket-annotated-cek}) on Pycket continuously transforms CEK triples $(\text{ast}, \text{env}, \text{cont})$ into new states by invoking the \texttt{interpret} method on the current AST node. This iterative process proceeds until an empty continuation is encountered, at which point a \texttt{Done} exception is triggered, storing and returning the computation’s final result.

        \paragraph{}%
            Pycket help the meta-tracer identify loops using two complementary mechanisms: the \emph{two-state tracking} and a dynamic \emph{call-graph} approach. In two-state tracking, Pycket filters out false loops by associating potential loop points with caller-callee pairs, represented by the current and previous AST nodes. Without additional context, the JIT may incorrectly recognize various control-flow points as loops, and the two-state approach effectively reduces such false positives. However, this mechanism alone struggles with complex recursive patterns, such as indirect recursion or proxy-based calls. To complement two-state tracking, Pycket employs a dynamic call-graph technique. This method incrementally constructs a call graph at runtime, with nodes representing invoked functions and edges representing caller-callee relationships. Whenever adding a new edge creates a cycle, Pycket recognizes the target function as a loop header suitable for tracing. The combination of these two approaches ensures robust loop detection and optimized trace generation, particularly in programs with intricate control flows involving recursion and higher-order functions~\cite{pycketmain, pycketmain2}.

        \paragraph{}%
            While we will explore Pycket’s performance characteristics further in \secref{section:pycket-performance-characteristics}, we first turn to relevant RPython trace optimizations, which will provide the context necessary for a detailed technical discussion about performance considerations in \chapterRef{chapter:problem}.

    \section{Trace Optimizations \& Runtime Feedback}

    \begin{paragraph-here}
        Traces are the real performance currency of a tracing JIT compiler.

        Good trace quality is a thing.
    \end{paragraph-here}


RPython’s trace optimizer includes a suite of standard compiler optimizations, such as commonsubexpression elimination, copy propagation, constant folding, and many others (Ardo l. 2012). One advantage of trace compilation for optimization is that the control-flow graph of a trace is a straight line. Trace optimizations and their supporting analyses can be implemented in two passes over the trace, one forward pass and one backward pass.

Inlining Inlining is a vital compiler optimization for high-level languages, both functional and object-oriented. In a tracing JIT compiler such as RPython, inlining comes for free from tracing (Gal et al. 2006). A given trace will include the inlined code from any functions called during tracing. This includes Racket-level functions as well as runtime system functions (Bolz et al. 2009). The highlyaggressive inlining produced by tracing is one of the keys to its successful performance: it eliminates function call overhead and exposes opportunities for other optimizations.

Loop-invariant Code Motion Loop-invariant code motion is implemented in RPython particularly simple way, by peeling off a single iteration of the loop, and then performing its standard suite of forward analyses to optimize the loop further (Ardo 2012).

Allocation Removal The CEK machine allocates a vast quantity of objects which would appear in the heap without optimization. This ranges from the tuple holding the three components of the machine state, to the environments holding each variable, to the continuations created for each operation.

        \begin{paragraph-here}
            warmup is a thing
        \end{paragraph-here}

        \begin{paragraph-here}
            introduce and explain promote, give an example from Pycket

            promote, turns arbitrary variables into constants
        \end{paragraph-here}

        \begin{paragraph-here}
            talk about escape analysis, and introduce and explain trace-elidable, give an example from pycket

            @elidable, it's like memoization of functions, for traces.

            a function is trace-elidable, if during the execution of the program, successive calls to the function with identical arguments always return the same result.
        \end{paragraph-here}
