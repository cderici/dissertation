% Tracing JITs and Meta-Tracing Frameworks
% Tracing Just-in-time (JIT) Compilers and Meta-tracing
\chapter[\texorpdfstring{TRACING JUST-IN-TIME (JIT) COMPILERS \& META-TRACING}
                          {RPython \& Meta-tracing}]{TRACING JUST-IN-TIME (JIT) COMPILERS \& META-TRACING}
    \label{chapter:rpython}

    \begin{chaptersynopsis}

        We do three things in this chapter:

        1. Thesis statement refers to meta-tracing JITs, so we explain what a meta-tracing JIT compiler is.

        2. Introduce RPython as a language, and a framework that Pycket is built on.

        3. Provide context for meta-tracing-related technical discussions in the rest of the study. traces, warmup, optimizations, runtime feedback, etc.

        \vspace{2em}

        Sections:
		\begin{itemize}
			\item Meta-tracing \& RPython Framework

                What problem does PyPy solve? How do they capture loops in user programs? JIT drivers, interpreter hints, green/red variables etc.
			\item Pycket: A Rudimentary Racket Interpreter Built on RPython Framework

                Primer on Pycket's fundamentals. It's language RPython, CEK core, and how it is built.
			\item Trace Optimizations \& Runtime Feedback

                Traces are the real performance currency. Relevant optimizations and runtime feedback (e.g promote, escape analysis, warmup, etc.).
		\end{itemize}
    \end{chaptersynopsis}

    \paragraph{}%
        In this chapter, we have three main objectives. First, we explain the concept of a meta-tracing Just-in-Time (JIT) compiler, clearly connecting it to our thesis statement presented earlier, that efficient self-hosting of full-scale functional languages is achievable using meta-tracing JIT compilation. Second, we introduce the RPython framework, detailing its role as the foundational technology upon which Pycket is constructed, including aspects of translation and toolchain workflow. Lastly, we outline key concepts related to RPython and meta-tracing—such as trace formation, optimization strategies, and runtime feedback—which serve as essential context for technical discussions and evaluations relating to performance in \chapterRef{chapter:problem} and \chapterRef{chapter:solution}.

    \paragraph{}%
        A Just-in-Time (JIT) compiler dynamically compiles frequently executed parts of a program at runtime, interleaving compilation with interpretation. Unlike Ahead-of-Time (AOT) compilers, which compile the entire program beforehand, JIT compilers selectively target hot paths identified through low-overhead profiling. When the interpreter repeatedly executes a particular sequence of instructions, the JIT compiler pauses interpretation momentarily, compiles and optimizes this sequence, and subsequently executes the optimized code whenever the same instruction path recurs \cite{dynamo}.

    \paragraph{}%
        JIT compilers have proven particularly effective for dynamic language virtual machines (VMs), generally adopting either method-based or trace-based approaches. Method-based compilers optimize frequently invoked methods, whereas trace-based compilers focus on frequently executed loops \cite{survey:05,jit-history:03}. This dissertation specifically focuses on trace-based compilation. Tracing JITs generate optimized machine code by tracing and compiling execution paths under two fundamental assumptions\cite{pypy-main}:

    \begin{enumerate}
        \item Programs spend most of their execution time in loops.
        \item Iterations of the same loop often follow similar execution paths.
    \end{enumerate}

    \inputSub{rpython}{fig-trace}

    \paragraph{}%
        To exploit these assumptions, tracing JIT compilers identify certain execution paths as \emph{hot loops}. Upon detecting a hot loop, the interpreter pauses evaluation to compile this loop into a \emph{trace}, subsequently using the optimized trace whenever the same path is executed again. A trace is a linear sequence of instructions with a single entry and potentially multiple exit points. \figref{fig:trace} illustrates a simplified trace, with inputs $p0$ and $p1$, comprising a preamble (due to loop unrolling) and an inner loop that jumps back to itself. \emph{Guards} within a trace define its exit points, performing runtime checks to ensure that conditions remain consistent with the initial tracing context and handling conditions for loop termination or deviation.

    % \begin{paragraph-here}
    %     The tracer is a component of a JIT compiler that is responsible for tracing the execution of the program and generating traces.
    % \end{paragraph-here}

    % \begin{paragraph-here}
    %     Whenever a hot loop is detected, the tracer generates a trace that captures the execution of the loop.

    %     For a given program, a time that it takes for the JIT to find and trace all the hot loops is called the warmup time.
    % \end{paragraph-here}

    \section{Meta-tracing \& RPython Framework}
        \begin{mainpoint}
            The problem that PyPy solves:

            Rather than the loops in the interpreter being evaluated, meta-tracing manages to capture the hot loops in the user program being interpreted.
        \end{mainpoint}

        \begin{paragraph-here}
            If the program that's being JIT compiled is a language interpreter that's interpreting a user program (i.e. if you apply a tracing JIT to a language interpreter), then a naive JIT compiler will try to capture the hot loops in the given program, the language interpreter itself, rather than in the user program being interpreted.

            [Define the terms we'll use later: there's the language interpreter, the tracing interpreter, and the user program being interpreted by the language interpreter. The language interpreter is the interpreter for the language that we want this system to implement. The tracing interpreter (i.e. the meta-tracer) is the interpreter that holds hands with the language interpreter and tracks its steps as the language interpreter interprets the given user program, stopping it every now and then to compile and store traces.]
        \end{paragraph-here}

        \begin{paragraph-here}
            the main dispatch loop in a language interpreter is The most significant loop. For a functional programming language, this would typically be a recursive descent loop that evaluate a sub-expression at each iteration. But this violates the second assumption of the tracing JITs where we assume several iterations of a hot loop take similar code paths. Because it is highly unlikely that a language interpreter keeps evaluating the same operation/expression over and over again.
        \end{paragraph-here}

        \begin{paragraph-here}
            Besides, by its nature, tracing captures the most essential part of the program being executed, and on an interpreter interpreting a user program, the main dispatch loop of the interpreter is not the most essential part, the hot loops within the user program are the most essential parts. So we really want the JIT compiler to capture the hot loops in the user program, rather than the interpreter itself. Meta-tracing defined in PyPy solves exactly that\cite{pypy-main}. It manages to capture the hot loops in the user program being interpreted, rather than the interpreter itself.
        \end{paragraph-here}

        \begin{paragraph-here}
            [What's a loop? (backwards jump) How is it detected in a language interpreter?]


            Fundamentally, there's no way for the tracing interpreter (the meta-tracer) to know the semantics of the language interpreter, without an explicit internal coding language or something. Therefore, when the meta-tracer traces the language interpreter, it can't know when a loop in the user program is happening. For this purpose, the tracing interpreter exposes an API that the language interpreter uses to communicate when and where a loop might be forming in the user program.

            A loop in a program is essentially a backwards jump to a previously executed instruction. To detect a loop in the user program, the meta-tracer allows the language to define its own logical \emph{program counter} using its own variables. For example in a bytecode interpreter, the program counter might be a compound value that consists of the bytecode of the operation and a global variable. This way, when the meta-tracer repeatedly sees the same value in the program counter that the language interpreter defined, it knows that the user code is looping.
        \end{paragraph-here}

        \begin{paragraph-here}
            For a more fine-grained control of the computation paths being traced, the language interpreter is armed with some functionality and some hints to guide the meta-tracer along the way (of interpreting the user program), most importantly the \emph{jit\_merge\_point}, and \emph{can\_enter\_jit}. These
        \end{paragraph-here}

        \begin{paragraph-here}
            In this study, we'll work with an interpreter for Racket language, namely Pycket, built as a meta-tracing JIT compiler. We'll introduce it properly in the next section.

            Because it's a meta-tracing compiler, when we evaluate a Racket program on Pycket, Pycket's meta-tracing JIT compiler captures the hot loops in the Racket program, rather than the CEK interpreter in RPython itself.

            Let's see concretely how the interpreter informs the meta-tracer for detecting loops in the user program.
        \end{paragraph-here}


        \begin{paragraph-here}
            So we define a JITdriver, and give it the green/red variables, and annotate the interpreter with  can-enter-jit, jit-merge-point, JITDriver, green-red variables.. As shown in \figref{fig:pycket-annotated-cek}. We'll talk about how it all works in \secref{section:pycket-primer}.
        \end{paragraph-here}

        \inputSub{rpython}{pycket-annotated-cek}

        \begin{paragraph-here}
            Segue into Pycket internals. Let's see how the interpreter is built around this CEK that you see.
        \end{paragraph-here}


    \section{Pycket: A Rudimentary Racket Interpreter Built on RPython Framework}
        \label{section:pycket-primer}

        \begin{paragraph-here}
            Pycket, first designed as a high-performance tracing JIT compiler for Racket, supporting wide variety of complex language features such as contracts, continuations, structures, dynamic binding, and more. It's shown to outperform existing compilers at the time (2015) including Racket's own JIT \cite{pycketmain}, later shown to eliminate approximately \%90 of the sound gradual typing overhead \cite{pycketmain2}. In this section we provide a primer on Pycket's fundamentals. It's language RPython, CEK core, and how it is built, etc.
        \end{paragraph-here}

        \begin{paragraph-here}
            What's RPython?

            Originally created for PyPy.
            RPython (Restricted Python) is a statically typed, object-oriented
            proper subset of Python that is designed during PyPy's development
            \cite{pypy06}. It restricts Python in a way that enables
            type-inference on RPython programs allowing an efficient conversion to
            a lower language such as C. The restrictions are mainly the dynamic
            features of Python and commonly listed as \cite{rpython07,rpython09};
            no mix types at the same location (all variables need to be type
            consistent and infereable), run-time reflection is not supported
            (i.e. changing method of classes at run-time), no closures, global and
            class bindings are assumed to be constants etc.
        \end{paragraph-here}

        \inputSub{rpython}{rpython-toolchain-diagram}

        \begin{paragraph-here}
            What's RPython framework? What does it do? How does it do it? It generates JIT compilers from given interpreters.

            The RPython framework takes RPython code and converts it to a chosen
            lower-level language, most commonly C. Given an interpreter written in
            RPython, the toolchain translates it to the target language by
            lowering it in numerous phases, where each phase has its own type
            system and a generic type inference engine. Because the RPython is a
            subset of Python, the entire process can use a Python run-time. The
            general translation process can be described as follows. It starts
            with loading the program into the run-time and get the function
            objects in memory as inputs. Using these function objects, the
            framework generates by abstract intepretation the control flow graphs
            for these functions that will be processed at further transformation
            steps. Then the annotator acts as a high-level type inference engine
            and assigns ``annotations'' to each variable at each flow graph. These
            annotations basically denotes the possible Python objects a variable
            might contain at the run-time. After the whole program is annotated,
            RTyper (RPython typer) takes control and starts lowering the
            annotations and some operations into the lower types and operations
            that would make sense to the targeted environment (e.g. structs,
            pointers, arrays in case of C), acting as a bridge between the
            high-level annotator and the low-level code generation. After RTyping,
            some optional backend optimizations are applied, such as inlining,
            malloc removal and escape analysis. Note that like Python, RPython is
            garbage collected, however, C is not. Therefore at this point a
            garbage collector is inserted into the program as well. Finally the
            typed and lowered flow-graphs are inputted into the code generation
            and the binary is generated. \cite{rpython07, pypy06, pypy08}
        \end{paragraph-here}



        \begin{paragraph-here}
            Enter Pycket, generated by that RPython framework.

            Pycket is first designed in 2014 as a high-performance JIT compiler
            for Racket, generated using the RPython meta-tracing framework
            \cite{bolz14-racket}. In its original design, as shown in
            \figref{fig:old-pycket}, Pycket relies on the Racket executable to
            read and fully expand a given module\cite{samth:11}, and then
            generates RPython AST for it and evaluates it within the interpreter
            loop \cite{pycket15}. The language interpreter is based on the CEK
            abstract machine and has the state $\langle e, \rho, \kappa \rangle$ ($e$ : control
            (program AST), $\rho$ : environment, $\kappa$ : continuation)
            \cite{felleisen87}. \figref{fig:cek} shows the transition rules and
            how the CEK-loop is implemented in Pycket. The interpreter loop
            continuously reduces the CEK triple until an empty continuation is
            reached, which triggers a \emph{Done} exception that returns the
            results.
        \end{paragraph-here}

        \inputSub{rpython}{old-pycket-runtime-flow-diagram}

        \begin{figure-here}
            OLD Pycket runtime flow diagram figure.
        \end{figure-here}

        Macros \& Modules

        Pycket uses Racket’s macro expander (Flatt 2002) to evaluate macros, thereby reducing Racket programs to just a few core forms implemented by the runtime system (TobinHochstadt et al. 2011).

        Assignment Conversion and ANF

        Once a module is expanded to core Racket and parsed from the serialized representation, Pycket performs two transformations on the AST. First, the program is converted to A-normal form (ANF), ensuring that all non-trivial expressions are named (Danvy 1991; Flanagan et al. 1993). For example [SIMPLE FIGURE]

        Next, we convert all mutable variables (those that are the target of set!) into heap-allocated cells. This is a common technique in Lisp systems, and Racket performs it as well. This approach allows environments to be immutable mappings from variables to values. Additionally, each AST node stores its static environment; see section 5.1 for how this is used.

        Primitives and Values

        Racket comes with over 1,400 primitive functions and values, of which Pycket implements nearly 900. These range from numeric operations, where Pycket implements the full numeric tower including bignums, rational numbers, and complex numbers, to regular expression matching, to input/output including a port abstraction. As of this writing, more than half of the non-test lines of code in Pycket implement primitive functions.

        hidden classes, type specializations

        CEK States and Representation

        With our program in its final form, we now execute it using the CEK machine.

        \begin{paragraph-here}
            We saw the CEK loop before (figure reference). This CEK loop evaluates Racket kernel. The Racket binary is applied to a given Racket program to expand it into \racketcode{\#\%kernel}.
        \end{paragraph-here}


        \begin{paragraph-here}
               How does Pycket help find loops? two-state representation and call-graph.

               section 4 in \cite{pycketmain}.
        \end{paragraph-here}


        \begin{paragraph-here}
            We'll talk about this Pycket's overall performance characteristics in \secref{section:pycket-performance-characteristics}. let's first talk about relevant RPython trace optimizations to provide a context for technical discussions about performance in \chapterRef{chapter:problem}.
        \end{paragraph-here}


    \section{Trace Optimizations \& Runtime Feedback}

    \begin{paragraph-here}
        Traces are the real performance currency of a tracing JIT compiler.

        Good trace quality is a thing.
    \end{paragraph-here}


RPython’s trace optimizer includes a suite of standard compiler optimizations, such as commonsubexpression elimination, copy propagation, constant folding, and many others (Ardo l. 2012). One advantage of trace compilation for optimization is that the control-flow graph of a trace is a straight line. Trace optimizations and their supporting analyses can be implemented in two passes over the trace, one forward pass and one backward pass.

Inlining Inlining is a vital compiler optimization for high-level languages, both functional and object-oriented. In a tracing JIT compiler such as RPython, inlining comes for free from tracing (Gal et al. 2006). A given trace will include the inlined code from any functions called during tracing. This includes Racket-level functions as well as runtime system functions (Bolz et al. 2009). The highlyaggressive inlining produced by tracing is one of the keys to its successful performance: it eliminates function call overhead and exposes opportunities for other optimizations.

Loop-invariant Code Motion Loop-invariant code motion is implemented in RPython particularly simple way, by peeling off a single iteration of the loop, and then performing its standard suite of forward analyses to optimize the loop further (Ardo 2012).

Allocation Removal The CEK machine allocates a vast quantity of objects which would appear in the heap without optimization. This ranges from the tuple holding the three components of the machine state, to the environments holding each variable, to the continuations created for each operation.

        \begin{paragraph-here}
            warmup is a thing
        \end{paragraph-here}

        \begin{paragraph-here}
            introduce and explain promote, give an example from Pycket

            promote, turns arbitrary variables into constants
        \end{paragraph-here}

        \begin{paragraph-here}
            talk about escape analysis, and introduce and explain trace-elidable, give an example from pycket

            @elidable, it's like memoization of functions, for traces.

            a function is trace-elidable, if during the execution of the program, successive calls to the function with identical arguments always return the same result.
        \end{paragraph-here}
