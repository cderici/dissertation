% Tracing JITs and Meta-Tracing Frameworks
% Tracing Just-in-time (JIT) Compilers and Meta-tracing
\chapter[\texorpdfstring{META-TRACING JUST-IN-TIME (JIT) COMPILERS}
                          {2. RPython \& Meta-tracing}]{META-TRACING JUST-IN-TIME (JIT) COMPILERS}
    \label{chapter:rpython}

    \begin{chaptersynopsis}[Chapter Synopsis]
        \footnotesize

        We do three things in this chapter:

        1. Thesis statement refers to meta-tracing JITs, so we explain what a meta-tracing \gls{jit} compiler is.

        2. Introduce RPython as a language, and as a framework that Pycket is built on.

        3. Provide context for meta-tracing-related technical discussions in the rest of the study. traces, warmup, optimizations, runtime feedback, etc.

        \vspace{2em}

        Sections:
		\begin{itemize}
			\item Meta-tracing \& RPython Framework

                What problem does PyPy solve? How do they capture loops in user programs? \gls{jit} drivers, interpreter hints, green/red variables etc.
			\item Pycket: A Rudimentary Racket Interpreter Built on RPython Framework

                Primer on Pycket's fundamentals. It's language RPython, CEK core, and how it is built.
			\item Trace Optimizations \& Runtime Feedback

                Traces are the real performance currency. Relevant optimizations and runtime feedback (e.g promote, escape analysis, warmup, etc.).
		\end{itemize}
    \end{chaptersynopsis}

    \paragraph{}%
        In this chapter, we have three main objectives. First, we explain the concept of a meta-tracing \gls{jit} compiler, clearly connecting it to our thesis statement presented earlier, that efficient self-hosting of full-scale functional languages is achievable using meta-tracing \gls{jit} compilation. Second, we introduce the RPython framework, detailing its role as the foundational technology upon which Pycket is constructed, including aspects of translation and toolchain workflow. Lastly, we outline key concepts related to RPython and meta-tracing—such as trace formation, optimization strategies, and runtime feedback—which serve as essential context for technical discussions and evaluations relating to performance in \chapterRef{chapter:problem} and \chapterRef{chapter:solution}.

    \paragraph{}%
        A \gls{jit} compiler dynamically compiles frequently executed parts of a program at runtime, interleaving compilation with interpretation. Unlike \gls{aot} compilers, which compile the entire program beforehand, \gls{jit} compilers selectively target hot paths identified through low-overhead profiling. When the interpreter repeatedly executes a particular sequence of instructions, the \gls{jit} compiler pauses interpretation momentarily, compiles and optimizes this sequence, and subsequently executes the optimized code whenever the same instruction path recurs \cite{dynamo}.

    \paragraph{}%
        \gls{jit} compilers have proven particularly effective for dynamic language virtual machines (VMs), generally adopting either method-based or trace-based approaches. Method-based compilers optimize frequently invoked methods, whereas trace-based compilers focus on frequently executed loops \cite{survey:05,jit-history:03}. This dissertation specifically focuses on trace-based compilation. Tracing \glspl{jit} generate optimized machine code by tracing and compiling execution paths under two fundamental assumptions\cite{pypy-main}:

    \begin{enumerate}
        \item Programs spend most of their execution time in loops.
        \item Iterations of the same loop often follow similar execution paths.
    \end{enumerate}

    \inputSub{rpython}{fig-trace}

    \paragraph{}%
        To exploit these assumptions, tracing \gls{jit} compilers identify certain execution paths as \emph{hot loops}. Upon detecting a hot loop, the interpreter pauses evaluation to compile this loop into a \emph{trace}, subsequently using the optimized trace whenever the same path is executed again. A trace is a linear sequence of instructions with a single entry and potentially multiple exit points. \figref{fig:trace} illustrates a simplified trace, with inputs $p0$ and $p1$, comprising a preamble (due to loop unrolling) and an inner loop that jumps back to itself. \emph{Guards} within a trace define its exit points, performing runtime checks to ensure that conditions remain consistent with the initial tracing context and handling conditions for loop termination or deviation.

    % \begin{paragraph-here}
    %     The tracer is a component of a JIT compiler that is responsible for tracing the execution of the program and generating traces.
    % \end{paragraph-here}

    % \begin{paragraph-here}
    %     Whenever a hot loop is detected, the tracer generates a trace that captures the execution of the loop.

    %     For a given program, a time that it takes for the JIT to find and trace all the hot loops is called the warmup time.
    % \end{paragraph-here}

    \section[\texorpdfstring{Tracing vs Meta-tracing in JIT Compilation}{Tracing vs Meta-tracing}]{Tracing vs Meta-tracing in JIT Compilation}
    \label{section:tracing-meta-tracing}

        \paragraph{}%
            When a tracing \gls{jit} compiler is applied to a language interpreter, it typically ends up tracing the loops of the interpreter itself—rather than the loops in the user program that the interpreter is executing. To understand why this is a problem, we need to distinguish three roles: the language interpreter (the implementation of the language semantics), the tracing interpreter (or meta-tracer), and the user program (the input being evaluated by the language interpreter). The meta-tracer effectively executes the language interpreter step-by-step, carrying out its operations and observing their effects. Without additional guidance, it has no visibility into the boundaries or semantics of the user-level code being interpreted, and so it naturally captures hot paths in the language interpreter logic rather than in the user program.

        \paragraph{}%
            Within a language interpreter, the primary dispatch loop is typically the most significant loop. For a functional programming language, this usually manifests as a recursive descent loop, evaluating sub-expressions iteratively. However, while evaluating a user program, this loop structure in the language interpreter fundamentally violates one of the core assumptions of tracing \gls{jit} compilers—that several iterations of a hot loop tend to follow similar code paths—because it is uncommon for an interpreter to repetitively evaluate the exact same operation or expression in succession. Rather, loops in the user program often unfold as different branches taken through the language interpreter’s dispatch logic.

        \paragraph{}%
            Furthermore, by nature, tracing captures the execution paths where a program spends most of its running time. In the scenario of applying a tracing \gls{jit} to a language interpreter, however, the interpreter's dispatch loop itself is typically not the most essential computational path. Instead, the loops within the user program represent the true computational hot spots. Thus, what we seek from a tracing \gls{jit} in this context is to identify and optimize the loops of the user program, not the loops within the interpreter. This is precisely the problem addressed by \emph{meta-tracing}, as defined in PyPy: it successfully redirects the tracing \gls{jit} compiler to identify and optimize the hot loops of the user program, rather than the interpreter logic itself \cite{pypy-main}.

        \paragraph{}%
            A loop in the context of execution is essentially a backward jump to an instruction or a logical position (e.g., a previously seen program counter). The meta-tracer, lacking any intrinsic semantic knowledge of the language interpreter it is running, cannot inherently detect such loops occurring within the interpreted user program. To overcome this, meta-tracing frameworks expose an \gls{api} through which the language interpreter explicitly communicates the occurrence and location of loops within the user program being interpreted. Specifically, the interpreter defines a logical \emph{program counter} composed of interpreter-specific variables. For instance, a bytecode interpreter might represent this counter by combining the current bytecode index with other execution-specific information. When the meta-tracer repeatedly observes the same logical program counter value, it infers the presence of a loop within the user program.

        \paragraph{}%
            For more fine-grained control of the traced execution paths, the language interpreter employs a set of annotations and hints to guide the meta-tracer as it executes the user program. Two of the most crucial annotations are \emph{jit\_merge\_point}, marking stable loop headers, and \emph{can\_enter\_jit}, indicating potential backward jumps. The logical program counter used for detecting loops is built from what are known as \emph{green} variables, which remain constant across multiple iterations, while other interpreter state variables that may change frequently are classified as \emph{red} variables. Both sets of variables, along with these hints, are registered with a \emph{JitDriver} reflection that provided by the RPython framework, an object acting as a mediator between the interpreter and the global meta-tracer. An interpreter may define multiple JitDrivers, each with its own logical compound program counter, although there remains a single unified meta-tracer. We discuss this interplay further in \secref{section:hot-branches}, when addressing advanced trace optimizations.

        \paragraph{}%
            In this dissertation, we will study a concrete instance of a language interpreter built using meta-tracing: Pycket, an implementation of the Racket language built on the RPython framework. We will introduce Pycket in detail in the following section. Due to the use of meta-tracing techniques, Pycket’s tracing \gls{jit} compiler specifically captures and optimizes hot loops found within the user-level Racket code, rather than within its own interpreter logic. To understand this clearly, we will now examine concretely how an interpreter communicates loop information to a meta-tracer, as illustrated by \figref{fig:pycket-annotated-cek}.

        \paragraph{}%
            In this dissertation, we focus primarily on Pycket, an interpreter for the Racket language initially developed as a rudimentary implementation using RPython’s meta-tracing framework. Throughout the study, we progressively transform Pycket from this initial interpreter into a full-scale implementation of Racket, specifically by self-hosting Racket on top of Pycket itself. This evolution is facilitated by the meta-tracing approach, as Pycket's tracing \gls{jit} effectively identifies and optimizes hot loops in the user-level Racket code, rather than within the CEK interpreter logic. To understand concretely how such loop detection and optimization occurs, we next illustrate how the interpreter informs the meta-tracer about loops, as depicted in \figref{fig:pycket-annotated-cek}, as an example usage of meta-tracing hints.

        \inputSub{rpython}{pycket-annotated-cek}

        \paragraph{}%
            Concretely, the Pycket interpreter defines a dedicated \texttt{JitDriver}, explicitly specifying green and red variables to inform the meta-tracer about user-level loops. In Pycket’s CEK machine, the green variables consist primarily of the AST node currently being evaluated and a \texttt{came\_from} indicator, which together help encode recursive calls—the only way to make loops. Pycket’s interpreter further employs annotations such as \texttt{jit\_merge\_point} and \texttt{can\_enter\_jit} to explicitly mark stable loop entry points and potential backward jumps, respectively. These details are shown in \figref{fig:pycket-annotated-cek}, and we will further examine their operation in detail in the following section (\secref{section:pycket-primer}).

        \paragraph{}%
            Having described Pycket’s use of meta-tracing annotations, we now transition to exploring its internal core architecture. Specifically, we will discuss how its interpreter is built around the CEK abstract machine, a foundational design component that remains constant throughout our study—even as we modify and extend Pycket’s broader architecture during the transition to a self-hosted Racket implementation.

    \section[\texorpdfstring{Pycket Primer: A Rudimentary Interpreter Built on RPython Framework}{Pycket Primer}]{Pycket Primer: A Rudimentary Interpreter Built on RPython Framework}
        \label{section:pycket-primer}

        \paragraph{}%
            Pycket was initially developed in 2014 as a high-performance tracing \gls{jit} compiler for the Racket language. From the start, Pycket aimed at efficiently supporting advanced Racket features such as contracts, continuations, structures, and dynamic binding, demonstrating significant performance improvements over existing compilers at the time, including Racket's own \gls{jit} compiler~\cite{pycketmain}. It was later shown that Pycket could remove nearly 90\% of the overhead associated with sound gradual typing~\cite{pycketmain2}. In this section, we provide a primer on Pycket's core fundamentals, focusing on the RPython framework upon which it is built, its CEK-based abstract machine, and the process of running Racket programs.

        \paragraph{}%
            RPython (Restricted Python) is a statically typed, object-oriented subset of Python, originally created during the development of the PyPy project~\cite{pypy06}. By restricting Python’s dynamic features, RPython enables type inference on programs, allowing efficient compilation to lower-level languages like C. Notably, RPython disallows mixing variable types at the same program location, ensuring all types are consistent and inferable. It also prohibits runtime reflection (such as modifying classes at runtime), closures, and assumes global and class-level bindings are constants~\cite{rpython07,rpython09}.

        \inputSub{rpython}{rpython-toolchain-diagram}

        \paragraph{}%
            The RPython framework is a toolchain designed to translate interpreters written in RPython into efficient low-level languages, typically C. As illustrated in \figref{fig:rpython-toolchain}, this translation involves several phases, each with its own type system and type inference mechanism. Initially, the RPython program is loaded into a Python runtime environment, and \glspl{cfg} are generated through abstract interpretation. Next, the annotator infers types and annotates variables at each \gls{cfg} node, reflecting possible Python object types. Following annotation, the RTyper (RPython typer) lowers annotations and operations into types compatible with the target language, acting as a bridge to code generation. Subsequent optional backend optimizations, such as inlining, malloc removal, and escape analysis, refine the resulting code. Because RPython programs assume automatic memory management, a garbage collector is inserted into the program before final translation into a binary executable~\cite{rpython07,pypy06,pypy08}.

        \paragraph{}%
            Pycket was built using this RPython framework. In its initial design, shown in \figref{fig:old-pycket-workflow-vertical}, Pycket utilized the Racket executable to read and expand modules into fully expanded code, after which Pycket itself converted the expanded code into its own AST representation for execution within its interpreter loop~\cite{samth:11,bolzMetatracingMakesFast2014,pycketmain}. This interpreter is based on the CEK abstract machine, defined by the state triple $\langle e,\rho,\kappa\rangle$ (where $e$ is the AST representing the control, $\rho$ is the environment mapping variables to values, and $\kappa$ is the continuation, capturing the context of the computation)~\cite{felleisen87}.

        \inputSub{rpython}{old-pycket-runtime-flow-diagram}

        \paragraph{}%
            In its original design, Pycket leverages Racket’s macro expander~\cite{flatt:2002} to preprocess macros, simplifying input programs into a small set of core forms that Pycket's runtime can directly handle~\cite{samth:11}. Initially, this macro expansion is performed externally by invoking the Racket executable, which expands modules and serializes them into JSON-encoded files for Pycket to read, convert into ASTs, and evaluate directly. As we will discuss in detail in \chapterRef{chapter:linklets}, we shift the macro expansion process into Pycket's runtime itself by importing Racket's expander linklet into Pycket's runtime. This shift is the key step that enables Pycket to become a fully self-hosted and independent execution environment for Racket.

        \paragraph{}%
            Once a module is expanded to core Racket forms, Pycket performs two transformations on its AST. First, it converts the AST into A-normal form (ANF), ensuring that all complex expressions are explicitly named, simplifying subsequent evaluation and optimization~\cite{danvy:93,flanagan:93}. Second, all mutable variables targeted by set! operations are transformed into heap-allocated cells. This approach, common in Lisp-family languages, enables environments to be immutable mappings from variables to values and allows each AST node to carry its static environment explicitly \cite{pycketmain}.

        \paragraph{}%
            Pycket implements approximately 1500 of Racket’s more than 2000 primitive operations and values. These primitives range from numeric computations, including bignums, rationals, and complex numbers, to regular expression handling, and sophisticated input/output abstractions. A substantial portion of Pycket’s implementation consists of these primitives, which are further extended by bootstrapping linklets discussed in \chapterRef{chapter:linklets}.

        \paragraph{}%
            Pycket implements approximately 1500 of Racket’s more than 2000 primitive operations and values. These primitives range from numeric computations, including bignums, rationals, and complex numbers, to sophisticated input/output abstractions and regular expression handling. Some of these primitive sets, notably input/output and regular expressions, are subsequently replaced by implementations supplied directly from Racket through bootstrapping linklets, as discussed in \chapterRef{chapter:linklets}. This approach allows Pycket to extend its runtime functionality using primitives provided as Racket code by the language itself.

        \paragraph{}%
            Additionally, Pycket employs some advanced techniques such as hidden classes, a technique borrowed from prototype-based object systems, to implement proxies efficiently. Proxies, crucial for enforcing soundness in gradual type systems and Racket’s contract systems, benefit significantly from Pycket’s approach, enhancing performance in both gradually typed and contract-heavy programs~\cite{findler-felleisen:2002,pycketmain2}. Moreover, Pycket naturally benefits from type specialization through tracing, eliminating the need for a separate type-feedback pass required by \glspl{jit} such as method-based \gls{jit} compilers~\cite{typeSpecial:2009}.

        \begin{figure}[t]
        \centering
        %--------------------------------------------------------------------
        % Syntax of expressions and continuations
        \[
        \begin{array}{lcl}
            e &::=& x \mid \lambda x.\,e \mid e\,e \\[4pt]
            \kappa &::=& [\,]
                    \mid \mathsf{arg}(e,\rho)::\kappa
                    \mid \mathsf{fun}(v,\rho)::\kappa
        \end{array}
        \]

        %--------------------------------------------------------------------
        % Transition rules of the CEK machine
        \[
        \begin{array}{rcl}
            \langle x,\rho,\kappa \rangle
            &\longmapsto&
            \langle \rho(x),\rho,\kappa \rangle \\[6pt]

            \langle (e_1\,e_2),\rho,\kappa \rangle
            &\longmapsto&
            \langle e_1,\rho,\mathsf{arg}(e_2,\rho)::\kappa \rangle \\[6pt]

            \langle v,\rho,\mathsf{arg}(e,\rho')::\kappa \rangle
            &\longmapsto&
            \langle e,\rho',\mathsf{fun}(v,\rho)::\kappa \rangle \\[6pt]

            \langle v,\rho,\mathsf{fun}(\lambda x.\,e,\rho')::\kappa \rangle
            &\longmapsto&
            \langle e,\rho'[x \mapsto v],\kappa \rangle
        \end{array}
        \]
        \caption{The CEK machine for the $\lambda$‑calculus \cite{pycketmain}.}
        \label{fig:cek-formal}
        \end{figure}

        \paragraph{}%
            After the user program is fully converted into its final AST form, Pycket evaluates it using the CEK abstract machine, whose semantics are given by the transition rules in Figure~\ref{fig:cek-formal}. A CEK state is represented as $\langle e,\rho,\kappa\rangle$, where $e$ is the expression currently being evaluated (the control), $\rho$ maps variable identifiers to their associated values (the environment), and $\kappa$ holds the computation context (the continuation). The continuation consists of frames that track pending evaluations, which come in two forms: an $\mathsf{arg}(e,\rho)$ frame captures an argument that is awaiting evaluation, and a $\mathsf{fun}(v,\rho)$ frame represents a function that is ready to receive its argument. Evaluating a variable involves simply looking up its value in the environment. Applying a function involves two steps—first evaluating the function itself and then evaluating its argument. The final step applies the evaluated function to its evaluated argument. Importantly, the CEK machine creates no additional continuation frames for tail calls, thus ensuring proper tail-calls \cite{pycketmain}.

        \paragraph{}%
            The corresponding RPython code implementing the CEK loop (\figref{fig:pycket-annotated-cek}) on Pycket continuously transforms CEK triples $(\text{ast}, \text{env}, \text{cont})$ into new states by invoking the \texttt{interpret} method on the current AST node. This iterative process proceeds until an empty continuation is encountered, at which point a \texttt{Done} exception is triggered, storing and returning the computation’s final result.

        % \begin{paragraph-here}[TODO]
        %     Possible good spot for mentioning environment pruning.
        % \end{paragraph-here}

        % \begin{paragraph-here}[TODO]
        %     Talk about strategies on data structures, strings etc.
        % \end{paragraph-here}

        \paragraph{}%
            Pycket help the meta-tracer identify loops using two complementary mechanisms: the \emph{two-state tracking} and a dynamic \emph{call-graph} approach. In two-state tracking, Pycket filters out false loops by associating potential loop points with caller-callee pairs, represented by the current and previous AST nodes. Without additional context, the \gls{jit} may incorrectly recognize various control-flow points as loops, and the two-state approach effectively reduces such false positives. However, this mechanism alone struggles with complex recursive patterns, such as indirect recursion or proxy-based calls. To complement two-state tracking, Pycket employs a dynamic call-graph technique. This method incrementally constructs a call graph at runtime, with nodes representing invoked functions and edges representing caller-callee relationships. Whenever adding a new edge creates a cycle, Pycket recognizes the target function as a loop header suitable for tracing. The combination of these two approaches ensures robust loop detection and optimized trace generation, particularly in programs with intricate control flows involving recursion and higher-order functions~\cite{pycketmain, pycketmain2}.

        \paragraph{}%
            While we will explore Pycket’s performance characteristics further in \secref{section:pycket-performance-characteristics}, we first turn to relevant RPython trace optimizations, which will provide the context necessary for a detailed technical discussion about performance considerations in \chapterRef{chapter:problem}.

    \section[\texorpdfstring{Trace Optimizations \& Runtime Feedback}{Trace Optimizations}]{Trace Optimizations \& Runtime Feedback}

        \paragraph{}%
            Traces are central to the performance characteristics of tracing \gls{jit} compilers. Good-quality traces directly influence the efficiency and effectiveness of \gls{jit} compilation. Quality here implies that traces capture frequently executed execution paths clearly and succinctly, avoiding excessive guards or unnecessary complexity. For example, traces that are tighter—having fewer exits—maximize the value derived from compiled machine code and facilitate other optimization opportunities, such as allocation removal. In contrast, poor-quality traces, characterized by multiple exit points or redundant control-flow checks, can introduce significant performance penalties, leading to frequent trace invalidations or costly fallbacks to interpreter execution. \cite{runtime-feedback:11}

        \paragraph{}%
            An important aspect affecting trace quality and overall \gls{jit} performance is warmup time. Warmup refers to the initial period during program execution when the \gls{jit} compiler is actively identifying and tracing hot loops. Initially, execution proceeds more slowly due to overheads from trace recording and compilation, as well as running interpreted code. Once the \gls{jit} compiler has completed its initial tracing and optimized key loops, execution starts to transition from interpreted paths to increased use of optimized compiled code, achieving a significant speedup. Thus, warmup time is the duration required for this transition, and minimizing it is critical for overall runtime performance.

        \paragraph{}%
            RPython’s trace optimizer incorporates several classic compiler optimizations tailored specifically for traces, including common sub-expression elimination, constant folding, copy propagation, and dead-code elimination. One inherent advantage of trace-based compilation for optimization purposes is the linear nature of traces. Unlike general control-flow graphs, a trace's straight-line structure simplifies analysis and optimization, typically requiring only forward and backward passes through the trace. This streamlined approach ensures efficient optimization, minimizing compilation overhead while maximizing runtime performance~\cite{loop-aware:12}.

        \paragraph{}%
            Inlining is a key optimization strategy significantly enhances performance in tracing \gls{jit} compilers, such as those provided by RPython. During trace recording, function calls encountered along the traced path are naturally inlined into the trace itself. This includes both high-level language functions and lower-level runtime routines. This aggressive inlining eliminates function call overhead, crosses abstraction level borders, thus significantly simplifies the execution path, and creates additional opportunities for further optimizations, such as constant propagation and automatic dead-code elimination~\cite{gal:2006,pypy-main}.

        \paragraph{}%
            Loop-invariant code motion is another effective optimization applied by RPython’s trace optimizer. Due to the linear nature of traces, loop-invariant computations are easily identified and relocated outside the loop. RPython accomplishes this by peeling off a single iteration from the loop, performing standard forward analyses, and optimizing the peeled iteration separately from the loop body~\cite{loop-aware:12}. Additionally, allocation removal is particularly relevant to Pycket’s CEK-based interpreter, which involves frequent allocations of state components such as continuations and environments. By analyzing object lifetimes and usages within traces, the optimizer can eliminate many heap allocations, significantly reducing memory pressure and improving runtime performance. We provide an in-depth analysis of memory performance in \sectionRef{section:memory}.

        \paragraph{}%
            Promotion is a specialized trace-optimization technique used by RPython that converts runtime variable values into compile-time constants within traces. This optimization explicitly introduces guards, such as \racketcode{guard(x == 0)}, to assert runtime invariants, thereby enabling the compiler to treat the guarded variables as constants in subsequent instructions within the trace. Although the original source code might not contain explicit constants, promotion allows the optimizer to leverage runtime information, effectively specializing traces for frequently occurring execution contexts. Promotion significantly enhances the trace specialization capability of the optimizer, leading to improved runtime performance through aggressive constant propagation and folding~\cite{bolzPhDThesis}.

        \begin{figure}[!htbp]                    % ordinary floating figure
            \centering
            \begin{minipage}{0.3\textwidth}
            \begin{lstlisting}[style=python-style,gobble=16]
                @jit.elidable
                def _is_ascii_elidable(s):
                    for c in s:
                        if ord(c) >= 128:
                            return False
                    return True
            \end{lstlisting}
            \end{minipage}
            \caption{A trace-elidable function in Pycket.}
            \label{fig:elidable-function}
        \end{figure}

        \paragraph{}%
            Escape analysis and the notion of trace-elidable functions further enhance trace optimizations in RPython. A function is deemed trace-elidable if successive calls with identical arguments during program execution yields the same result. Naturally, such a function should be free of any side-effects. Annotating such functions with the \texttt{@elidable} decorator allows the optimizer to replace repeated calls with cached results, effectively performing trace-level memoization. \figref{fig:elidable-function} provides an example of such an elidable function in Pycket. By replacing repeated function calls within traces with previously computed results, this optimization substantially reduces runtime overhead, further streamlining trace execution and improving overall performance~\cite{bolzPhDThesis}.
