\chapter[\texorpdfstring{APPROACHES TO IMPROVE SELF-HOSTING PERFORMANCE}
                          {7. Improving Performance}]{APPROACHES TO IMPROVE SELF-HOSTING PERFORMANCE}

	\label{chapter:solution}

  \begin{chaptersynopsis}
    We have identified solution approaches that are worthy of further investigation for improving self-hosting performance.

    You either try to avoid branch-heavy code, or you try to get better at it.

    Sections:
		\begin{itemize}
			\item \ref{section:hot-branches} Guiding Tracer Away from Branch-Heavy Computation

				Presenting a method to detect branch-heavy computations and help the tracer forgo them seems like a promising idea.
			\item \ref{section:stackful} Mitigating Memory Pressure in Branch-Heavy Computation

				A hybrid model of computation (e.g., a stackful model alongside CEK) seems to mitigate memory issues in some cases.
		\end{itemize}
  \end{chaptersynopsis}

  \paragraph{}%
    As demonstrated extensively in \chapterRef{chapter:problem}, the predominant performance degradation on Pycket self-hosting Racket arises from intricate branching patterns during macro expansion and associated runtime computations, which significantly inflate both trace sizes and GC overhead. In other words, branch-heavy computation and memory problems appear central to the performance challenges encountered when self-hosting on a meta-tracing JIT compiler.

  \paragraph{}%
    Addressing these core issues has the potential to transform self-hosting from a theoretical convenience into a practical and robust methodology for building language runtimes. By reducing the impact of branch-heavy computations and controlling memory usage, we can achieve consistent performance benefits from meta-tracing JIT compilers.

  \paragraph{}%
    The fundamental nature of self-hosting entails extensive branching, especially during the macro expansion phase. In Racket's case, macro expansion inherently involves deeply nested pattern-matching and conditional branching \cite{icfp2019}. Similarly, the limitations of meta-tracing JIT compilers like RPython when handling interpreters with complex control-flow patterns—such as extensive branching and recursion—are extensively documented in Bolz's dissertation \cite{bolzPhDThesis}. Additionally, Bolz et al.'s work further emphasizes how intricate branching within interpreters can lead to trace explosion and limited reuse \cite{pypy-main}. More generally, literature on compiler bootstrapping and self-hosting underscores that these processes inherently involve extensive branching and recursive computations, making simplification intrinsically challenging \cite{appelCompilingContinuations2007}. Consequently, while simplifying these computations might theoretically seem feasible, practical evidence and prior research consistently indicate intrinsic limitations in reducing branch complexity within self-hosting environments.


  \paragraph{}%
    Partial evaluation (PE) is a commonly used optimization technique in tracing JIT compilers. Being a program transformation (and specialization) operation, given a program and some \emph{static} variables (e.g. inputs or annotations), PE attempts to constant-fold away every operation that it can infer to be constants. The rest of the program (the operations that it can't fold) are outputted as the --hopefully-- more optimized and specialized version of the original program. \cite{truffle-graal} uses PE to collapse interpreter dispatch, and \cite{practical-partial} reports good results across JavaScript, Ruby, and R.  \cite{traceMonkey} applies a closely related form of runtime specialization, while \cite{trace-vs-PE} treats both tracing and partial evaluation as general meta-compilation techniques.

  \paragraph{}%
    Despite these successes, applying PE to Pycket's embedded Racket macro expander would be detrimental, particularly due to the branch-heavy nature of macro expansion. In Pycket, the Racket macro expander is loaded as ordinary Racket code through the \textit{expander linklet} and evaluated at boot time. Partially evaluating this code would transform a single, general-purpose expander into many \emph{program-specific} expanders. Given the macro expander's reliance on deeply nested, pattern-matching conditionals, the specialized traces would inline substantial amounts of branching logic. This process in a meta-tracing system results in trace explosion, diminished reuse across different programs, and frequent trace invalidation once trace-size thresholds are reached—behavior already documented extensively in PyPy’s meta-tracer.

  \paragraph{}%
    Furthermore, since macro expansion typically occurs infrequently (usually once per module load), any upfront overhead incurred by PE would not be amortized by repeated execution, diminishing its potential performance benefits. Given these factors, we opted not to implement PE in our current setup. Implementing such specialization within the RPython framework would entail development efforts comparable to building Pycket itself from scratch, rendering it impractical as an exploratory optimization.

  \paragraph{}%
    In this chapter, we explore targeted strategies to enhance performance specifically for branch-heavy computations on meta-tracing JIT compilers and propose viable directions for addressing these central performance challenges. First, we discuss techniques to guide the JIT compiler away from generating extensive, non-reusable traces that degrade performance. Second, we propose methods aimed explicitly at mitigating memory pressure—a critical bottleneck in self-hosting environments. While recognizing the preliminary nature of our current evidence, as discussed in detail in \chapterRef{chapter:problem}, we nonetheless demonstrate the potential efficacy of these approaches, providing motivation for further research and investigation.

	\section[\texorpdfstring{Guiding Tracer Away from Branch-Heavy Computation}{Hot Branches}]{Guiding Tracer Away from Branch-Heavy Computation}
    \label{section:hot-branches}
		\begin{mainpoint}
			Trying to detect branch-heavy computations and help the tracer forgo them seems like a promising idea.
		\end{mainpoint}

    \paragraph{}%
      A critical observation from \chapterRef{chapter:problem} is that the majority of performance overhead in self-hosting on meta-tracing JIT compilers stems from generating large, intricate, and often non-reusable traces during macro expansion phases. These expansive traces significantly degrade runtime performance due to the frequency and complexity of branching operations inherent in macro expanders.

    \paragraph{}%
      Importantly, as indicated in the performance analyses conducted in \chapterRef{chapter:problem}, once the expansion phase is completed, the runtime performance of self-hosted Pycket becomes comparable to the original Pycket implementation (without self-hosting). This highlights that the macro expansion phase, characterized by its branching complexity, is a key bottleneck to achieving overall runtime efficiency.

    \paragraph{}%
      Thus, guiding the tracer away from computations that yield large, non-reusable traces represents a targeted strategy to mitigate this bottleneck. Specifically, if the JIT compiler can effectively avoid or limit tracing the macro expansion's heavily branched computations, the most costly part of the self-hosting runtime can be significantly accelerated. This strategy is not intended to eliminate branching itself but rather aims to selectively bypass or simplify the generation of traces that cause the most severe performance penalties.


    \begin{paragraph-here}
      Explain the technical details of how we do it. Trace splitting, sticking, and splicing using a separate driver.
    \end{paragraph-here}

    \begin{show-experiment}
      Apply this to branchy and show that it works.
    \end{show-experiment}

    \begin{paragraph-here}
      Talk about the experiment, further breakdown if needed.
    \end{paragraph-here}

	\section[\texorpdfstring{Mitigating Memory Pressure in Branch-Heavy Computation}{CEK + Stackful Model}]{Mitigating Memory Pressure in Branch-Heavy Computation}
    \label{section:stackful}

		\begin{mainpoint}
			A hybrid model of computation (e.g., a stackful model alongside CEK) could mitigate memory issues.
		\end{mainpoint}

    \paragraph{}%
      A prominent contributor to the performance overhead in self-hosting via meta-tracing JIT compilers is the considerable memory pressure arising from frequent heap allocations, especially continuations and environments. Recall that the explicit continuations allocated on the heap are fundamental to the CEK machine's ability to implement complex control operations, including proper tail-calls, which inherently leads to frequent continuation allocations in Pycket. [TODO: Here's a brief summary from \chapterRef{chapter:problem}]

    \paragraph{}%
      To address memory issues without altering the underlying runtime system or garbage collector, we propose incorporating a stackful interpreter alongside the existing CEK machine in Pycket. This hybrid approach leverages the stack to handle certain computation phases, thereby significantly reducing the number of continuation allocations on the heap. The stackful interpreter is realized as a simple recursive interpreter compiled by the RPython framework into a recursive C program utilizing the native stack.

    \paragraph{}%
      Despite its simplicity, the use of a stackful interpreter introduces notable challenges related to stack management, particularly concerning stack overflows due to arbitrarily deep recursion levels. Furthermore, entirely replacing the CEK interpreter with a stack-based alternative would forgo the performance and convenience benefits offered by explicit continuations in the CEK machine, particularly in managing complex control operations and proper tail-calls.

    \paragraph{}%
      Consequently, we advocate for an approach where the two interpreters operate in tandem, strategically switching between them to optimize overall performance. The primary goal of this dual-interpreter strategy is to judiciously balance heap and stack usage—leveraging rapid stack allocations and improved locality while retaining the sophisticated control and optimization capabilities provided by the heap-based CEK interpreter.


    \subsection{Formalism for CEK \+ Stackful}

      \inputSub{solution}{cek-stackful-grammar}

      \paragraph{}%
        To facilitate deeper understanding and more precise experimental control, we developed a formal model integrating both the CEK and stackful interpreters. This formalism evaluates a simplified subset of Racket, closely resembling the Racket Core language described previously in \secref{section:linklet-semantics}, but excluding the linklet variable forms. Additionally, this model introduces specialized \racketcode{convert} forms explicitly designed for managing interpreter transitions directly from the source code. \figref{fig:st-cek-grammar} illustrates this simplified language, and \figref{fig:cek-convert-stack} depicts the fundamental interaction model, allowing for deliberate interpreter switching through these special forms.

      \begin{figure}[!h]
        \centering
        \includegraphics[scale=0.3]{\inputFigPath{solution}{cek-stackful-switch}}
        \caption{Stackful \& CEK Switch}
        \label{fig:cek-convert-stack}
      \end{figure}

      \paragraph{}%
        The underlying objective of this approach is to reduce heap memory pressure induced by large, long-lived continuations. Developed as an executable formalism using PLT Redex, this model enables rigorous experimentation with the semantics of interpreter transitions. \figref{fig:cek-reduction-relation-redacted} and \figref{fig:interpret-stack-redacted} provide simplified versions of the CEK and Stackful reduction semantics, respectively. Complete semantics are available in \appendixRef{appendix:cek-stackful-redex}.

      \begin{figure}[!h]
        \centering
        \includegraphics[scale=0.4]{\inputFigPath{solution}{cek-reduction-relation}}
        \caption{CEK Reduction Relation [PLACEHOLDER]}
        \label{fig:cek-reduction-relation-redacted}
      \end{figure}

      \begin{figure}[!h]
        \centering
        \includegraphics[scale=0.4]{\inputFigPath{solution}{interpret-stack-redacted}}
        \caption{Stackful Model (Redacted) [PLACEHOLDER]}
        \label{fig:interpret-stack-redacted}
      \end{figure}

      \begin{todo}[Technical TODO]
        Figure out how to use this model to show the general heap memory usage drops, as opposed to CEK alone.
      \end{todo}

      % \inputSub{solution}{cek-stackful-formalism}

      % \inputSub{solution}{cek-stackful-lemmas}

    \subsection{CEK \+ Stackful Prototype on Pycket}

      \paragraph{}%
        Building upon the insights gained from the formalism, we implemented a practical prototype of the hybrid interpreter approach on Pycket. This prototype aligns closely with the conceptual framework illustrated in \figref{fig:cek-convert-stack}, though it primarily initiates computations using the CEK interpreter. This design choice accommodates Pycket's need to load and instantiate the bootstrapping linklets at startup, subsequently beginning computations via the \racketcode{read}, \racketcode{expand}, and evaluation processes.

      \paragraph{}%
        In the implemented prototype, the CEK interpreter transfers control to the stackful interpreter upon encountering computational constructs such as \racketcode{let} or \racketcode{begin}. The switching mechanism itself is straightforward, via direct invocation of the stackful interpreter. The stackful interpreter is augmented with a trampoline mechanism to efficiently manage tail-call optimizations. Furthermore, it automatically hands control back to the CEK interpreter under three key conditions: (i) completion of computation and return, (ii) execution involving CPS-transformed primitives (e.g. \racketcode{call/cc}), or (iii) detection of potential stack overflow situations. The return to CEK is handled by unwinding the stack and installing necessary continuations via a \verb|ConvertStack| exception mechanism, effectively allowing seamless continuation of the computation in CEK mode.

      \begin{todo}[REVISE NUMBERS]
        Revise the numbers in the paragraph below.
      \end{todo}

      \paragraph{}%
        The central investigative goals for this prototype revolve around confirming reduced memory overhead, minimal interpreter-switching overhead, and bounded copying of activation records from the stack to the heap. To assess these goals, we conducted experiments using the \texttt{Branchy} program from \chapterRef{chapter:problem} (see \figref{fig:branchy-introduce-code}). We executed \texttt{Branchy} on Pycket under two configurations: the hybrid Stackful+CEK model and the CEK-only model. These experiments involved varying auto-generated inputs—specifically, sequences composed exclusively of 1s, exclusively of 2s, and randomized sequences containing numbers between 0 and 7—with sequence lengths of both 1000 and 2000 elements. For input sequences containing only 1s or only 2s, runtime performance was largely comparable between the two interpreter configurations, albeit with a noticeable increase in warmup time observed for the stackful interpreter. This increased warmup is anticipated, as RPython's meta-tracing JIT is not optimized for recursive interpreter structures, thus requiring further optimization efforts. Notably, we consistently observed improved runtime performance in the hybrid Stackful+CEK configuration during experiments utilizing randomized inputs, both for sequences of 1000 and 2000 numbers. \figref{fig:hybrid-model-overall-good} presents median runtime results specifically for the 1000-element randomized sequences.

      \begin{figure}[!h]
      %\begin{wrapfigure}{l}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.5]{\inputFigPath{solution}{stackful}}
        \caption{Overall performance is improved with Stackful \& CEK hybrid model. [PLACEHOLDER]}
        \label{fig:hybrid-model-overall-good}
      \end{figure}

      \begin{show-experiment}
        Continuation-heavy programs behave like this on CEK+Stackful hybrid model.
      \end{show-experiment}

      \begin{show-experiment}
       Run memory experiments/demos from [[chapter - problem]] with the CEK+Stackful to show the difference.
      \end{show-experiment}

      \begin{paragraph-here}
        So overall memory usage is reduced, so the technique works. [DUE TECH WORK]
      \end{paragraph-here}

      \begin{todo}[Technical TODO]
        Figure out how to show that the memory overhead is reduced.
      \end{todo}

      \begin{paragraph-here}
        We didn't add much by doing this. [DUE TECH WORK]
      \end{paragraph-here}

      \begin{todo}[Technical TODO]
        Figure out how to show that the number of points where a switch occurs between the two interpreters and the switching overhead should both be minimal
      \end{todo}

      \begin{todo}[Technical TODO]
        Figure out how to show that the amount of copying activation records from the stack into the continuations in the heap should be bounded.
      \end{todo}

      \paragraph{}%
        Taking a step towards addressing the fundamental challenges in self-hosting on meta-tracing frameworks—namely, intelligent meta-tracing of programs with complex control flows and effective management of heap memory pressure—we demonstrate viable strategies for achieving efficient self-hosting of Racket on Pycket. We believe the approaches explored herein hold broad applicability, potentially serving as foundational methods for self-hosting on meta-tracing frameworks more generally.



