\chapter{Approaches to Improve Self-hosting Performance}

	\label{chapter:solution}

    \begin{chaptersynopsis}
      We have identified solution approaches that are worthy of further investigation for improving self-hosting performance.

      You either try to avoid branch-heavy code, or you try to get better at meta-tracing branch-heavy code.
    \end{chaptersynopsis}

    \begin{paragraph-here}
      Branch-heavy computation and memory problems seem to be the central player in self-hosting performance.

      Show evidence from [[chapter - problem]]
    \end{paragraph-here}

    \begin{paragraph-here}
      This suggests that solving these issues have the potential of making self-hosting a practical and reliable method for building language runtimes.
    \end{paragraph-here}

    \begin{paragraph-here}
      The nature of this type of computation makes it a fundamental problem for meta-tracing (aforementioned in earlier chapters). There's not much to do to get better at this.
    \end{paragraph-here}

    \begin{paragraph-here}
      Partial evaluation is a common go-to in optimizing tracing in JIT compilers, but in our case it's bad because specialization hits branch-heavy computation the hardest.


      As we talk more in \secref{section:related-work}, \cite{truffle-graal} used partial evaluation to remove interpreter dispatch overhead.
    \cite{practical-partial} classifies partial evaluation as a standard optimization strategy in dynamic language JIT compilers, showing great performance for JavaScript, Ruby, and R.
    \cite{traceMonkey} also performs runtime specialization to try to remove interpreter overhead. \cite{trace-vs-PE} even explicitly recognizes partial evaluation as an alternative to tracing, recognizing both as  two “suitable meta-compilation techniques” for building language-independent JITs.
    \end{paragraph-here}

    \begin{paragraph-here}
      This chapter breaks down how we can deal with branch-heavy computation, and proposes some directions for solving the issues.

        - First section talks about how to steer the JIT away from tracing computations that might result in large non-reusable traces.

        - Second section proposes a method to help reduce the memory pressure that comes with self-hosting.
    \end{paragraph-here}

    \begin{paragraph-here}
      As we discussed in [[chapter - problem]], this requires further research, however, we do provide some preliminary evidences towards the efficacy of the proposed solutions to show that they are worthy of further investigation.
    \end{paragraph-here}


	\section{Guiding Tracer Away from Branch-Heavy Computation}
		\begin{mainpoint}
			Trying to detect branch-heavy computations and help the tracer forgo them seems like a promising idea.
		\end{mainpoint}


    \begin{paragraph-here}
      Most of the added cost in self-hosting on a meta-tracing JIT comes from spending a whole lot of time in producing gigantic non-reusable traces in the program expansion phase, talked in [[chapter - problem]].
    \end{paragraph-here}

    \begin{paragraph-here}
      We also mention in [[chapter - problem]] that when we do get passed the expansion, we have performance almost equivalent to the Original Pycket (without self-hosting) -- shown in last section in [[chapter - problem]].
    \end{paragraph-here}

    \begin{paragraph-here}
      Therefore, if we can help tracer avoid the computations that lead to that gigantic non-reusable traces trap, then we'll remove the part of self-hosting that slows down the system (by running the biggest branchiest code in the runtime--the expander--faster.).
    \end{paragraph-here}

    \begin{todo}
      TECH WORK -- Mature Hot Branch detection enough to run shit.
    \end{todo}

    \begin{paragraph-here}
      Explain the technique.
      How do we do it? BREAKDOWN -- DUE TECH WORK
    \end{paragraph-here}

    \begin{show-experiment}
      Apply this to branchy and show that it works.
    \end{show-experiment}

    \begin{paragraph-here}
      Talk about the experiment, further breakdown if needed.
    \end{paragraph-here}

	\section{Mitigating Memory Pressure in Branch-Heavy Computation}
		\begin{mainpoint}
			A hybrid model of computation (e.g., a stackful model alongside CEK) could mitigate memory issues.
		\end{mainpoint}

    \begin{paragraph-here}
       In [[chapter - problem]] memory section, we went into detail about the life in the heap and what's going on wiht the GC. Here's a brief summary.
    \end{paragraph-here}

    \begin{paragraph-here}
      To address these memory issues, we propose to use a stackful model along with the CEK. (AAA)
    \end{paragraph-here}

    (AAA) To address this issue (without modifying the underlying framework nor
    the GC), we propose to use a stackful interpreter to decrease the continuation allocations on the heap. The stackful interpreter here is
    a simple recursive interpreter for evaluating Racket forms, which is
    translated by the RPython framework into a recursive C program that
    uses the native stack. Recall that, however, the Pycket's interpreter
    is based on the CEK machine, and the heap allocated explicit
    continuations makes it very convenient to implement complex control
    operations with continuations (as well as the proper handling of
    tail-calls). Therefore the proposed approach is to use a stackful
    interpreter \emph{alongside} the CEK, rather than replacing the
    current CEK with a stackful interpreter. The idea is to use the stack
    and heap in balance to decrease the GC pressure and take advantage of
    using the stack, such as rapid allocations, locality etc.


    \begin{paragraph-here}
      Stackful model for Racket is not without any problems of its own (BBB)
    \end{paragraph-here}

    (BBB) Using the stack to implement a language like Racket, however,
    understandibly introduces some challenges regarding the stack
    management, such as handling overflows due to arbitrarily deep
    recursion. Additionally, implementing a stackful interpreter on Pyket
    for the entire Racket would be illogical, since it would not only
    require implementing everything from scratch including the
    continuations, but also not benefit from the current CEK's
    performance. Therefore the logical approach is to use the two
    interpreters together, repeatedly switch between the two whenever
    appropriate, making sure that the switches don't induce a high
    performance overhead.

    \begin{paragraph-here}
      But we shouldn't trade one performance issue for another (CCC)
    \end{paragraph-here}

    (CCC) One of the most fundamental points here is the interaction between the
    two interpreters. Essentially, we want to use the stack to save some
    heap space (thereby reducing the GC pressure) and take advantage of
    faster allocations and locality. On the other hand, we don't want to
    lose the benefits of the JIT's performance optimizations on the
    CEK. Moreover, the stack management needs to be on point to avoid
    problems such as switching back to the CEK when the stack is very deep
    (e.g. stack overflow), since that would require allocating enough
    continuations on the heap to counterbalance the performance benefit we
    get from using the stackful interpreter.

    \subsection{Formalism for CEK \+ Stackful}

      \begin{paragraph-here}
        I developed a formalism for cek+stackful (wrote a grammar). WHAT DO YOU DO WITH THAT FORMALISM??
      \end{paragraph-here}

      To understand this problem better and to work on a simpler setup, I
      developed a formalism that includes both the CEK machine and the
      stackful interpreter, evaluating a very simple subset of Racket
      together, shown in \figref{fig:st-cek--grammar}, which is similar to the
      Racket Core language used in \secref{subsec:linklets-semantics} minus
      the forms for the linklet variables, plus some \racketcode{convert}
      forms for controlling the interpreter switches from the source
      level. \figref{fig:cek-convert-stack} shows the overview of the
      interaction between the two models. We can start with either of the
      interpreters and switch back and forth between them using the
      \racketcode{convert} forms manually within the source code.

      \begin{todo}
        Find a way to mention/introduce, maybe even use the Redex model instead of straight-up math?.

        We could introduce the relations mathematically, and discuss running the redex models as opposed to do math equations for stuff.
        Maybe it would be more convenient to show reductions in math, tho.
      \end{todo}

      \begin{figure}[h]
        \centering
        \footnotesize
        %--- tweak the padding between text and frame (optional) ---
        \setlength{\fboxsep}{6pt}% default is 3 pt
        %--- boxed grammar -----------------------------------------
        \fbox{%
          \begin{minipage}{0.9\linewidth}
            \begin{align*}
              e &::=\; x\; |\; v\; |\; (e\; e\; \ldots)\; |\; (\textbf{if}\; e\; e\; e)\; |\; (o\; e\; e)\; \\
                &\; |\; (\textbf{begin}\; e\; e\; \ldots)\; |\; (\textbf{lambda}\; (x_{\_!\_}\; \ldots)\; e)\\
                &\; |\; (\textbf{set!}\; x\; e)\; |\; (\textbf{raises}\; e)\\
                &\; |\; (\textbf{let\dash values}\; (((x)\; e)\; \ldots)\; e)\\
                &\; |\; (\textbf{convert\dash to\dash stackful}\; e)\\
                &\; |\; (\textbf{convert\dash to\dash cek}\; e)
            \end{align*}
          \end{minipage}}
        %-----------------------------------------------------------
        \caption[hede]{Source Language for Stackful \& CEK Models}
        \label{fig:st-cek-grammar}
      \end{figure}

      \begin{figure}[!h]
      %\begin{wrapfigure}{l}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.5]{\inputFigPath{solution}{cek-stackful-switch}}
        \label{fig:cek-convert-stack}
      \end{figure}

    \subsection{CEK \+ Stackful Prototype on Pycket}

      \begin{paragraph-here}
        In addition to the formalism, I implemented it on Pycket to see how well it would perform.
      \end{paragraph-here}

      In addition, the formalism, in order to see how well this would perform
      in action, we also prototyped an implementation on Pycket. It works similarly to the model in \figref{fig:cek-convert-stack}, except
      the starting point is the CEK, because Pycket needs to instantiate and
      import the bootstrapping linklets at the startup and start a
      computation by loading and running the program via the
      \racketcode{read}, \racketcode{expand} etc.

      \begin{paragraph-here}
        CEK starts the computation, switches at every let, begin, continuations etc...
      \end{paragraph-here}

      The CEK starts the computation and handles the control to the stackful
      whenever it encounters a \racketcode{let} or a \racketcode{begin}
      form. The switch happens by just calling the stackful interpreter. The
      stackful interpreter includes a trampoline to handle the tail-calls,
      and in the current implementation it automatically switches back to
      the CEK whenever \textbf{i)} it returns, or \textbf{ii)} a CPSed
      primitive (e.g. a higher order function) is used, or \textbf{iii)} a
      stack overflow is detected. The switch back to the CEK happens by
      rewinding the stack via a \verb|ConvertStack| exception in which the
      forms in each activation record installs a continuation. The CEK takes
      the control and all the information at the previous switch-to-stack
      point and continues from there.

      \begin{show-experiment}
        The main investigation is about increasing overall performance:

        - The memory overhead should be reduced.

        - We didn't add too much overhead by doing this.

          - The number of points where a switch occurs between the two interpreters and the switching overhead should both be minimal

          - the amount of copying activation records from the stack into the continuations in the heap should be bounded.
      \end{show-experiment}

      The main investigation here is about the increase in the overall
      performance with the following constraints: \textbf{i)} The number of
      points where a switch occurs between the two interpreters and the
      switching overhead should both be minimal, and \textbf{ii)} the amount
      of copying activation records from the stack into the continuations in
      the heap should be bounded.

      \begin{show-experiment}
        Run the hybrid on Branchy from [[chapter - problem]]. This shows the overall performance is good.
      \end{show-experiment}

      \begin{paragraph-here}
        Since we're interested in self-hosting, we ran branchy on this and saw that ........... SHOW THE FIGURE
      \end{paragraph-here}

      Since we're interested in the performance
      on self-hosting Racket on Pycket, we are still interested in
      interpreter-style dispatch loops. Therefore as a preliminary
      experiment, we synthesized a program by simplifying the Racket's
      \racketcode{fasl->s-exp} function from the FASL\footnote{fast-load
        serialization} library, which deserializes a given byte stream into
      an s-expression. \figref{fig:branchy} shows the simplified program,
      which takes a list of numbers and takes different branches depending
      on the number. Note that some of the recursive calls are not
      tail-calls, i.e. installs a continuation every time it is evaluated.

      \begin{figure}[!h]
      %\begin{wrapfigure}{l}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.5]{\inputFigPath{solution}{stackful}}
      \end{figure}

      We ran this program on Pycket using Stackful+CEK and only-CEK with
      different auto-generated inputs containing; only 1s, only 2s, and
      random [0-7], each having 1000 and 2000 numbers. The runtimes for the
      ones with only 1s and 2s are quite similar, except a larger warmup
      time for the stackful interpreter, which is expected since RPython's
      meta-tracing is not designed for recursive interpreters, therefore it
      needs to be further optimized. However, we observe a consistent
      increase in the run-time performance in the randomized experiments
      (both 1000 and 2000) which visits different branches. Below are the
      medians for the run-times for the experiments using 1000 numbers.


      \begin{show-experiment}
        Run the hybrid on continuation heavy programs.
      \end{show-experiment}

      \begin{paragraph-here}
        Continuation-heavy programs behave like this on CEK+Stackful
      \end{paragraph-here}

      Because of the limitations in the variety of programs that the
      stackful interpreter can currently run (without large amounts of
      switching), I seek to further characterize the performance of the
      stackful evaluation along the CEK interpreter especially on programs
      with heavy use of continuations.

      \begin{show-experiment}
        We need to run every memory experiment/demo from [[chapter - problem]] with the CEK+Stackful to show the difference.
      \end{show-experiment}

      \begin{paragraph-here}
        Paragraphs that discuss the memory stuff to conclude.
      \end{paragraph-here}

      \begin{todo}
        Figure out how to show that the memory overhead is reduced.
      \end{todo}

      \begin{todo}
        Figure out how to show that the number of points where a switch occurs between the two interpreters and the switching overhead should both be minimal
      \end{todo}

      \begin{todo}
        Figure out how to show that the amount of copying activation records from the stack into the continuations in the heap should be bounded.
      \end{todo}

      \begin{paragraph-here}
        So after all these todo's we can say that we didn't add much by doing this.
      \end{paragraph-here}

      \begin{paragraph-here}
        A slight conclusion paragraph (since right after this the actual Conclusion chapter will follow.)
      \end{paragraph-here}

      Taking a step towards solving the fundamental issues in self-hosting
      on meta-tracing, namely, intelligent meta-tracing of the programs with
      complex control-flow and proper handling of the heap pressure, will
      allow efficient self-hosting of Racket on meta-tracing Pycket. We
      believe these are broadly applicable solutions that will serve as
      principle approaches in self-hosting on meta-tracing frameworks in
      general.
