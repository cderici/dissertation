\chapter[\texorpdfstring{APPROACHES TO IMPROVE SELF-HOSTING PERFORMANCE}
                          {7. Improving Performance}]{APPROACHES TO IMPROVE SELF-HOSTING PERFORMANCE}

	\label{chapter:solution}

  \begin{chaptersynopsis}[Chapter Synopsis - \emph{Chapter Content: 60\%}]\footnotesize
    \footnotesize

    We have identified solution approaches that are worthy of further investigation for improving self-hosting performance.

    You either try to avoid branch-heavy code, or you try to get better at it.

    Sections:
		\begin{itemize}
			\item \ref{section:hot-branches} Guiding Tracer Away from Branch-Heavy Computation

				Presenting a method to detect branch-heavy computations and help the tracer forgo them seems like a promising idea.
			\item \ref{section:stackful} Mitigating Memory Pressure in Branch-Heavy Computation

				A hybrid model of computation (e.g., a stackful model alongside CEK) seems to mitigate memory issues in some cases.
		\end{itemize}
  \end{chaptersynopsis}

  \paragraph{}%
    We demonstrated extensively in \chapterRef{chapter:problem}, that the predominant performance degradation on Pycket self-hosting Racket arises from intricate branching patterns during macro expansion and associated runtime computations. This degradation significantly inflates both trace sizes and the GC overhead. In other words, branch-heavy computation and memory problems appear central to the performance challenges encountered when self-hosting on a meta-tracing JIT compiler.

  \paragraph{}%
    Addressing these core issues has the potential to transform self-hosting from a theoretical convenience into a practical and robust methodology for building language runtimes. By reducing the impact of branch-heavy computations and controlling memory usage, we can achieve consistent performance benefits from meta-tracing JIT compilers.

  \paragraph{}%
    The fundamental nature of self-hosting entails extensive branching, especially during the macro expansion phase. In Racket's case, macro expansion inherently involves deeply nested pattern-matching and conditional branching \cite{icfp2019}. Similarly, the limitations of meta-tracing JIT compilers like RPython when handling interpreters with complex control-flow patterns—such as extensive branching and recursion—are extensively documented in Bolz's dissertation \cite{bolzPhDThesis}. Additionally, Bolz et al.'s work further emphasizes how intricate branching within interpreters can lead to trace explosion and limited reuse \cite{pypy-main}. More generally, literature on compiler bootstrapping and self-hosting underscores that these processes inherently involve extensive branching and recursive computations, making simplification intrinsically challenging \cite{appelCompilingContinuations2007}. Consequently, while simplifying these computations might theoretically seem feasible, practical evidence and prior research consistently indicate intrinsic limitations in reducing branch complexity within self-hosting environments.


  \paragraph{}%
    Partial evaluation (PE) is a commonly used optimization technique in tracing JIT compilers. Being a program transformation (and specialization) operation, given a program and some \emph{static} variables (e.g. inputs or annotations), PE attempts to constant-fold away every operation that it can infer to be constants. The rest of the program (the operations that it can't fold) are outputted as the --hopefully-- more optimized and specialized version of the original program. \cite{truffle-graal} uses PE to collapse interpreter dispatch, and \cite{practical-partial} reports good results across JavaScript, Ruby, and R.  \cite{traceMonkey} applies a closely related form of runtime specialization, while \cite{trace-vs-PE} treats both tracing and partial evaluation as general meta-compilation techniques.

  \paragraph{}%
    Despite these successes, applying PE to Pycket's embedded Racket macro expander would be detrimental, particularly due to the branch-heavy nature of macro expansion. In Pycket, the Racket macro expander is loaded as ordinary Racket code through the \textit{expander linklet} and evaluated at boot time. Partially evaluating this code would transform a single, general-purpose expander into many \emph{program-specific} expanders. Given the macro expander's reliance on deeply nested, pattern-matching conditionals, the specialized traces would inline substantial amounts of branching logic. This process in a meta-tracing system results in trace explosion, diminished reuse across different programs, and frequent trace invalidation once trace-size thresholds are reached—behavior already documented extensively in PyPy’s meta-tracer.

  \paragraph{}%
    Furthermore, since macro expansion typically occurs infrequently (usually once per module load), any upfront overhead incurred by PE would not be amortized by repeated execution, diminishing its potential performance benefits. Given these factors, we opted not to implement PE in our current setup. Implementing such specialization within the RPython framework would entail development efforts comparable to building Pycket itself from scratch, rendering it impractical as an exploratory optimization.

  \paragraph{}%
    In this chapter, we explore targeted strategies to enhance performance specifically for branch-heavy computations on meta-tracing JIT compilers and propose viable directions for addressing these central performance challenges. First, we discuss techniques to guide the JIT compiler away from generating extensive, non-reusable traces that degrade performance. Second, we propose methods aimed explicitly at mitigating memory pressure—a critical bottleneck in self-hosting environments. While recognizing the preliminary nature of our current evidence, as discussed in detail in \chapterRef{chapter:problem}, we nonetheless demonstrate the potential efficacy of these approaches, providing motivation for further research and investigation.

	\section[\texorpdfstring{Guiding Tracer Away from Branch-Heavy Computation}{Hot Branches}]{Guiding Tracer Away from Branch-Heavy Computation}
    \label{section:hot-branches}

    \begin{paragraph-here}% 1
      A critical observation from \chapterRef{chapter:problem} is that the majority of performance overhead in self-hosting on meta-tracing JIT compilers stems from generating large, intricate, and often non-reusable traces during macro expansion phases. These expansive traces significantly degrade runtime performance due to the frequency and complexity of branching operations inherent in macro expanders.
    \end{paragraph-here}


    \begin{paragraph-here}% 2
      Importantly, as indicated in the performance analyses conducted in \chapterRef{chapter:problem}, once the expansion phase is completed, the runtime performance of self-hosted Pycket becomes comparable to the original Pycket implementation (without self-hosting). This highlights that the macro expansion phase, characterized by its branching complexity, is a key bottleneck to achieving overall runtime efficiency.
    \end{paragraph-here}

    \begin{paragraph-here}% 3
      Thus, guiding the tracer away from computations that yield large, non-reusable traces represents a targeted strategy to mitigate this bottleneck. Specifically, if the JIT compiler can effectively avoid or limit tracing the macro expansion's heavily branched computations, the most costly part of the self-hosting runtime can be significantly accelerated. This strategy is not intended to eliminate branching itself but rather aims to selectively bypass or simplify the generation of traces that cause the most severe performance penalties.
    \end{paragraph-here}

    \begin{paragraph-here}% 4
      We experimented with the following approaches to guide the tracer away from branch-heavy computation and here we explain these approaches and show their efficacy. Unfortunately none of them are quite effective, and as we stated before, this only by itself requires extensive research (like another PhD study) to solve.
    \end{paragraph-here}

    \begin{paragraph-here}% 5
      Recall from \chapterRef{chapter:rpython}, that a language interpreter written in RPython utilizes JitDriver reflection provided by the RPython framework to define its own program counter (pc) and use hints such as jit\_merge\_point and can\_enter\_jit to indicate a loop header and the spot where the interpreter might perform a backwards jump, respectively. Every time the `can\_enter\_jit` is called on the JIT driver, a counter associated with the green variables is increased. An example can be seen in Figure 2 in Section 3.1. Pycket’s two-state tracking defines the program counter (green variables) as a pair of a lambda and an application (call-site) to determine the trace header. Each lambda ast node in Pycket has an enable\_jitting() and disable\_jitting() methods that indirectly controls the behavior of calling `can\_enter\_jit` at the CEK loop.
    \end{paragraph-here}

    \begin{paragraph-here}% 6
      Unfortunately there's no dynamic "this path is not worth tracing, abort now" option that'll abondon a trace mid-tracing. Therefore dynamically detecting nested if levels and try to get out of tracing the lambda that we're currently tracing is not an option.
    \end{paragraph-here}

    \begin{paragraph-here}% 7
      \paragraph{Statically disabling jit for branch-heavy lambdas} Statically detecting the depth of nested if statements is an option. Recall from [[chapter - rpython]] that Pycket performs a few whole code analyses such as a-normalization and assign-convert, which rebuilds the ast from inside out. During this process, we mark the `lambda` that has deep nested conditionals as \emph{jit blocked} (essentially blocks its `should\_enter` field and jit never calls `can\_enter\_jit` for that function). This, however, contradicts with our earlier observation in \chapterRef{chapter:problem} that branchiness is not a static property, but a property of a computation that depends on the input. In other words, we can have a loop dominated with heavy branches but also has a shallow branch that an input takes exclusively, then this approach hurts more than it helps because we disable jitting for the entire function.
    \end{paragraph-here}

    \begin{paragraph-here}% 8
      \paragraph{Using meta-hints to guide the tracer} Meta-hints is a generalization of the RPython interpreter hints that extends further the communication between the language being implemented and the tracing-JIT run-time. We introduce two Racket forms: \racketcode{define/jit-merge-point} and \racketcode{meta-can-enter-jit} to be used in the user code. \racketcode{define/jit-merge-point} constructs a `lambda` that's *jit-block*ed by default (i.e. its `should\_enter` is always False), so normally it's not gonna be jitted by default. Whenever we hit a \racketcode{meta-can-enter-jit} in the Racket code, we flip the switch and call `can\_enter\_jit` on the JIT driver. Therefore by using \racketcode{meta-can-enter-jit} in the Racket code only at the shallow branches inside the loop, we ensure that the counter that's associated with the loop inside the `lambda` is considered hot only when shallow branches are taken repeatedly.
    \end{paragraph-here}

    \begin{figure-here}
      branchy annotated with meta-hints
    \end{figure-here}

    \begin{paragraph-here}% 9
      \figref{} shows the Branchy from \figref{branchy} that's annotated with meta-hints. Only the shallow loop calls are marked with \racketcode{meta-can-enter-jit}. Thus, when running on the input list, the deep branches do not contribute to the hotness of the loop, only the shallow branches do. Obviously this not a general solution. It is a heuristic that states basically "if branchiness is a concern, a trace will be more useful if the language level computation is shallow in terms of nested if levels", and an input can certainly take deep branches along with the shallow branches as well, so as soon as the tracing starts (the shallow branches are taken enough times) the tracer won't differentiate between shallow or deep branches, it'll record everything.
    \end{paragraph-here}

    \begin{paragraph-here}% 10
      One downside of this approach is the annotation of the user code, which is not very realistic for programs of the size similar to the Racket's macro expander. Considering the complex indirections in the imported sub-systems facilitating self-hosting, annotation of meta-hints becomes difficult, and also we lose the ability to use the exported functionalities as is, which is not idea.
    \end{paragraph-here}

    \begin{paragraph-here}% 11
      \paragraph{Add nested if depth to green variables} Recall that in Pycket's two state representation we have the lambda ast and come\_from information to detect loops. In this approach, we add a third variable to the green variables that represents a "depth" score for nested conditionals. For instance in Branchy, Exit B would have a depth score of 5, and Exit A would have a depth score of 1. The idea is that every time we cross those spots, if the depth score is more than a certain number, we add that number to the depth. So for example, if we have a threshold of 3, next time we take the Exit B, the depth variable would become 10 (because 5 > 3), but at Exit A the depth stays the same (because 1 < 3). This way, the green variables that `can\_enter\_jit` method increases the counter for will be different at each time we loop at a deep branch, therefore they won't contribute to the hotness of the loop.
    \end{paragraph-here}

    \begin{figure-here}
      chart to show run times running branchy on different inputs with no optimizattions, meta-hints, and extended green variables.
    \end{figure-here}

    \begin{paragraph-here}% 12
      \figref{} shows the overall run times for running Branchy with all 0 input (takes the deep branch path), all 20 (takes the shallow deep branch), and randomized input, with meta-hints and extended green variables. All run times are averages over 1000 runs with the same setup explained in \sectionRef{benchmarkSetups}, and random inputs are kept the same across different settings.
    \end{paragraph-here}

    \begin{paragraph-here}% 13
      Both meta-hints approach and the green variable with depth info approach are able to eliminate long trace that we see in \figref{longbranchytracefigure} for the deep path. This is not necessarily an inprovement on the run-time, however, because that path is kept being interpreted and that's inherently slow. Therefore, we do eliminate time for recording, compiling and optimizing, but if these deep branchy paths that are not JITted are taken frequently then it's gonna get much worse. At the same time, we see that both approaches trace the loop that goes through the shallow path, thus perform similarly to the unoptimized version. With the random input we do see a slowdown incurred by recording deep branches alongside the shallow branches, confirming our concern from earlier that once the shallow paths trigger a recording, then all the deep paths that branchy goes through afterwards also flood into the trace.
    \end{paragraph-here}

    \begin{paragraph-here}% 14
      While these approaches don't provide a clear win for a general scenario because of their targeted nature, evidence shows that they do warrant further investigation. In particular, meta-hints approach can be further generalized by dynamically altering the green variables on the JIT driver at the user code level. Similar to our second approach with the depth info, by adding additional components to the green variables to extend the definition of the program counter, the interpreter can dynamically modify the profiling performed by the tracer. This exaggerates the annotation needs to be done in the user code, and further specializes the behavior of the profiling to the user code, however, with a more fine grained control we postulate that a more finessed approach can be achieved. Furthermore, the interpreter can also utilize more than one JIT driver. A driver that focuses on branch-heavy nature alongside the driver with the original green variables can be employed to have a more fine grained control at the interpreter level.
    \end{paragraph-here}

    \begin{paragraph-here}% 15
      Fundamentally, as we discussed in \chapterRef{chapter:problem}, the issue is to find the best trace for computations like these on a branch-heavy computations. Traces that contain all the conditional decisions become highly specialized, therefore not very useful unless same paths are taken by the interpreter (via same input patterns)--which is unrealistic. On the other hand, frequent paths that are not traced because of techniques like we introduced here saves tracing time, but progressively hinder the runtime performance as they're being interpreted instead of JITted. Nevertheless, evidence suggests that alleviating issues that are specific to the mechanisms used in self-hosting with targeted approaches like these can indeed open the path to efficient self-hosting on meta-tracing JIT compilers.
    \end{paragraph-here}


	\section[\texorpdfstring{Mitigating Memory Pressure in Branch-Heavy Computation}{CEK + Stackful Model}]{Mitigating Memory Pressure in Branch-Heavy Computation}
    \label{section:stackful}

    \paragraph{}%
      A prominent contributor to the performance overhead in self-hosting via meta-tracing JIT compilers is the considerable memory pressure arising from frequent heap allocations, especially continuations and environments. Recall that the explicit continuations allocated on the heap are fundamental to the CEK machine's ability to implement complex control operations, including proper tail-calls, which inherently leads to frequent continuation allocations in Pycket. [TODO: Here's a brief summary from \chapterRef{chapter:problem}]

    \paragraph{}%
      To address memory issues without altering the underlying runtime system or garbage collector, we propose incorporating a stackful interpreter alongside the existing CEK machine in Pycket. This hybrid approach leverages the stack to handle certain computation phases, thereby significantly reducing the number of continuation allocations on the heap. The stackful interpreter is realized as a simple recursive interpreter compiled by the RPython framework into a recursive C program utilizing the native stack. For example, when interpreting a \racketcode{let}-form, instead of creating a \racketcode{LetCont} to evaluate its body after the right-hand-side like we do in the CEK machine, in the stackful interpreter we recursively evaluate the right-hand-side and then interpret the body, using the native stack for the natural continuation.

    \paragraph{}%
      Despite its simplicity, the use of a stackful interpreter introduces notable challenges related to stack management, particularly concerning stack overflows due to arbitrarily deep recursion. Furthermore, entirely replacing the CEK interpreter with a stack-based alternative would forgo the performance benefits offered by explicit continuations in the CEK machine, particularly in managing complex control operations and proper tail-calls.

    \paragraph{}%
      Consequently, we advocate for an approach where the two interpreters operate in tandem, strategically switching between them to optimize overall performance. The primary goal of this dual-interpreter strategy is to judiciously balance heap and stack usage—leveraging rapid stack allocations and improved locality while retaining the sophisticated control and optimization capabilities provided by the heap-based CEK interpreter.

    \subsection{CEK \+ Stackful Prototype on Pycket}

      \paragraph{}%
        We implemented a practical prototype of the hybrid interpreter approach on Pycket. This prototype aligns closely with the conceptual framework illustrated in \figref{fig:cek-convert-stack}, though it primarily initiates computations using the CEK interpreter. This design choice accommodates Pycket's need to load and instantiate the bootstrapping linklets at startup, subsequently beginning computations via the \racketcode{read}, \racketcode{expand}, and evaluation processes.

      \paragraph{}%
        In the implemented prototype, the CEK interpreter transfers control to the stackful interpreter upon encountering computational constructs such as \racketcode{let} or \racketcode{begin}. The switching mechanism itself is straightforward, via direct invocation of the stackful interpreter. The stackful interpreter is augmented with a trampoline mechanism to efficiently manage tail-call optimizations. Furthermore, it automatically hands control back to the CEK interpreter under three key conditions: (i) completion of computation and return, (ii) execution involving CPS-transformed primitives (e.g. \racketcode{call/cc}), or (iii) detection of potential stack overflow situations. The return to CEK is handled by unwinding the stack and installing necessary continuations via a \verb|ConvertStack| exception mechanism, effectively allowing seamless continuation of the computation in CEK mode.

      \begin{todo}[REVISE NUMBERS]
        Revise the numbers in the paragraph below.
      \end{todo}

      \paragraph{}%
        The central investigative goals for this prototype revolve around confirming reduced memory overhead, minimal interpreter-switching overhead, and bounded copying of activation records from the stack to the heap. To assess these goals, we conducted experiments using the \texttt{Branchy} program from \chapterRef{chapter:problem} (see \figref{fig:branchy-introduce-code}). We executed \texttt{Branchy} on Pycket under two configurations: the hybrid Stackful+CEK model and the CEK-only model. These experiments involved varying auto-generated inputs—specifically, sequences composed exclusively of 1s, exclusively of 2s, and randomized sequences containing numbers between 0 and 7—with sequence lengths of both 1000 and 2000 elements. For input sequences containing only 1s or only 2s, runtime performance was largely comparable between the two interpreter configurations, albeit with a noticeable increase in warmup time observed for the stackful interpreter. This increased warmup is anticipated, as RPython's meta-tracing JIT is not optimized for recursive interpreter structures, thus requiring further optimization efforts. Notably, we consistently observed improved runtime performance in the hybrid Stackful+CEK configuration during experiments utilizing randomized inputs, both for sequences of 1000 and 2000 numbers. \figref{fig:hybrid-model-overall-good} presents median runtime results specifically for the 1000-element randomized sequences.

      \begin{figure}[!h]
      %\begin{wrapfigure}{l}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.5]{\inputFigPath{solution}{stackful}}
        \caption{Overall performance is improved with Stackful \& CEK hybrid model. [PLACEHOLDER]}
        \label{fig:hybrid-model-overall-good}
      \end{figure}

      \begin{show-experiment}
        Continuation-heavy programs behave like this on CEK+Stackful hybrid model.
      \end{show-experiment}

      \begin{show-experiment}
       Run memory experiments/demos from [[chapter - problem]] with the CEK+Stackful to show the difference.
      \end{show-experiment}

      \begin{paragraph-here}
        So overall memory usage is reduced, so the technique works. [DUE TECH WORK]
      \end{paragraph-here}

      \begin{todo}[Technical TODO]
        Figure out how to show that the memory overhead is reduced.
      \end{todo}

      \begin{paragraph-here}
        We didn't add much by doing this. [DUE TECH WORK]
      \end{paragraph-here}

      \begin{todo}[Technical TODO]
        Figure out how to show that the number of points where a switch occurs between the two interpreters and the switching overhead should both be minimal
      \end{todo}

      \begin{todo}[Technical TODO]
        Figure out how to show that the amount of copying activation records from the stack into the continuations in the heap should be bounded.
      \end{todo}

    \subsection{Formalism for CEK \+ Stackful}

      \inputFigure{solution}{cek-stackful-grammar}
s
      \paragraph{}%
        To facilitate deeper understanding and more precise experimental control, we developed a formal model integrating both the CEK and stackful interpreters. This formalism evaluates a simplified subset of Racket, closely resembling the Racket Core language described previously in \secref{section:linklet-semantics}, but excluding the linklet variable forms. Additionally, this model introduces specialized \racketcode{convert} forms explicitly designed for managing interpreter transitions directly from the source code. \figref{fig:st-cek-grammar} illustrates this simplified language, and \figref{fig:cek-convert-stack} depicts the fundamental interaction model, allowing for deliberate interpreter switching through these special forms.

      \inputFigure{solution}{cek-stackful-switch}

      \paragraph{}%
        The underlying objective of this approach is to reduce heap memory pressure induced by large, long-lived continuations. Developed as an executable formalism using PLT Redex, this model enables rigorous experimentation with the semantics of interpreter transitions. \figref{fig:cek-reduction-relation-redacted} and \figref{fig:interpret-stack-redacted} provide simplified versions of the CEK and Stackful reduction semantics, respectively. Complete semantics are available in \appendixRef{appendix:cek-stackful-redex}.

      \begin{figure}[!h]
        \centering
        \includegraphics[scale=0.4]{\inputFigPath{solution}{cek-reduction-relation}}
        \caption{CEK Reduction Relation [PLACEHOLDER]}
        \label{fig:cek-reduction-relation-redacted}
      \end{figure}

      \inputFigure{solution}{cek-reduction-relation-redacted}

      \begin{figure}[!h]
        \centering
        \includegraphics[scale=0.4]{\inputFigPath{solution}{interpret-stack-redacted}}
        \caption{Stackful Model (Redacted) [PLACEHOLDER]}
        \label{fig:interpret-stack-redacted}
      \end{figure}

      \begin{todo}[Technical TODO]
        Figure out how to use this model to show the general heap memory usage drops, as opposed to CEK alone.
      \end{todo}

      % \inputSub{solution}{cek-stackful-formalism}

      % \inputSub{solution}{cek-stackful-lemmas}

      \paragraph{}%
        Taking a step towards addressing the fundamental challenges in self-hosting on meta-tracing frameworks—namely, intelligent meta-tracing of programs with complex control flows and effective management of heap memory pressure—we demonstrate viable strategies for achieving efficient self-hosting of Racket on Pycket. We believe the approaches explored herein hold broad applicability, potentially serving as foundational methods for self-hosting on meta-tracing frameworks more generally.



