\chapter{Approaches to Improve Self-hosting Performance}

	\label{chapter:solution}

    \begin{chaptersynopsis}
      We have identified solution approaches that are worthy of further investigation for improving self-hosting performance.

      You either try to avoid branch-heavy code, or you try to get better at it.
    \end{chaptersynopsis}

    \begin{paragraph-here}
      Branch-heavy computation and memory problems seem to be the central player in self-hosting performance.

      Show evidence from [[chapter - problem]]
    \end{paragraph-here}

    \begin{paragraph-here}
      This suggests that solving these issues could make self-hosting a practical and reliable method for building language runtimes.
    \end{paragraph-here}

    \begin{paragraph-here}
      The nature of branch-heavy computation on a tracing JIT makes self-hosting fundamentally problematic on meta-tracing (aforementioned in earlier chapters). There's not much to be done to make self-hosting less branch-heavy (cite?).

      (Need some citations to show self-hosting/bootstrapping a language is fundamentally done via branch-heavy computations.)
    \end{paragraph-here}

    \paragraph{}%
      Partial evaluation is a common optimization in tracing JIT compilers. For example, \cite{truffle-graal} uses it to collapse interpreter dispatch, and \cite{practical-partial} reports good results across JavaScript, Ruby, and R.  \cite{traceMonkey} applies a closely related form of runtime specialization, while \cite{trace-vs-PE} treats both tracing and partial evaluation as general meta-compilation techniques.

    \paragraph{}%
      Despite these successes, applying partial evaluation to Pycket's embedded Racket macro expander would be detrimental, particularly due to the branch-heavy nature of macro expansion. In Pycket the Racket macro expander is loaded as ordinary Racket code through the \textit{expander linklet} and evaluated at boot time. Partially evaluating this code would turn a single, general-purpose expander into many \emph{program-specific} expanders. Because macro expansion is dominated by deeply nested, pattern-matching conditionals, the specialized traces would inline large amounts of branch logic. In a meta-tracing system this leads to trace explosion, poor cross-program reuse, and frequent invalidation once trace-size limits are exceeded—behavior already documented in PyPy’s meta-tracer.

    \paragraph{}%
      Furthermore, since macro expansion typically occurs infrequently (usually once per module load), any upfront overhead incurred by partial evaluation would not be amortized by repeated execution, diminishing its potential performance benefits. Given these factors, we opted not to implement partial evaluation in our current setup. Implementing such specialization within the RPython framework would entail development efforts comparable to building Pycket itself from scratch, rendering it impractical as an exploratory optimization.

    \begin{paragraph-here}
      This chapter breaks down how we can improve the performance of branch-heavy computation on meta-tracing JIT, and proposes some directions for solving the issues.

        - First section talks about how to steer the JIT away from tracing computations that might result in large non-reusable traces.

        - Second section proposes a method to help reduce the memory pressure that comes with self-hosting.

      As we discussed in [[chapter - problem]], this requires further research, however, we do provide some preliminary evidences towards the efficacy of the proposed solutions to show that they are worthy of further investigation.
    \end{paragraph-here}


	\section{Guiding Tracer Away from Branch-Heavy Computation}
		\begin{mainpoint}
			Trying to detect branch-heavy computations and help the tracer forgo them seems like a promising idea.
		\end{mainpoint}


    \begin{paragraph-here}
      Most of the added cost in self-hosting on a meta-tracing JIT comes from spending a whole lot of time in producing gigantic non-reusable traces in the program expansion phase, talked in [[chapter - problem]].
    \end{paragraph-here}

    \begin{paragraph-here}
      We also mention in [[chapter - problem]] that when we do get passed the expansion, we have performance almost equivalent to the Original Pycket (without self-hosting) -- shown in last section in [[chapter - problem]].
    \end{paragraph-here}

    \begin{paragraph-here}
      Therefore, if we can help tracer avoid the computations that lead to that gigantic non-reusable traces trap, then we'll remove the part of self-hosting that actually slows down the system (by running the biggest branchiest code in the runtime--the expander--faster.).
    \end{paragraph-here}

    \begin{paragraph-here}
      Explain the technical details of how we do it. Trace splitting, sticking, and splicing using a separate driver.
    \end{paragraph-here}

    \begin{show-experiment}
      Apply this to branchy and show that it works.
    \end{show-experiment}

    \begin{paragraph-here}
      Talk about the experiment, further breakdown if needed.
    \end{paragraph-here}

	\section{Mitigating Memory Pressure in Branch-Heavy Computation}
		\begin{mainpoint}
			A hybrid model of computation (e.g., a stackful model alongside CEK) could mitigate memory issues.
		\end{mainpoint}

    \begin{paragraph-here}
       In [[chapter - problem]] memory section, we went into detail about the life in the heap and what's going on wiht the GC. Here's a brief summary.
    \end{paragraph-here}

    \begin{paragraph-here}
      To address this issue (without modifying the underlying framework nor the GC), we propose to use a stackful interpreter to decrease the continuation allocations on the heap. The stackful interpreter here is a simple recursive interpreter for evaluating Racket forms, which is translated by the RPython framework into a recursive C program that uses the native stack. Recall that, however, the Pycket's interpreter is based on the CEK machine, and the heap allocated explicit continuations makes it very convenient to implement complex control operations with continuations (as well as the proper handling of tail-calls). Therefore, the proposed approach is to use a stackful interpreter \emph{alongside} the CEK, rather than replacing the current CEK with a stackful interpreter. The idea is to use the stack and heap in balance to decrease the GC pressure and take advantage of using the stack, such as rapid allocations, locality etc.
    \end{paragraph-here}

    \begin{paragraph-here}
      Using the stack to implement a language like Racket, however, understandably introduces some challenges regarding the stack management, such as handling overflows due to arbitrarily deep recursion. Additionally, implementing a stackful interpreter on Pycket for the entire Racket would be illogical, since it would not only
    require implementing everything from scratch including the
    continuations, but also not benefit from the current CEK's
    performance. Therefore, the logical approach is to use the two
    interpreters together, repeatedly switch between the two whenever
    appropriate/beneficial, making sure that the switches don't induce a high
    performance overhead.
    \end{paragraph-here}

    \begin{paragraph-here}
        One of the most fundamental points here is the interaction between the
      two interpreters. Essentially, we want to use the stack to save some
      heap space (thereby reducing the GC pressure) and take advantage of
      faster allocations and locality. On the other hand, we don't want to
      lose the benefits of the JIT's performance optimizations on the
      CEK. Moreover, the stack management needs to be on point to avoid
      problems such as switching back to the CEK when the stack is very deep
      (e.g. stack overflow), since that would require allocating enough
      continuations on the heap to counterbalance the performance benefit we
      get from using the stackful interpreter.
    \end{paragraph-here}

    \subsection{Formalism for CEK \+ Stackful}

      \inputSub{solution}{cek-stackful-grammar}

      \begin{paragraph-here}
          To understand this problem better and to work on a simpler setup, I
        developed a formalism that includes both the CEK machine and the
        stackful interpreter, evaluating a very simple subset of Racket
        together, shown in \figref{fig:st-cek-grammar}, which is similar to the
        Racket Core language used in \secref{section:linklet-semantics} minus
        the forms for the linklet variables, plus some \racketcode{convert}
        forms for controlling the interpreter switches from the source
        level. \figref{fig:cek-convert-stack} shows the basic
        interaction between the two models. We can start with either of the
        interpreters and switch back and forth between them using the
        \racketcode{convert} forms manually within the source code.
      \end{paragraph-here}

      \begin{figure}[!h]
        \centering
        \includegraphics[scale=0.3]{\inputFigPath{solution}{cek-stackful-switch}}
        \caption{Stackful \& CEK Switch}
        \label{fig:cek-convert-stack}
      \end{figure}

      \begin{paragraph-here}
        The idea is, again, to try to reduce some memory pressure on the heap caused by the long-lived big objects, by utilizing the stack. As an executable formalism, this is developed on PLT Redex. \figref{fig:cek-reduction-relation-redacted} and \figref{fig:interpret-stack-redacted} shows the redacted semantics of the CEK model and the Stackful model respectively. The full semantics are available in \appendixRef{appendix:cek-stackful-redex}.
      \end{paragraph-here}

      \begin{figure}[!h]
        \centering
        \includegraphics[scale=0.4]{\inputFigPath{solution}{cek-reduction-relation}}
        \caption{CEK Reduction Relation [PLACEHOLDER]}
        \label{fig:cek-reduction-relation-redacted}
      \end{figure}

      \begin{figure}[!h]
        \centering
        \includegraphics[scale=0.4]{\inputFigPath{solution}{interpret-stack-redacted}}
        \caption{Stackful Model (Redacted) [PLACEHOLDER]}
        \label{fig:interpret-stack-redacted}
      \end{figure}

      \begin{todo}[Technical TODO]
        Figure out how to use this model to show the general heap memory usage drops, as opposed to CEK alone.
      \end{todo}

      % \inputSub{solution}{cek-stackful-formalism}

      % \inputSub{solution}{cek-stackful-lemmas}

    \subsection{CEK \+ Stackful Prototype on Pycket}

      \begin{paragraph-here}
          In addition, the formalism, in order to see how well this would perform
        in action, we also prototyped an implementation on Pycket. It works similarly to the model in \figref{fig:cek-convert-stack}, except
        the starting point is the CEK, because Pycket needs to instantiate and
        import the bootstrapping linklets at the startup and start a
        computation by loading and running the program via the
        \racketcode{read}, \racketcode{expand} etc.
      \end{paragraph-here}

      \begin{paragraph-here}
          The CEK starts the computation and hands the control to the stackful
        whenever it encounters a \racketcode{let} or a \racketcode{begin}
        form. The switch happens by just calling the stackful interpreter. The
        stackful interpreter includes a trampoline to handle the tail-calls,
        and in the current implementation it automatically switches back to
        the CEK whenever \textbf{i)} it returns, or \textbf{ii)} a CPSed
        primitive (e.g. a higher order function) is used, or \textbf{iii)} a
        stack overflow is detected. The switch back to the CEK happens by
        rewinding the stack via a \verb|ConvertStack| exception in which the
        forms in each activation record installs a continuation. The CEK takes
        the control and all the information at the previous switch-to-stack
        point and continues from there.
      \end{paragraph-here}

      \begin{paragraph-here}
          The main investigation here is about the increase in the overall
        performance with the following constraints: \textbf{i)} The memory overhead should be reduced. \textbf{ii)} The number of
        points where a switch occurs between the two interpreters and the
        switching overhead should both be minimal, and \textbf{iii)} the amount
        of copying activation records from the stack into the continuations in
        the heap should be bounded.
      \end{paragraph-here}


      \begin{paragraph-here}
        Since we're interested in self-hosting performance, we ran branchy from [[chapter - problem]] on Pycket using Stackful+CEK and only-CEK with
      different auto-generated inputs containing; only 1s, only 2s, and
      random [0-7], each having 1000 and 2000 numbers. The runtimes for the
      ones with only 1s and 2s are quite similar, except a larger warmup
      time for the stackful interpreter, which is expected since RPython's
      meta-tracing is not designed for recursive interpreters, therefore it
      needs to be further optimized. However, we observe a consistent
      increase in the run-time performance in the randomized experiments
      (both 1000 and 2000) which visits different branches. \figref{fig:hybrid-model-overall-good} shows the
      medians for the run-times for the experiments using 1000 numbers.
      \end{paragraph-here}

      \begin{figure}[!h]
      %\begin{wrapfigure}{l}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.5]{\inputFigPath{solution}{stackful}}
        \caption{Overall performance is improved with Stackful \& CEK hybrid model. [PLACEHOLDER]}
        \label{fig:hybrid-model-overall-good}
      \end{figure}

      \begin{show-experiment}
        Continuation-heavy programs behave like this on CEK+Stackful hybrid model.
      \end{show-experiment}

      \begin{show-experiment}
       Run memory experiments/demos from [[chapter - problem]] with the CEK+Stackful to show the difference.
      \end{show-experiment}

      \begin{paragraph-here}
        So overall memory usage is reduced, so the technique works.
      \end{paragraph-here}

      \begin{todo}[Technical TODO]
        Figure out how to show that the memory overhead is reduced.
      \end{todo}

      \begin{paragraph-here}
        We didn't add much by doing this.
      \end{paragraph-here}

      \begin{todo}[Technical TODO]
        Figure out how to show that the number of points where a switch occurs between the two interpreters and the switching overhead should both be minimal
      \end{todo}

      \begin{todo}[Technical TODO]
        Figure out how to show that the amount of copying activation records from the stack into the continuations in the heap should be bounded.
      \end{todo}


      \begin{paragraph-here}
        A slight conclusion paragraph (since right after this the actual Conclusion chapter will follow.)

          Taking a step towards solving the fundamental issues in self-hosting
        on meta-tracing, namely, intelligent meta-tracing of the programs with
        complex control-flow and proper handling of the heap pressure, will
        allow efficient self-hosting of Racket on meta-tracing Pycket. We
        believe these are broadly applicable solutions that will serve as
        principle approaches in self-hosting on meta-tracing frameworks in
        general.
      \end{paragraph-here}


