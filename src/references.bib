@book{appelCompilingContinuations2007,
  title = {Compiling with {{Continuations}}},
  author = {Appel, Andrew W.},
  year = {2007},
  month = jan,
  publisher = {Cambridge University Press},
  address = {USA},
  abstract = {This book shows how continuation-passing style is used as an intermediate representation to perform optimizations and program transformations. Continuations can be used to compile most programming languages. The method is illustrated in a compiler for the programming language Standard ML. Prior knowledge of ML, however, is not necessary, as the author carefully explains each concept as it arises. This is the first book to show how concepts from the theory of programming languages can be applied to the production of practical optimizing compilers for modern languages like ML. All the details of compiling are covered, including the interface to a runtime system and garbage collector.},
  isbn = {978-0-521-03311-4}
}

@article{bolz15-meta-vm,
  title = {The Impact of Meta-Tracing on {{VM}} Design and Implementation},
  author = {Bolz, Carl Friedrich and Tratt, Laurence},
  year = {2015},
  month = feb,
  journal = {Science of Computer Programming},
  series = {Special {{Issue}} on {{Advances}} in {{Dynamic Languages}}},
  volume = {98},
  pages = {408--421},
  issn = {0167-6423},
  doi = {10.1016/j.scico.2013.02.001},
  urldate = {2019-11-12},
  abstract = {Most modern languages are implemented using Virtual Machines (VMs). While the best VMs use Just-In-Time (JIT) compilers to achieve good performance, JITs are costly to implement, and few VMs therefore come with one. The RPython language allows tracing JIT VMs to be automatically created from an interpreter, changing the economics of VM implementation. In this paper, we explain, through two concrete VMs, how meta-tracing RPython VMs can be designed and optimised, and, experimentally, the performance levels one might reasonably expect from them.},
  langid = {english},
  keywords = {Meta-tracing,Programming languages,Virtual machines},
  file = {C\:\\Users\\caner\\Zotero\\storage\\LRDAMC8H\\Bolz and Tratt - 2015 - The impact of meta-tracing on VM design and implem.pdf;C\:\\Users\\caner\\Zotero\\storage\\4EAFPWT3\\S0167642313000269.html}
}

@article{bolzHowNotWrite,
  title = {How to Not Write {{Virtual Machines}} for {{Dynamic Languages}}},
  author = {Bolz, Carl Friedrich and Rigo, Armin},
  pages = {11},
  langid = {english},
  file = {C:\Users\caner\Zotero\storage\I36I5QGI\Bolz and Rigo - How to not write Virtual Machines for Dynamic Lang.pdf}
}

@article{bolzMetatracingMakesFast2014,
  title = {Meta-Tracing Makes a Fast {{Racket}}},
  author = {Bolz, Carl Friedrich and Pape, Tobias and Siek, Jeremy and {Tobin-Hochstadt}, Sam},
  year = {2014},
  pages = {7},
  abstract = {Tracing just-in-time (JIT) compilers record and optimize the instruction sequences they observe at runtime. With some modifications, a tracing JIT can perform well even when the executed program is itself an interpreter, an approach called meta-tracing. The advantage of meta-tracing is that it separates the concern of JIT compilation from language implementation, enabling the same JIT compiler to be used with many different languages. The RPython meta-tracing JIT compiler has enabled the efficient interpretation of several dynamic languages including Python (PyPy), Prolog, and Smalltalk. In this paper we present initial findings in applying the RPython JIT to Racket. Racket comes from the Scheme family of programming languages for which there are mature static optimizing compilers. We present the result of spending just a couple person-months implementing and tuning an implementation of Racket written in RPython. The results are promising, with a geometric mean equal to Racket's performance and within a factor of 2 slower than Gambit and Larceny on a collection of standard Scheme benchmarks. The results on individual benchmarks vary widely. On the positive side, our interpreter is sometimes up to two to six times faster than Gambit, an order of magnitude faster than Larceny, and two orders of magnitude faster than the Racket JIT compiler when making heavy use of continuations. On the negative side, our interpreter is sometimes three times slower than Racket, nine times slower than Gambit, and five times slower than Larceny.},
  langid = {english},
  file = {C:\Users\caner\Zotero\storage\2RGFN3VC\Bolz et al. - Meta-tracing makes a fast Racket.pdf}
}

@article{bolzPhDThesis,
  title = {{Meta-Tracing Just-in-Time Compilation for RPython}},
  author = {Bolz, Carl Friedrich},
  langid = {ngerman},
  file = {C:\Users\caner\Zotero\storage\57J8CJLC\Bolz - Meta-Tracing Just-in-Time Compilation for RPython.pdf}
}

@article{branch-predict:03,
  title = {Optimizing Indirect Branch Prediction Accuracy in Virtual Machine Interpreters},
  author = {Casey, Kevin and Ertl, M. Anton and Gregg, David},
  year = {2007},
  month = oct,
  journal = {ACM Trans. Program. Lang. Syst.},
  volume = {29},
  number = {6},
  pages = {37--es},
  issn = {0164-0925},
  doi = {10.1145/1286821.1286828},
  urldate = {2025-07-16},
  abstract = {Interpreters designed for efficiency execute a huge number of indirect branches and can spend more than half of the execution time in indirect branch mispredictions. Branch target buffers (BTBs) are the most widely available form of indirect branch prediction; however, their prediction accuracy for existing interpreters is only 2\%--50\%. In this article we investigate two methods for improving the prediction accuracy of BTBs for interpreters: replicating virtual machine (VM) instructions and combining sequences of VM instructions into superinstructions. We investigate static (interpreter build-time) and dynamic (interpreter runtime) variants of these techniques and compare them and several combinations of these techniques. To show their generality, we have implemented these optimizations in VMs for both Java and Forth. These techniques can eliminate nearly all of the dispatch branch mispredictions, and have other benefits, resulting in speedups by a factor of up to 4.55 over efficient threaded-code interpreters, and speedups by a factor of up to 1.34 over techniques relying on dynamic superinstructions alone.},
  file = {C:\Users\caner\Zotero\storage\7927UYB9\Casey et al. - 2007 - Optimizing indirect branch prediction accuracy in virtual machine interpreters.pdf}
}

@inproceedings{clasp_llvm,
  title = {Clasp {{Common Lisp Implementation}} and {{Optimization}}},
  booktitle = {Proceedings of the 11th {{European Lisp Symposium}} on {{European Lisp Symposium}}},
  author = {Schafmeister, Christian A. and Wood, Alex},
  year = {2018},
  month = apr,
  series = {{{ELS2018}}},
  pages = {59--64},
  publisher = {European Lisp Scientific Activities Association},
  address = {Marbella, Spain},
  urldate = {2025-06-26},
  abstract = {We describe implementation strategies and updates made in the last two years to clasp,[1, 2] a new Common Lisp implementation that interoperates with C++, uses the cleavir compiler, and uses the LLVM backend[4]. Improvements in clasp have been made in many areas. The most important changes are: (1) Tagged pointers and immediate values have been incorporated. (2) A fast generic function dispatch approach has been implemented that allows clasp to carry out generic function dispatch as fast as SBCL, a highly optimized free implementation of Common Lisp. The generic function dispatch memoization approach was developed by Robert Strandh[8] and demonstrates a 20x improvement in performance of generic function dispatch. (3) The new LLVM feature ``Thin Link Time Optimization'' has been added, which speeds up generated code by removing call overhead throughout the system. (4) Type inference has been added to the cleavir compiler, which is part of clasp. Type inference removes redundant run-time type checks and dead code paths. Type inference currently provides about a 30\% speedup in microbenchmarks.[9] (5) Constants and literals have been moved close to the code and ``instruction pointer addressing'' has been incorporated to speed access to literals and constants. (6) Pre-emptive multithreading has been added to clasp, based on pthreads, supporting the Bordeaux threads library. (7) The overall build time for clasp has been reduced from five to eight hours over two years ago to approximately one hour at present.},
  isbn = {978-2-9557474-2-1}
}

@inproceedings{clos_overview_mop,
  title = {The {{Common Lisp Object System}}: {{An Overview}}},
  shorttitle = {The {{Common Lisp Object System}}},
  booktitle = {Proceedings of the {{European Conference}} on {{Object-Oriented Programming}}},
  author = {DeMichiel, Linda G. and Gabriel, Richard P.},
  year = {1987},
  month = jun,
  series = {{{ECOOP}} '87},
  pages = {151--170},
  publisher = {Springer-Verlag},
  address = {Berlin, Heidelberg},
  urldate = {2025-07-03},
  abstract = {The Common Lisp Object System is an object-oriented system that is based on the concepts of generic functions, multiple inheritance, and method combination. All objects in the Object System are instances of classes that form an extension to the Common Lisp type system. The Common Lisp Object System is based on a meta-object protocol that renders it possible to alter the fundamental structure of the Object System itself. The Common Lisp Object System has been proposed as a standard for ANSI Common Lisp and has been tentatively endorsed by X3J13.},
  isbn = {978-3-540-18353-2}
}

@article{collapse:17,
  title = {Collapsing {{Towers}} of {{Interpreters}}},
  author = {Amin, Nada and Rompf, Tiark},
  year = {2017},
  month = dec,
  journal = {Proc. ACM Program. Lang.},
  volume = {2},
  number = {POPL},
  pages = {52:1--52:33},
  issn = {2475-1421},
  doi = {10.1145/3158140},
  urldate = {2019-11-29},
  abstract = {Given a tower of interpreters, i.e., a sequence of multiple interpreters interpreting one another as input programs, we aim to collapse this tower into a compiler that removes all interpretive overhead and runs in a single pass. In the real world, a use case might be Python code executed by an x86 runtime, on a CPU emulated in a JavaScript VM, running on an ARM CPU. Collapsing such a tower can not only exponentially improve runtime performance, but also enable the use of base-language tools for interpreted programs, e.g., for analysis and verification. In this paper, we lay the foundations in an idealized but realistic setting.   We present a multi-level lambda calculus that features staging constructs and stage polymorphism: based on runtime parameters, an evaluator either executes source code (thereby acting as an interpreter) or generates code (thereby acting as a compiler). We identify stage polymorphism, a programming model from the domain of high-performance program generators, as the key mechanism to make such interpreters compose in a collapsible way.   We present Pink, a meta-circular Lisp-like evaluator on top of this calculus, and demonstrate that we can collapse arbitrarily many levels of self-interpretation, including levels with semantic modifications. We discuss several examples: compiling regular expressions through an interpreter to base code, building program transformers from modi ed interpreters, and others. We develop these ideas further to include reflection and reification, culminating in Purple, a reflective language inspired by Brown, Blond, and Black, which realizes a conceptually infinite tower, where every aspect of the semantics can change dynamically. Addressing an open challenge, we show how user programs can be compiled and recompiled under user-modified semantics.},
  keywords = {compiler,interpreter,Lisp,reflection,Scala,staging},
  file = {C:\Users\caner\Zotero\storage\ZDQXC6SI\Amin and Rompf - 2017 - Collapsing Towers of Interpreters.pdf}
}

@book{comp-cont:92,
  title = {Compiling with Continuations},
  author = {Appel, Andrew W.},
  year = {1992},
  publisher = {Cambridge University Press},
  address = {USA},
  isbn = {978-0-521-41695-5}
}

@inproceedings{comp-without-cont:17,
  title = {Compiling {{Without Continuations}}},
  booktitle = {Proceedings of the 38th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Maurer, Luke and Downen, Paul and Ariola, Zena M. and Peyton Jones, Simon},
  year = {2017},
  series = {{{PLDI}} 2017},
  pages = {482--494},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/3062341.3062380},
  urldate = {2019-11-29},
  abstract = {Many fields of study in compilers give rise to the concept of a join point---a place where different execution paths come together. Join points are often treated as functions or continuations, but we believe it is time to study them in their own right. We show that adding join points to a direct-style functional intermediate language is a simple but powerful change that allows new optimizations to be performed, including a significant improvement to list fusion. Finally, we report on recent work on adding join points to the intermediate language of the Glasgow Haskell Compiler.},
  isbn = {978-1-4503-4988-8},
  keywords = {ANF,CPS,GHC,Haskell,intermediate languages,list fusion},
  file = {C:\Users\caner\Zotero\storage\UM27HJWZ\Maurer et al. - 2017 - Compiling Without Continuations.pdf}
}

@inproceedings{cont-heap-stack:90,
  title = {Representing {{Control}} in the {{Presence}} of {{First-class Continuations}}},
  booktitle = {Proceedings of the {{ACM SIGPLAN}} 1990 {{Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Hieb, R. and Dybvig, R. Kent and Bruggeman, Carl},
  year = {1990},
  series = {{{PLDI}} '90},
  pages = {66--77},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/93542.93554},
  urldate = {2019-11-29},
  abstract = {Languages such as Scheme and Smalltalk that provide continuations as first-class data objects present a challenge to efficient implementation. Allocating activation records in a heap has proven unsatisfactory because of increased frame linkage costs, increased garbage collection overhead, and decreased locality of reference. However, simply allocating activation records on a stack and copying them when a continuation is created results in unbounded copying overhead. This paper describes a new approach based on stack allocation that does not require the stack to be copied when a continuation is created and that allows us to place a small upper bound on the amount copied when a continuation is reinstated. This new approach is faster than the naive stack allocation approach, and it does not suffer from the problems associated with unbounded copying. For continuation-intensive programs, our approach is at worst a constant factor slower than the heap allocation approach, and for typical programs, it is significantly faster. An important additional benefit is that recovery from stack overflow is handled gracefully and efficiently.},
  isbn = {978-0-89791-364-5},
  file = {C:\Users\caner\Zotero\storage\EKD5S5T3\Hieb et al. - 1990 - Representing Control in the Presence of First-clas.pdf}
}

@inproceedings{converge:05,
  title = {Compile-Time {{Meta-programming}} in a {{Dynamically Typed OO Language}}},
  booktitle = {Proceedings of the 2005 {{Symposium}} on {{Dynamic Languages}}},
  author = {Tratt, Laurence},
  year = {2005},
  series = {{{DLS}} '05},
  pages = {49--63},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/1146841.1146846},
  urldate = {2019-11-28},
  abstract = {Compile-time meta-programming allows programs to be constructed by the user at compile-time. Although LISP derived languages have long had such facilities, few modern languages are capable of compile-time meta-programming, and of those that do many of the most powerful are statically typed functional languages. In this paper I present the dynamically typed object orientated language Converge which allows compile-time meta-programming in the spirit of Template Haskell. Converge demonstrates that integrating powerful, safe compile-time meta-programming features into a dynamic language requires few restrictions to the flexible development style facilitated by the paradigm. In this paper I detail Converge's compile-time meta-programming facilities, much of which is adapted from Template Haskell, contain several features new to the paradigm. Finally I explain how such a facility might be integrated into similar languages.},
  file = {C:\Users\caner\Zotero\storage\A6ZCCMXB\Tratt - 2005 - Compile-time Meta-programming in a Dynamically Typ.pdf}
}

@inproceedings{danvy:93,
  title = {Separating Stages in the Continuation-Passing Style Transformation},
  booktitle = {Proceedings of the 20th {{ACM SIGPLAN-SIGACT}} Symposium on {{Principles}} of Programming Languages},
  author = {Lawall, Julia L. and Danvy, Olivier},
  year = {1993},
  month = mar,
  series = {{{POPL}} '93},
  pages = {124--136},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/158511.158613},
  urldate = {2025-06-23},
  abstract = {The continuation-passing style (CPS) transformation is powerful but complex. Our thesis is that this transformation is in fact compound, and we set out to stage it. We factor the CPS transformation into several steps, separating aspects in each step: (1) Intermediate values are named; (2) Continuations are introduced; (3) Sequencing order is decided and administrative reductions are performed.Step 1 determines the evaluation order (e.g., call-by-name or call-by-value). Step 2 isolates the introduction of continuations and is expressed with local, structure-preserving rewrite rules --- a novel aspect standing in sharp contrast with the usual CPS transformations. Step 3 determines the ordering of continuations (e.g., left-to-right or right-to-left evaluation) and leads to the familiar-looking continuation-passing terms.Step 2 is completely reversible and Steps 1 and 3 form Galois connections. Together they lead to the direct style (DS) transformation of our earlier work (including first-class continuations): (1) Intermediate continuations are named and sequencing order is abstracted; (2) Second-class continuations are eliminated; (3) Administrative reductions are performed.A subset of these transformations can leverage program manipulation systems: CPS-based compilers can modify sequencing to improve e.g., register allocation; static program analyzers can yield more precise results; and overspecified CPS programs can be rescheduled. Separating aspects of the CPS transformation also enables a new programming style, with applications to nondeterministic programming. As a byproduct, our work also suggests a new continuation semantics for unspecified sequencing orders in programming languages (e.g., Scheme).},
  isbn = {978-0-89791-560-1},
  file = {C:\Users\caner\Zotero\storage\5LRL7IK2\Lawall and Danvy - 1993 - Separating stages in the continuation-passing style transformation.pdf}
}

@article{dynamo,
  title = {Dynamo: {{A Transparent Dynamic Optimization System}}},
  author = {Bala, Vasanth and Duesterwald, Evelyn and Banerjia, Sanjeev},
  pages = {12},
  abstract = {We describe the design and implementation of Dynamo, a software dynamic optimization system that is capable of transparently improving the performance of a native instruction stream as it executes on the processor. The input native instruction stream to Dynamo can be dynamically generated (by a JIT for example), or it can come from the execution of a statically compiled native binary. This paper evaluates the Dynamo system in the latter, more challenging situation, in order to emphasize the limits, rather than the potential, of the system. Our experiments demonstrate that even statically optimized native binaries can be accelerated Dynamo, and often by a significant degree. For example, the average performance of --O optimized SpecInt95 benchmark binaries created by the HP product C compiler is improved to a level comparable to their --O4 optimized version running without Dynamo. Dynamo achieves this by focusing its efforts on optimization opportunities that tend to manifest only at runtime, and hence opportunities that might be difficult for a static compiler to exploit. Dynamo's operation is transparent in the sense that it does not depend on any user annotations or binary instrumentation, and does not require multiple runs, or any special compiler, operating system or hardware support. The Dynamo prototype presented here is a realistic implementation running on an HP PA-8000 workstation under the HPUX 10.20 operating system.},
  langid = {english}
}

@inproceedings{dynStatComp:12,
  title = {Adding {{Dynamically-typed Language Support}} to a {{Statically-typed Language Compiler}}: {{Performance Evaluation}}, {{Analysis}}, and {{Tradeoffs}}},
  shorttitle = {Adding {{Dynamically-typed Language Support}} to a {{Statically-typed Language Compiler}}},
  booktitle = {Proceedings of the 8th {{ACM SIGPLAN}}/{{SIGOPS Conference}} on {{Virtual Execution Environments}}},
  author = {Ishizaki, Kazuaki and Ogasawara, Takeshi and Castanos, Jose and Nagpurkar, Priya and Edelsohn, David and Nakatani, Toshio},
  year = {2012},
  series = {{{VEE}} '12},
  pages = {169--180},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/2151024.2151047},
  urldate = {2019-11-29},
  abstract = {Applications written in dynamically typed scripting languages are increasingly popular for Web software development. Even on the server side, programmers are using dynamically typed scripting languages such as Ruby and Python to build complex applications quickly. As the number and complexity of dynamically typed scripting language applications grows, optimizing their performance is becoming important. Some of the best performing compilers and optimizers for dynamically typed scripting languages are developed entirely from scratch and target a specific language. This approach is not scalable, given the variety of dynamically typed scripting languages, and the effort involved in developing and maintaining separate infrastructures for each. In this paper, we evaluate the feasibility of adapting and extending an existing production-quality method-based Just-In-Time (JIT) compiler for a language with dynamic types. Our goal is to identify the challenges and shortcomings with the current infrastructure, and to propose and evaluate runtime techniques and optimizations that can be incorporated into a common optimization infrastructure for static and dynamic languages. We discuss three extensions to the compiler to support dynamically typed languages: (1) simplification of control flow graphs, (2) mapping of memory locations to stack-allocated variables, and (3) reduction of runtime overhead using language semantics. We also propose four new optimizations for Python in (2) and (3). These extensions are effective in reduction of compiler working memory and improvement of runtime performance. We present a detailed performance evaluation of our approach for Python, finding an overall improvement of 1.69x on average (up to 2.74x) over our JIT compiler without any optimization for dynamically typed languages and Python.},
  isbn = {978-1-4503-1176-2},
  keywords = {dynamically typed language,just-in-time compiler,python},
  file = {C:\Users\caner\Zotero\storage\MRG8T5J8\Ishizaki et al. - 2012 - Adding Dynamically-typed Language Support to a Sta.pdf}
}

@inproceedings{enginesOriginal,
  title = {Engines Build Process Abstractions},
  booktitle = {Proceedings of the 1984 {{ACM Symposium}} on {{LISP}} and Functional Programming},
  author = {Haynes, Christopher T. and Friedman, Daniel P.},
  year = {1984},
  month = aug,
  series = {{{LFP}} '84},
  pages = {18--24},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/800055.802018},
  urldate = {2025-07-04},
  abstract = {Engines are a new programming language abstraction for timed preemption. In conjunction with first class continuations, engines allow the language to be extended with a time-sharing implementation of process abstraction facilities. To illustrate engine programming techniques, we implement a round-robin process scheduler. The importance of simple but powerful primitives such as engines is discussed.},
  isbn = {978-0-89791-142-9},
  file = {C:\Users\caner\Zotero\storage\WCJ4LMLR\Haynes and Friedman - 1984 - Engines build process abstractions.pdf}
}

@inproceedings{erlang_otp_hpc,
  title = {High-Performance Technical Computing with Erlang},
  booktitle = {Proceedings of the 7th {{ACM SIGPLAN}} Workshop on {{ERLANG}}},
  author = {Scalas, Alceste and Casu, Giovanni and Pili, Piero},
  year = {2008},
  month = sep,
  series = {{{ERLANG}} '08},
  pages = {49--60},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1411273.1411281},
  urldate = {2025-07-03},
  abstract = {High-performance Technical Computing (HPTC) is a branch of HPC (High-performance Computing) that deals with scientific applications, such as physics simulations. Due to its numerical nature, it has been traditionally based on low-level or mathematically-oriented languages (C, C++, Fortran), extended with libraries that implement remote execution and inter-process communication (like MPI and PVM).But those libraries just provide what Erlang does out-of-the-box: networking, process distribution, concurrency, interprocess communication and fault tolerance. So, is it possible to use Erlang as a foundation for developing HPTC applications?This paper shows our experiences in using Erlang for distributed number-crunching systems. We introduce two extensions: a simple and efficient foreign function interface (FFI), and an Erlang binding for numerical libraries. We use them as a basis for developing a simple mathematically-oriented programming language (in the style of Matlab™) compiled into Core Erlang. These tools are later used for creating a HPTC framework (based on message-passing) and an IDE for distributed applications.The results of this research and development show that Erlang/OTP can be used as a platform for developing large and scalable numerical applications.},
  isbn = {978-1-60558-065-4},
  file = {C:\Users\caner\Zotero\storage\PWQBJDAV\Scalas et al. - 2008 - High-performance technical computing with erlang.pdf}
}

@inproceedings{felleisen87,
  title = {Control Operators, the {{SECD-machine}}, and the {$\lambda$}-Calculus},
  booktitle = {Formal {{Description}} of {{Programming Concepts}}},
  author = {Felleisen, Matthias and Friedman, Daniel P.},
  year = {1987},
  keywords = {Lambda calculus,SECD machine}
}

@misc{findler-felleisen:2002,
  title = {Contracts for Higher-Order Functions {\textbar} {{Proceedings}} of the Seventh {{ACM SIGPLAN}} International Conference on {{Functional}} Programming},
  urldate = {2025-06-23},
  howpublished = {https://dl-acm-org.proxyiub.uits.iu.edu/doi/10.1145/581478.581484},
  file = {C:\Users\caner\Zotero\storage\DJUW4VN7\581478.html}
}

@article{flanagan:93,
  title = {The Essence of Compiling with Continuations},
  author = {Flanagan, Cormac and Sabry, Amr and Duba, Bruce F. and Felleisen, Matthias},
  year = {1993},
  month = jun,
  journal = {SIGPLAN Not.},
  volume = {28},
  number = {6},
  pages = {237--247},
  issn = {0362-1340},
  doi = {10.1145/173262.155113},
  urldate = {2025-06-23},
  abstract = {In order to simplify the compilation process, many compilers for higher-order languages use the continuation-passing style (CPS) transformation in a first phase to generate an intermediate representation of the source program. The salient aspect of this intermediate form is that all procedures take an argument that represents the rest of the computation (the ``continuation''). Since the nai{\textasciidieresis}ve CPS transformation considerably increases the size of programs, CPS compilers perform reductions to produce a more compact intermediate representation. Although often implemented as a part of the CPS transformation, this step is conceptually a second phase. Finally, code generators for typical CPS compilers treat continuations specially in order to optimize the interpretation of continuation parameters.A thorough analysis of the abstract machine for CPS terms show that the actions of the code generator invert the nai{\textasciidieresis}ve CPS translation step. Put differently, the combined effect of the three phases is equivalent to a source-to-source transformation that simulates the compaction phase. Thus, fully developed CPS compilers do not need to employ the CPS transformation but can achieve the same results with a simple source-level transformation.},
  file = {C:\Users\caner\Zotero\storage\VWHIVX6I\Flanagan et al. - 1993 - The essence of compiling with continuations.pdf}
}

@article{flatt:2002,
  title = {Composable and Compilable Macros: You Want It When?},
  shorttitle = {Composable and Compilable Macros},
  author = {Flatt, Matthew},
  year = {2002},
  month = sep,
  journal = {SIGPLAN Not.},
  volume = {37},
  number = {9},
  pages = {72--83},
  issn = {0362-1340},
  doi = {10.1145/583852.581486},
  urldate = {2025-06-23},
  abstract = {Many macro systems, especially for Lisp and Scheme, allow macro transformers to perform general computation. Moreover, the language for implementing compile-time macro transformers is usually the same as the language for implementing run-time functions. As a side effect of this sharing, implementations tend to allow the mingling of compile-time values and run-time values, as well as values from separate compilations. Such mingling breaks programming tools that must parse code without executing it. Macro implementors avoid harmful mingling by obeying certain macro-definition protocols and by inserting phase-distinguishing annotations into the code. However, the annotations are fragile, the protocols are not enforced, and programmers can only reason about the result in terms of the compiler's implementation. MzScheme---the language of the PLT Scheme tool suite---addresses the problem through a macro system that separates compilation without sacrificing the expressiveness of macros.},
  file = {C:\Users\caner\Zotero\storage\55CGSQJ9\Flatt - 2002 - Composable and compilable macros you want it when.pdf}
}

@article{flattMacrosThatWork2012,
  title = {Macros That {{Work Together}}: {{Compile-time}} Bindings, Partial Expansion, and Definition Contexts},
  shorttitle = {Macros That {{Work Together}}},
  author = {Flatt, Matthew and Culpepper, Ryan and Darais, David and Findler, Robert Bruce},
  year = {2012},
  month = mar,
  journal = {Journal of Functional Programming},
  volume = {22},
  number = {2},
  pages = {181--216},
  issn = {1469-7653, 0956-7968},
  doi = {10.1017/S0956796812000093},
  urldate = {2020-01-30},
  abstract = {Racket is a large language that is built mostly within itself. Unlike the usual approach taken by non-Lisp languages, the self-hosting of Racket is not a matter of bootstrapping one implementation through a previous implementation, but instead a matter of building a tower of languages and libraries via macros. The upper layers of the tower include a class system, a component system, pedagogic variants of Scheme, a statically typed dialect of Scheme, and more. The demands of this language-construction effort require a macro system that is substantially more expressive than previous macro systems. In particular, while conventional Scheme macro systems handle stand-alone syntactic forms adequately, they provide weak support for macros that share information or macros that use existing syntactic forms in new contexts. This paper describes and models features of the Racket macro system, including support for general compile-time bindings, sub-form expansion and analysis, and environment management. The presentation assumes a basic familiarity with Lisp-style macros, and it takes for granted the need for macros that respect lexical scope. The model, however, strips away the pattern and template system that is normally associated with Scheme macros, isolating a core that is simpler, can support pattern and template forms themselves as macros, and generalizes naturally to Racket's other extensions.},
  langid = {english},
  file = {C\:\\Users\\caner\\Zotero\\storage\\KJ3MZ6SZ\\Flatt et al. - 2012 - Macros that Work Together Compile-time bindings, .pdf;C\:\\Users\\caner\\Zotero\\storage\\9IWC2NZF\\375043C6746405B22014D235FA4C90C3.html}
}

@misc{gal:2006,
  title = {Incremental {{Dynamic Code Generation}} with {{Trace Trees}} {\textbar} {{Request PDF}}},
  journal = {ResearchGate},
  urldate = {2025-06-24},
  abstract = {Request PDF {\textbar} Incremental Dynamic Code Generation with Trace Trees {\textbar} The unit of compilation for traditional just-in-time compilers is the method. We have explored trace-based compilation, in which the unit of... {\textbar} Find, read and cite all the research you need on ResearchGate},
  howpublished = {https://www.researchgate.net/publication/213877755\_Incremental\_Dynamic\_Code\_Generation\_with\_Trace\_Trees},
  langid = {english},
  file = {C:\Users\caner\Zotero\storage\I8RVLKRY\213877755_Incremental_Dynamic_Code_Generation_with_Trace_Trees.html}
}

@book{gc:16,
  title = {The {{Garbage Collection Handbook}}: {{The Art}} of {{Automatic Memory Management}}},
  shorttitle = {The {{Garbage Collection Handbook}}},
  author = {Jones, Richard and Hosking, Antony and Moss, Eliot},
  year = {2011},
  month = jul,
  edition = {1st},
  publisher = {Chapman \& Hall/CRC},
  abstract = {Published in 1996, Richard Joness Garbage Collection was a milestone in the area of automatic memory management. The field has grown considerably since then, sparking a need for an updated look at the latest state-of-the-art developments. The Garbage Collection Handbook: The Art of Automatic Memory Management brings together a wealth of knowledge gathered by automatic memory management researchers and developers over the past fifty years. The authors compare the most important approaches and state-of-the-art techniques in a single, accessible framework. The book addresses new challenges to garbage collection made by recent advances in hardware and software. It explores the consequences of these changes for designers and implementers of high performance garbage collectors. Along with simple and traditional algorithms, the book covers parallel, incremental, concurrent, and real-time garbage collection. Algorithms and concepts are often described with pseudocode and illustrations. The nearly universal adoption of garbage collection by modern programming languages makes a thorough understanding of this topic essential for any programmer. This authoritative handbook gives expert insight on how different collectors work as well as the various issues currently facing garbage collectors. Armed with this knowledge, programmers can confidently select and configure the many choices of garbage collectors. Web ResourceThe books online bibliographic database at www.gchandbook.org includes over 2,500 garbage collection-related publications. Continually updated, it contains abstracts for some entries and URLs or DOIs for most of the electronically available ones. The database can be searched online or downloaded as BibTeX, PostScript, or PDF.},
  isbn = {978-1-4200-8279-1}
}

@article{ghc_llvm_backend,
  title = {An {{llVM}} Backend for {{GHC}}},
  author = {Terei, David A. and Chakravarty, Manuel M.T.},
  year = {2010},
  month = sep,
  journal = {SIGPLAN Not.},
  volume = {45},
  number = {11},
  pages = {109--120},
  issn = {0362-1340},
  doi = {10.1145/2088456.1863538},
  urldate = {2025-06-26},
  abstract = {In the presence of ever-changing computer architectures, high-quality optimising compiler backends are moving targets that require specialist knowledge and sophisticated algorithms. In this paper, we explore a new backend for the Glasgow Haskell Compiler (GHC) that leverages the Low Level Virtual Machine (LLVM), a new breed of compiler written explicitly for use by other compiler writers, not high-level programmers, that promises to enable outsourcing of low-level and architecture-dependent aspects of code generation. We discuss the conceptual challenges and our backend design. We also provide an extensive quantitative evaluation of the performance of the backend and of the code it produces.},
  file = {C:\Users\caner\Zotero\storage\VRKITGY6\Terei and Chakravarty - 2010 - An llVM backend for GHC.pdf}
}

@article{hayashizakiImprovingPerformanceTracebased2011,
  title = {Improving the Performance of Trace-Based Systems by False Loop Filtering},
  author = {Hayashizaki, Hiroshige and Wu, Peng and Inoue, Hiroshi and Serrano, Mauricio J. and Nakatani, Toshio},
  year = {2011},
  month = mar,
  journal = {ACM SIGPLAN Notices},
  volume = {46},
  number = {3},
  pages = {405--418},
  issn = {0362-1340, 1558-1160},
  doi = {10.1145/1961296.1950412},
  urldate = {2025-03-27},
  abstract = {Trace-based compilation is a promising technique for language compilers and binary translators. It offers the potential to expand the compilation scopes that have traditionally been limited by method boundaries.             Detecting repeating cyclic execution paths and capturing the detected repetitions into traces is a key requirement for trace selection algorithms to achieve good optimization and performance with small amounts of code. One important class of repetition detection is cyclic-path-based repetition detection, where a cyclic execution path (a path that starts and ends at the same instruction address) is detected as a repeating cyclic execution path.             However, we found many cyclic paths that are not repeating cyclic execution paths, which we call false loops. A common class of false loops occurs when a method is invoked from multiple call-sites. A cycle is formed between two invocations of the method from different call-sites, but which does not represent loops or recursion. False loops can result in shorter traces and smaller compilation scopes, and degrade the performance.             We propose false loop filtering, an approach to reject false loops in the repetition detection step of trace selection, and a technique called false loop filtering by call-stack-comparison, which rejects a cyclic path as a false loop if the call stacks at the beginning and the end of the cycle are different.             We applied false loop filtering to our trace-based Java™ JIT compiler that is based on IBM's J9 JVM. We found that false loop filtering achieved an average improvement of 16\% and 10\% for the DaCapo benchmark when applied to two baseline trace selection algorithms, respectively, with up to 37\% improvement for individual benchmarks. In the end, with false loop filtering, our trace-based JIT achieves a performance comparable to that of the method-based J9 JVM/JIT using the corresponding optimization level.},
  langid = {english},
  file = {C:\Users\caner\Zotero\storage\QQGFA4RB\Hayashizaki et al. - 2011 - Improving the performance of trace-based systems by false loop filtering.pdf}
}

@inproceedings{hotpath:06,
  title = {{{HotpathVM}}: {{An Effective JIT Compiler}} for {{Resource-constrained Devices}}},
  shorttitle = {{{HotpathVM}}},
  booktitle = {Proceedings of the {{2Nd International Conference}} on {{Virtual Execution Environments}}},
  author = {Gal, Andreas and Probst, Christian W. and Franz, Michael},
  year = {2006},
  series = {{{VEE}} '06},
  pages = {144--153},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/1134760.1134780},
  urldate = {2019-11-16},
  abstract = {We present a just-in-time compiler for a Java VM that is small enough to fit on resource-constrained devices, yet is surprisingly effective. Our system dynamically identifies traces of frequently executed bytecode instructions (which may span several basic blocks across several methods) and compiles them via Static Single Assignment (SSA) construction. Our novel use of SSA form in this context allows to hoist instructions across trace side-exits without necessitating expensive compensation code in off-trace paths. The overall memory consumption (code and data) of our system is only 150 kBytes, yet benchmarks show a speedup that in some cases rivals heavy-weight just-in-time compilers.},
  isbn = {978-1-59593-332-4},
  keywords = {dynamic compilation,embedded and resource-constrained systems,mixed-mode interpretive compiled systems,software trace scheduling,static single assignment form,virtual machines},
  file = {C:\Users\caner\Zotero\storage\MUT26PBM\Gal et al. - 2006 - HotpathVM An Effective JIT Compiler for Resource-.pdf}
}

@article{icfp2019,
  title = {Rebuilding {{Racket}} on {{Chez Scheme}} ({{Experience Report}})},
  author = {Flatt, Matthew and Derici, Caner and Dybvig, R. Kent and Keep, Andrew W. and Massaccesi, Gustavo E. and Spall, Sarah and {Tobin-Hochstadt}, Sam and Zeppieri, Jon},
  year = {2019},
  month = jul,
  journal = {Proc. ACM Program. Lang.},
  volume = {3},
  number = {ICFP},
  pages = {78:1--78:15},
  issn = {2475-1421},
  doi = {10.1145/3341642},
  urldate = {2019-11-12},
  abstract = {We rebuilt Racket on Chez Scheme, and it works well\&mdash;as long as we're allowed a few patches to Chez Scheme. DrRacket runs, the Racket distribution can build itself, and nearly all of the core Racket test suite passes. Maintainability and performance of the resulting implementation are good, although some work remains to improve end-to-end performance. The least predictable part of our effort was how big the differences between Racket and Chez Scheme would turn out to be and how we would manage those differences. We expect Racket on Chez Scheme to become the main Racket implementation, and we encourage other language implementers to consider Chez Scheme as a target virtual machine.},
  keywords = {Racket,Scheme},
  file = {C:\Users\caner\Zotero\storage\76JHJJYP\Flatt et al. - 2019 - Rebuilding Racket on Chez Scheme (Experience Repor.pdf}
}

@article{ir_cacm,
  title = {Intermediate Representation},
  author = {Chow, Fred},
  year = {2013},
  month = dec,
  journal = {Commun. ACM},
  volume = {56},
  number = {12},
  pages = {57--62},
  issn = {0001-0782},
  doi = {10.1145/2534706.2534720},
  urldate = {2025-06-26},
  abstract = {The increasing significance of intermediate representations in compilers.},
  file = {C:\Users\caner\Zotero\storage\FMNRNXNJ\Chow - 2013 - Intermediate representation.pdf}
}

@inproceedings{izawaAmalgamatingDifferentJIT2020b,
  title = {Amalgamating Different {{JIT}} Compilations in a Meta-Tracing {{JIT}} Compiler Framework},
  booktitle = {Proceedings of the 16th {{ACM SIGPLAN International Symposium}} on {{Dynamic Languages}}},
  author = {Izawa, Yusuke and Masuhara, Hidehiko},
  year = {2020},
  month = nov,
  pages = {1--15},
  publisher = {ACM},
  address = {Virtual USA},
  doi = {10.1145/3426422.3426977},
  urldate = {2025-06-13},
  isbn = {978-1-4503-8175-8},
  langid = {english},
  file = {C:\Users\caner\Zotero\storage\WAKJ6FNQ\Izawa and Masuhara - 2020 - Amalgamating different JIT compilations in a meta-tracing JIT compiler framework.pdf}
}

@misc{izawaLightweightMethodGenerating2025,
  title = {A {{Lightweight Method}} for {{Generating Multi-Tier JIT Compilation Virtual Machine}} in a {{Meta-Tracing Compiler Framework}}},
  author = {Izawa, Yusuke and Masuhara, Hidehiko and {Bolz-Tereick}, Carl Friedrich},
  year = {2025},
  month = apr,
  number = {arXiv:2504.17460},
  eprint = {2504.17460},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.17460},
  urldate = {2025-05-28},
  abstract = {Meta-compiler frameworks, such as RPython and Graal/Truffle, generate high-performance virtual machines (VMs) from interpreter definitions. Although they generate VMs with high-quality just-in-time (JIT) compilers, they still lack an important feature that dedicated VMs (i.e., VMs that are developed for specific languages) have, namely {\textbackslash}emph\{multi-tier compilation\}. Multi-tier compilation uses light-weight compilers at early stages and highly-optimizing compilers at later stages in order to balance between compilation overheads and code quality. We propose a novel approach to enabling multi-tier compilation in the VMs generated by a meta-compiler framework. Instead of extending the JIT compiler backend of the framework, our approach drives an existing (heavyweight) compiler backend in the framework to quickly generate unoptimized native code by merely embedding directives and compile-time operations into interpreter definitions. As a validation of the approach, we developed 2SOM, a Simple Object Machine with a two-tier JIT compiler based on RPython. 2SOM first applies the tier-1 threaded code generator that is generated by our proposed technique, then, to the loops that exceed a threshold, applies the tier-2 tracing JIT compiler that is generated by the original RPython framework. Our performance evaluation that runs a program with a realistic workload showed that 2SOM improved, when compared against an RPython-based VM, warm-up performance by 15{\textbackslash}\%, with merely a 5{\textbackslash}\% reduction in peak performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Programming Languages},
  file = {C\:\\Users\\caner\\Zotero\\storage\\SX29ZQUE\\Izawa et al. - 2025 - A Lightweight Method for Generating Multi-Tier JIT Compilation Virtual Machine in a Meta-Tracing Com.pdf;C\:\\Users\\caner\\Zotero\\storage\\7WFHXFWI\\2504.html}
}

@article{jit-history:03,
  title = {A {{Brief History}} of {{Just-in-time}}},
  author = {Aycock, John},
  year = {2003},
  month = jun,
  journal = {ACM Comput. Surv.},
  volume = {35},
  number = {2},
  pages = {97--113},
  issn = {0360-0300},
  doi = {10.1145/857076.857077},
  urldate = {2019-12-01},
  abstract = {Software systems have been using "just-in-time" compilation (JIT) techniques since the 1960s. Broadly, JIT compilation includes any translation performed dynamically, after a program has started execution. We examine the motivation behind JIT compilation and constraints imposed on JIT compilation systems, and present a classification scheme for such systems. This classification emerges as we survey forty years of JIT work, from 1960--2000.},
  keywords = {dynamic compilation,Just-in-time compilation},
  file = {C:\Users\caner\Zotero\storage\48RY7S5V\Aycock - 2003 - A Brief History of Just-in-time.pdf}
}

@article{lambdamachine,
  title = {Trace-Based {{Just-in-time Compilation}} for {{Lazy Functional Programming Languages}}},
  author = {Schilling, Thomas},
  pages = {213},
  langid = {english},
  file = {C:\Users\caner\Zotero\storage\U76SZ52S\Schilling - Trace-based Just-in-time Compilation for Lazy Func.pdf}
}

@inproceedings{layering:09,
  title = {Optimization of {{Dynamic Languages Using Hierarchical Layering}} of {{Virtual Machines}}},
  booktitle = {Proceedings of the 5th {{Symposium}} on {{Dynamic Languages}}},
  author = {Yermolovich, Alexander and Wimmer, Christian and Franz, Michael},
  year = {2009},
  series = {{{DLS}} '09},
  pages = {79--88},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/1640134.1640147},
  urldate = {2019-11-29},
  abstract = {Creating an interpreter is a simple and fast way to implement a dynamic programming language. With this ease also come major drawbacks. Interpreters are significantly slower than compiled machine code because they have a high dispatch overhead and cannot perform optimizations. To overcome these limitations, interpreters are commonly combined with just-in-time compilers to improve the overall performance. However, this means that a just-in-time compiler has to be implemented for each language. We explore the approach of taking an interpreter of a dynamic language and running it on top of an optimizing trace-based virtual machine, i.e., we run a guest VM on top of a host VM. The host VM uses trace recording to observe the guest VM executing the application program. Each recorded trace represents a sequence of guest VM bytecodes corresponding to a given execution path through the application program. The host VM optimizes and compiles these traces to machine code, thus eliminating the need for a custom just-in-time compiler for the guest VM. The guest VM only needs to provide basic information about its interpreter loop to the host VM.},
  isbn = {978-1-60558-769-1},
  keywords = {actionscript,dynamic languages,hierarchical virtual machines,lua,trace compilation},
  file = {C:\Users\caner\Zotero\storage\FSVM3V8T\Yermolovich et al. - 2009 - Optimization of Dynamic Languages Using Hierarchic.pdf}
}

@inproceedings{llvm_verification,
  title = {Leveraging {{Compiler Intermediate Representation}} for {{Multi-}} and {{Cross-Language Verification}}},
  booktitle = {Verification, {{Model Checking}}, and {{Abstract Interpretation}}: 21st {{International Conference}}, {{VMCAI}} 2020, {{New Orleans}}, {{LA}}, {{USA}}, {{January}} 16--21, 2020, {{Proceedings}}},
  author = {Garzella, Jack J. and Baranowski, Marek and He, Shaobo and Rakamari{\'c}, Zvonimir},
  year = {2020},
  month = jan,
  pages = {90--111},
  publisher = {Springer-Verlag},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-030-39322-9_5},
  urldate = {2025-06-26},
  abstract = {Developers nowadays regularly use numerous programming languages with different characteristics and trade-offs. Unfortunately, implementing a software verifier for a new language from scratch is a large and tedious undertaking, requiring expert knowledge in multiple domains, such as compilers, verification, and constraint solving. Hence, only a tiny fraction of the used languages has readily available software verifiers to aid in the development of correct programs. In the past decade, there has been a trend of leveraging popular compiler intermediate representations (IRs), such as LLVM IR, when implementing software verifiers. Processing IR promises out-of-the-box multi- and cross-language verification since, at least in theory, a verifier ought to be able to handle a program in any programming language (and their combination) that can be compiled into the IR. In practice though, to the best of our knowledge, nobody has explored the feasibility and ease of such integration of new languages. In this paper, we provide a procedure for adding support for a new language into an IR-based verification toolflow. Using our procedure, we extend the SMACK verifier with prototypical support for 6 additional languages. We assess the quality of our extensions through several case studies, and we describe our experience in detail to guide future efforts in this area.},
  isbn = {978-3-030-39321-2}
}

@inproceedings{loop-aware:12,
  title = {Loop-Aware {{Optimizations}} in {{PyPy}}'s {{Tracing JIT}}},
  booktitle = {Proceedings of the 8th {{Symposium}} on {{Dynamic Languages}}},
  author = {Ard{\"o}, H{\aa}kan and Bolz, Carl Friedrich and Fija{\l}kowski, Maciej},
  year = {2012},
  series = {{{DLS}} '12},
  pages = {63--72},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/2384577.2384586},
  urldate = {2019-03-22},
  abstract = {One of the nice properties of a tracing just-in-time compiler (JIT) is that many of its optimizations are simple, requiring one forward pass only. This is not true for loop-invariant code motion which is a very important optimization for code with tight kernels. Especially for dynamic languages that typically perform quite a lot of loop invariant type checking, boxed value unwrapping and virtual method lookups. In this paper we explain a scheme pioneered within the context of the LuaJIT project for making basic optimizations loop-aware by using a simple pre-processing step on the trace without changing the optimizations themselves. We have implemented the scheme in RPython's tracing JIT compiler. PyPy's Python JIT executing simple numerical kernels can become up to two times faster, bringing the performance into the ballpark of static language compilers.},
  isbn = {978-1-4503-1564-7},
  keywords = {loop-invariant code motion,optimization,tracing JIT},
  file = {C:\Users\caner\Zotero\storage\QSMR5HNR\Ardö et al. - 2012 - Loop-aware Optimizations in PyPy's Tracing JIT.pdf}
}

@misc{LuaJITLanguageToolkit,
  title = {{{LuaJIT Language Toolkit}}}
}

@inproceedings{malloc-removal:11,
  title = {Allocation {{Removal}} by {{Partial Evaluation}} in a {{Tracing JIT}}},
  booktitle = {Proceedings of the 20th {{ACM SIGPLAN Workshop}} on {{Partial Evaluation}} and {{Program Manipulation}}},
  author = {Bolz, Carl Friedrich and Cuni, Antonio and FijaBkowski, Maciej and Leuschel, Michael and Pedroni, Samuele and Rigo, Armin},
  year = {2011},
  series = {{{PEPM}} '11},
  pages = {43--52},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/1929501.1929508},
  urldate = {2019-11-16},
  abstract = {The performance of many dynamic language implementations suffers from high allocation rates and runtime type checks. This makes dynamic languages less applicable to purely algorithmic problems, despite their growing popularity. In this paper we present a simple compiler optimization based on online partial evaluation to remove object allocations and runtime type checks in the context of a tracing JIT. We evaluate the optimization using a Python VM and find that it gives good results for all our (real-life) benchmarks.},
  isbn = {978-1-4503-0485-6},
  keywords = {optimization,partial evaluation,tracing jit},
  file = {C:\Users\caner\Zotero\storage\7GRXDUIE\Bolz et al. - 2011 - Allocation Removal by Partial Evaluation in a Trac.pdf}
}

@book{millerCommonLanguageInfrastructure2003,
  title = {The {{Common Language Infrastructure Annotated Standard}}},
  author = {Miller, Jim S. and Ragsdale, Susann},
  year = {2003},
  month = sep,
  publisher = {Addison-Wesley Longman Publishing Co., Inc.},
  address = {USA},
  abstract = {From the Publisher The Common Language Infrastructure Annotated Standard is the definitive guide to understanding the annotated specification for the Common Language Infrastructure (CLI) standard. With annotations and code samples from both the ECMA standards committee and the Microsoft Common Language Runtime (CLR) team, this book goes beyond the online documentation to clarify and amplify the original standard and describe its implementation. The core of this book is the international CLI standard. The text describes the CLI and its parts and provides all the information needed to implement a Virtual Execution System (VES) or design a compiler that runs on top of a VES and generates portable code. Author Jim Miller draws upon his experience as editor of the CLI standard and lead of the Microsoft CLR team to guide readers through the CLI blueprint and to a complete understanding of the CLR. Features of this book include: A heavily annotated architectural overview of the standard A description of the semantics of metadata A complete specification of the Portable Executable (PE) file format Coverage of file format and metadata layout An overview of the CLI libraries A detailed description of the Common Intermediate Language (CIL) instruction set Sample programs and other annexes to the standard An enhanced online index that allows readers to quickly and easily search the entire text for specific topics The Common Language Infrastructure Annotated Standard is the single source programmers, language and tool designers, and library and VES developers need to renderfully comprehensible.},
  isbn = {978-0-321-15493-4}
}

@misc{mozblog,
  title = {You Lose More When Slow than You Gain When Fast -- {{Nicholas Nethercote}}},
  urldate = {2025-06-12},
  howpublished = {https://blog.mozilla.org/nnethercote/2011/05/31/you-lose-more-when-slow-than-you-gain-when-fast/},
  file = {C:\Users\caner\Zotero\storage\RG2D8I54\you-lose-more-when-slow-than-you-gain-when-fast.html}
}

@inproceedings{multi-stage:17,
  title = {Refining Semantics for Multi-Stage Programming},
  booktitle = {Proceedings of the 16th {{ACM SIGPLAN International Conference}} on {{Generative Programming}}: {{Concepts}} and {{Experiences}}},
  author = {Ge, Rui and Garcia, Ronald},
  year = {2017},
  month = oct,
  series = {{{GPCE}} 2017},
  pages = {2--14},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3136040.3136047},
  urldate = {2025-07-16},
  abstract = {The multi-stage programming paradigm supports runtime code generation and execution. Though powerful, its potential is impeded by the lack of static analysis support. Van Horn and Might proposed a general-purpose approach to systematically develop static analyses by transforming an environmental abstract machine, which evolves a control string, an environment and a continuation as a program evaluates. To the best of our knowledge, no such semantics exists for a multi-stage language like MetaML.  We develop and prove correct an environmental abstract machine semantics for MetaML by gradually refining the reference substitutional structural operational semantics. Highlights of our approach include leveraging explicit substitutions to bridge the gap between substitutional and environmental semantics, and devising meta-environments to model the complexities of variable bindings in multi-stage environmental semantics.},
  isbn = {978-1-4503-5524-7},
  file = {C:\Users\caner\Zotero\storage\6T3SY8T2\Ge and Garcia - 2017 - Refining semantics for multi-stage programming.pdf}
}

@inproceedings{practical-partial,
  title = {Practical Partial Evaluation for High-Performance Dynamic Language Runtimes},
  booktitle = {Proceedings of the 38th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {W{\"u}rthinger, Thomas and Wimmer, Christian and Humer, Christian and W{\"o}{\ss}, Andreas and Stadler, Lukas and Seaton, Chris and Duboscq, Gilles and Simon, Doug and Grimmer, Matthias},
  year = {2017},
  month = jun,
  pages = {662--676},
  publisher = {ACM},
  address = {Barcelona Spain},
  doi = {10.1145/3062341.3062381},
  urldate = {2025-06-17},
  abstract = {Most high-performance dynamic language virtual machines duplicate language semantics in the interpreter, compiler, and runtime system. This violates the principle to not repeat yourself. In contrast, we define languages solely by writing an interpreter. The interpreter performs specializations, e.g., augments the interpreted program with type information and profiling information. Compiled code is derived automatically using partial evaluation while incorporating these specializations. This makes partial evaluation practical in the context of dynamic languages: It reduces the size of the compiled code while still compiling all parts of an operation that are relevant for a particular program. When a speculation fails, execution transfers back to the interpreter, the program re-specializes in the interpreter, and later partial evaluation again transforms the new state of the interpreter to compiled code. We evaluate our approach by comparing our implementations of JavaScript, Ruby, and R with best-inclass specialized production implementations. Our generalpurpose compilation system is competitive with production systems even when they have been heavily optimized for the one language they support. For our set of benchmarks, our speedup relative to the V8 JavaScript VM is 0.83x, relative to JRuby is 3.8x, and relative to GNU R is 5x.},
  isbn = {978-1-4503-4988-8},
  langid = {english},
  file = {C:\Users\caner\Zotero\storage\8V8IYVQT\Würthinger et al. - 2017 - Practical partial evaluation for high-performance dynamic language runtimes.pdf}
}

@inproceedings{pycketmain,
  title = {Pycket: {{A Tracing JIT}} for a {{Functional Language}}},
  shorttitle = {Pycket},
  booktitle = {Proceedings of the 20th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Bauman, Spenser and Bolz, Carl Friedrich and Hirschfeld, Robert and Kirilichev, Vasily and Pape, Tobias and Siek, Jeremy G. and {Tobin-Hochstadt}, Sam},
  year = {2015},
  series = {{{ICFP}} 2015},
  pages = {22--34},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/2784731.2784740},
  urldate = {2019-03-22},
  abstract = {We present Pycket, a high-performance tracing JIT compiler for Racket. Pycket supports a wide variety of the sophisticated features in Racket such as contracts, continuations, classes, structures, dynamic binding, and more. On average, over a standard suite of benchmarks, Pycket outperforms existing compilers, both Racket's JIT and other highly-optimizing Scheme compilers. Further, Pycket provides much better performance for Racket proxies than existing systems, dramatically reducing the overhead of contracts and gradual typing. We validate this claim with performance evaluation on multiple existing benchmark suites. The Pycket implementation is of independent interest as an application of the RPython meta-tracing framework (originally created for PyPy), which automatically generates tracing JIT compilers from interpreters. Prior work on meta-tracing focuses on bytecode interpreters, whereas Pycket is a high-level interpreter based on the CEK abstract machine and operates directly on abstract syntax trees. Pycket supports proper tail calls and first-class continuations. In the setting of a functional language, where recursion and higher-order functions are more prevalent than explicit loops, the most significant performance challenge for a tracing JIT is identifying which control flows constitute a loop---we discuss two strategies for identifying loops and measure their impact.},
  isbn = {978-1-4503-3669-7},
  keywords = {contracts,functional languages,JIT compilers,Racket,tracing},
  file = {C:\Users\caner\Zotero\storage\2JP3BUTY\Bauman et al. - 2015 - Pycket A Tracing JIT for a Functional Language.pdf}
}

@article{pycketmain2,
  title = {Sound Gradual Typing: Only Mostly Dead},
  shorttitle = {Sound Gradual Typing},
  author = {Bauman, Spenser and {Bolz-Tereick}, Carl Friedrich and Siek, Jeremy and {Tobin-Hochstadt}, Sam},
  year = {2017},
  month = oct,
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {1},
  number = {OOPSLA},
  pages = {1--24},
  issn = {24751421},
  doi = {10.1145/3133878},
  urldate = {2019-11-10},
  langid = {english},
  file = {C:\Users\caner\Zotero\storage\DSSSEG9Z\Bauman et al. - 2017 - Sound gradual typing only mostly dead.pdf}
}

@inproceedings{pyhaskell,
  title = {Trace-Based Just-Intime Compiler for {{Haskell}} with {{RPython}}},
  author = {Thomassen, Wiik},
  year = {2013},
  abstract = {Can Haskell benefit from tracing JIT optimization techniques, and is the RPython translation toolchain suitable for purely functional, lazy languages such as Haskell? RPython has been used to implement VMs for many different programming languages, but not for any purely functional or lazy languages. Haskell has achieved impressive speed with ahead-of-time optimizations. Attempts at trace-based JIT optimizations of Haskell have so far not achieved greater speed than static compilation. PyHaskell, a prototype Haskell VM with a meta-tracing JIT compiler written in RPython, shows that the RPython toolchain is suitable for Haskell. While the meta-tracer greatly speeds up PyHaskell, it does not yet beat GHC.},
  keywords = {Ahead-of-time compilation,Compiler,INtime RTOS / INtime for Windows,Just-in-time-concept,Lazy evaluation,Mathematical optimization,OpenVMS,Programming language,Programming Languages,Prototype,PyPy,Static library,The Glorious Glasgow Haskell Compilation System,Toolchain,Tracer,Tracing just-in-time compilation},
  file = {C:\Users\caner\Zotero\storage\V3VMBSPM\Thomassen - 2013 - Trace-based just-intime compiler for Haskell with .pdf}
}

@inproceedings{pypy-main,
  title = {Tracing the {{Meta-level}}: {{PyPy}}'s {{Tracing JIT Compiler}}},
  shorttitle = {Tracing the {{Meta-level}}},
  booktitle = {Proceedings of the 4th {{Workshop}} on the {{Implementation}}, {{Compilation}}, {{Optimization}} of {{Object-Oriented Languages}} and {{Programming Systems}}},
  author = {Bolz, Carl Friedrich and Cuni, Antonio and Fijalkowski, Maciej and Rigo, Armin},
  year = {2009},
  series = {{{ICOOOLPS}} '09},
  pages = {18--25},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/1565824.1565827},
  urldate = {2019-03-22},
  abstract = {We attempt to apply the technique of Tracing JIT Compilers in the context of the PyPy project, i.e., to programs that are interpreters for some dynamic languages, including Python. Tracing JIT compilers can greatly speed up programs that spend most of their time in loops in which they take similar code paths. However, applying an unmodified tracing JIT to a program that is itself a bytecode interpreter results in very limited or no speedup. In this paper we show how to guide tracing JIT compilers to greatly improve the speed of bytecode interpreters. One crucial point is to unroll the bytecode dispatch loop, based on two kinds of hints provided by the implementer of the bytecode interpreter. We evaluate our technique by applying it to two PyPy interpreters: one is a small example, and the other one is the full Python interpreter.},
  isbn = {978-1-60558-541-3},
  file = {C:\Users\caner\Zotero\storage\ZW64A5W8\Bolz et al. - 2009 - Tracing the Meta-level PyPy's Tracing JIT Compile.pdf}
}

@inproceedings{pypy06,
  title = {{{PyPy}}'s {{Approach}} to {{Virtual Machine Construction}}},
  booktitle = {Companion to the 21st {{ACM SIGPLAN Symposium}} on {{Object-oriented Programming Systems}}, {{Languages}}, and {{Applications}}},
  author = {Rigo, Armin and Pedroni, Samuele},
  year = {2006},
  series = {{{OOPSLA}} '06},
  pages = {944--953},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/1176617.1176753},
  urldate = {2019-11-12},
  abstract = {The PyPy project seeks to prove both on a research and a practical level the feasibility of constructing a virtual machine (VM) for a dynamic language in a dynamic language - in this case, Python. The aim is to translate (i.e. compile) the VM to arbitrary target environments, ranging in level from C/Posix to Smalltalk/Squeak via Java and CLI/.NET, while still being of reasonable efficiency within these environments.A key tool to achieve this goal is the systematic reuse of the Python language as a system programming language at various levels of our architecture and translation process. For each level, we design a corresponding type system and apply a generic type inference engine - for example, the garbage collector is written in a style that manipulates simulated pointer and address objects, and when translated to C these operations become C-level pointer and address instructions.},
  isbn = {978-1-59593-491-8},
  keywords = {metacircularity,Python,retargettable code generation,type inference,virtual machine},
  file = {C:\Users\caner\Zotero\storage\ALGTU6SG\Rigo and Pedroni - 2006 - PyPy's Approach to Virtual Machine Construction.pdf}
}

@misc{pypy08,
  title = {The {{Architecture}} of {{Open Source Applications}} ({{Volume}} 2): {{PyPy}}},
  urldate = {2019-11-12},
  howpublished = {https://www.aosabook.org/en/pypy.html},
  file = {C:\Users\caner\Zotero\storage\B2QBSUWA\pypy.html}
}

@article{racket:create-langs,
  title = {Creating Languages in {{Racket}}},
  author = {Flatt, Matthew},
  year = {2012},
  month = jan,
  journal = {Commun. ACM},
  volume = {55},
  number = {1},
  pages = {48--56},
  issn = {0001-0782},
  doi = {10.1145/2063176.2063195},
  urldate = {2025-07-08},
  abstract = {Sometimes you just have to make a better mousetrap.},
  file = {C:\Users\caner\Zotero\storage\ND6ZFPJ2\Flatt - 2012 - Creating languages in Racket.pdf}
}

@article{randomizedTesting,
  title = {Random Testing for Higher-Order, Stateful Programs},
  author = {Klein, Casey and Flatt, Matthew and Findler, Robert Bruce},
  year = {2010},
  month = oct,
  journal = {SIGPLAN Not.},
  volume = {45},
  number = {10},
  pages = {555--566},
  issn = {0362-1340},
  doi = {10.1145/1932682.1869505},
  urldate = {2025-06-28},
  abstract = {Testing is among the most effective tools available for finding bugs. Still, we know of no automatic technique for generating test cases that expose bugs involving a combination of mutable state and callbacks, even though objects and method overriding set up exactly that combination. For such cases, a test generator must create callbacks or subclasses that aggressively exercise side-effecting operations using combinations of generated objects.This paper presents a new algorithm for randomly testing programs that use state and callbacks. Our algorithm exploits a combination of contracts and environment bindings to guide the test-case generator toward interesting inputs. Our prototype implementation for Racket (formerly PLT Scheme) - which has a Java-like class system, but with first-class classes as well as gbeta-like augmentable methods - uncovered dozens of bugs in a well-tested and widely used text-editor library.We describe our approach in a precise, formal notation, borrowing the techniques used to describe operational semantics and type systems. The formalism enables us to provide a compact and self-contained explanation of the core of our technique without the ambiguity usually present in pseudo-code descriptions.},
  file = {C:\Users\caner\Zotero\storage\Q29ZZ87P\Klein et al. - 2010 - Random testing for higher-order, stateful programs.pdf}
}

@book{redexBook,
  title = {Semantics {{Engineering}} with {{PLT Redex}}},
  author = {Felleisen, Matthias and Findler, Robert Bruce and Flatt, Matthew},
  year = {2009},
  month = jul,
  edition = {1st},
  publisher = {The MIT Press},
  abstract = {This text is the first comprehensive presentation of reduction semantics in one volume; it also introduces the first reliable and easy-to-use tool set for such forms of semantics. Software engineers have long known that automatic tool support is critical for rapid prototyping and modeling, and this book is addressed to the working semantics engineer (graduate student or professional language designer). The book comes with a prototyping tool suite to develop, explore, test, debug, and publish semantic models of programming languages. With PLT Redex, semanticists can formulate models as grammars and reduction models on their computers with the ease of paper and pencil. The text first presents a framework for the formulation of language models, focusing on equational calculi and abstract machines, then introduces PLT Redex, a suite of software tools for expressing these models as PLT Redex models. Finally, experts describe a range of models formulated in Redex. PLT Redex comes with the PLT Scheme implementation, available free at http://www.plt-scheme.org/. Readers can download the software and experiment with Redex as they work their way through the book. For more information (including the preface, a sample syllabus, and a quick introduction to Redex), see the Redex website at http://redex.plt-scheme.org/. Matthias Felleisen, Robert Bruce Findler, and Matthew Flatt are the authors (with Shiram Krishnamurthi) of How to Design Programs: An Introduction to Programming and Computing, also published by the MIT Press.},
  isbn = {978-0-262-06275-6}
}

@inproceedings{rpython07,
  title = {{{RPython}}: {{A Step Towards Reconciling Dynamically}} and {{Statically Typed OO Languages}}},
  shorttitle = {{{RPython}}},
  booktitle = {Proceedings of the 2007 {{Symposium}} on {{Dynamic Languages}}},
  author = {Ancona, Davide and Ancona, Massimo and Cuni, Antonio and Matsakis, Nicholas D.},
  year = {2007},
  series = {{{DLS}} '07},
  pages = {53--64},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/1297081.1297091},
  urldate = {2019-11-12},
  abstract = {Although the C-based interpreter of Python is reasonably fast, implementations on the CLI or the JVM platforms offers some advantages in terms of robustness and interoperability. Unfortunately, because the CLI and JVM are primarily designed to execute statically typed, object-oriented languages, most dynamic language implementations cannot use the native bytecodes for common operations like method calls and exception handling; as a result, they are not able to take full advantage of the power offered by the CLI and JVM. We describe a different approach that attempts to preserve the flexibility of Python, while still allowing for efficient execution. This is achieved by limiting the use of the more dynamic features of Python to an initial, bootstrapping phase. This phase is used to construct a final RPython (Restricted Python) program that is actually executed. RPython is a proper subset of Python, is statically typed, and does not allow dynamic modification of class or method definitions; however, it can still take advantage of Python features such as mixins and first-class methods and classes. This paper presents an overview of RPython, including its design and its translation to both CLI and JVM bytecode. We show how the bootstrapping phase can be used to implement advanced features, like extensible classes and generative programming. We also discuss what work remains before RPython is truly ready for general use, and compare the performance of RPython with that of other approaches.},
  isbn = {978-1-59593-868-8},
  keywords = {.NET,JVM,Python},
  file = {C:\Users\caner\Zotero\storage\MVZCMYBI\Ancona et al. - 2007 - RPython A Step Towards Reconciling Dynamically an.pdf}
}

@inproceedings{rpython09,
  title = {{{PyGirl}}: {{Generating Whole-System VMs}} from High-Level Models Using {{PyPy}}},
  shorttitle = {{{PyGirl}}},
  author = {Bruni, Camillo and Verwaest, Toon},
  year = {2009},
  month = jun,
  volume = {33},
  pages = {328--347},
  doi = {10.1007/978-3-642-02571-6_19},
  abstract = {Virtual machines (VMs) emulating hardware devices are generally implemented in low-level languages for performance reasons. This results in unmaintainable systems that are difficult to understand. In this paper we report on our experience using the PyPy toolchain to improve the portability and reduce the complexity of whole-system VM implementations. As a case study we implement a VM prototype for a Nintendo Game Boy, called PyGirl, in which the high-level model is separated from low-level VM implementation issues. We shed light on the process of refactoring from a low-level VM implementation in Java to a high-level model in RPython. We show that our whole-system VM written with PyPy is significantly less complex than standard implementations, without substantial loss in performance.},
  file = {C:\Users\caner\Zotero\storage\ESEZHPD5\Bruni and Verwaest - 2009 - PyGirl Generating Whole-System VMs from high-leve.pdf}
}

@inproceedings{runtime-feedback:11,
  title = {Runtime {{Feedback}} in a {{Meta-tracing JIT}} for {{Efficient Dynamic Languages}}},
  booktitle = {Proceedings of the 6th {{Workshop}} on {{Implementation}}, {{Compilation}}, {{Optimization}} of {{Object-Oriented Languages}}, {{Programs}} and {{Systems}}},
  author = {Bolz, Carl Friedrich and Cuni, Antonio and Fija{\textbackslash}lkowski, Maciej and Leuschel, Michael and Pedroni, Samuele and Rigo, Armin},
  year = {2011},
  series = {{{ICOOOLPS}} '11},
  pages = {9:1--9:8},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/2069172.2069181},
  urldate = {2019-03-22},
  abstract = {Meta-tracing JIT compilers can be applied to a variety of different languages without explicitly encoding language semantics into the compiler. So far, they lacked a way to give the language implementor control over runtime feedback. This restricted their performance. In this paper we describe the mechanisms in PyPy's meta-tracing JIT that can be used to control runtime feedback in language-specific ways. These mechanisms are flexible enough to express classical VM techniques such as maps and runtime type feedback.},
  isbn = {978-1-4503-0894-6},
  keywords = {code generation,interpreter,meta-programming,runtime feedback,tracing JIT},
  file = {C:\Users\caner\Zotero\storage\WQ2DSCQT\Bolz et al. - 2011 - Runtime Feedback in a Meta-tracing JIT for Efficie.pdf}
}

@article{samth:11,
  title = {Languages as Libraries},
  author = {{Tobin-Hochstadt}, Sam and {St-Amour}, Vincent and Culpepper, Ryan and Flatt, Matthew and Felleisen, Matthias},
  pages = {10},
  abstract = {Programming language design benefits from constructs for extending the syntax and semantics of a host language. While C's stringbased macros empower programmers to introduce notational shorthands, the parser-level macros of Lisp encourage experimentation with domain-specific languages. The Scheme programming language improves on Lisp with macros that respect lexical scope.},
  langid = {english},
  file = {C:\Users\caner\Zotero\storage\Y47HQ2G4\Tobin-Hochstadt et al. - Languages as libraries.pdf}
}

@article{sccp:91,
  title = {Constant {{Propagation}} with {{Conditional Branches}}},
  author = {Wegman, Mark N. and Zadeck, F. Kenneth},
  year = {1991},
  month = apr,
  journal = {ACM Trans. Program. Lang. Syst.},
  volume = {13},
  number = {2},
  pages = {181--210},
  issn = {0164-0925},
  doi = {10.1145/103135.103136},
  urldate = {2019-11-28},
  abstract = {Constant propagation is a well-known global flow analysis problem. The goal of constant propagation is to discover values that are constant on all possible executions of a program and to propagate these constant values as far foward through the program as possible. Expressions whose operands are all constants can be evaluated at compile time and the results propagated further. Using the algorithms presented in this paper can produce smaller and faster compiled programs. The same algorithms can be used for other kinds of analyses (e.g., type of determination). We present four algorithms in this paper, all conservitive in the sense that all constants may not be found, but each constant found is constant over all possible executions of the program. These algorithms are among the simplest, fastest, and most powerful global constant propagation algorithms known. We also present a new algorithm that performs a form of interprocedural data flow analysis in which aliasing information is gathered in conjunction with constant progagation. Several variants of this algorithm are considered.},
  keywords = {abstract interpretation,code optimization,constant propagation,control flow graph,interprocedural analysis,procedure integration,static single assignment form,type determination},
  file = {C:\Users\caner\Zotero\storage\T3TVZ5UD\Wegman and Zadeck - 1991 - Constant Propagation with Conditional Branches.pdf}
}

@inproceedings{schneiderEfficientHandlingGuards2012,
  title = {The Efficient Handling of Guards in the Design of {{RPython}}'s Tracing {{JIT}}},
  booktitle = {Proceedings of the Sixth {{ACM}} Workshop on {{Virtual}} Machines and Intermediate Languages},
  author = {Schneider, David and Bolz, Carl Friedrich},
  year = {2012},
  month = oct,
  pages = {3--12},
  publisher = {ACM},
  address = {Tucson Arizona USA},
  doi = {10.1145/2414740.2414743},
  urldate = {2025-06-20},
  isbn = {978-1-4503-1633-0},
  langid = {english},
  file = {C:\Users\caner\Zotero\storage\EWXZMD8Y\Schneider and Bolz - 2012 - The efficient handling of guards in the design of RPython's tracing JIT.pdf}
}

@inproceedings{schneiderEfficientHandlingGuards2012a,
  title = {The Efficient Handling of Guards in the Design of {{RPython}}'s Tracing {{JIT}}},
  booktitle = {Proceedings of the Sixth {{ACM}} Workshop on {{Virtual}} Machines and Intermediate Languages},
  author = {Schneider, David and Bolz, Carl Friedrich},
  year = {2012},
  month = oct,
  series = {{{VMIL}} '12},
  pages = {3--12},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2414740.2414743},
  urldate = {2025-06-20},
  abstract = {Tracing just-in-time (JIT) compilers record linear control flow paths, inserting operations called guards at points of possible divergence. These operations occur frequently in generated traces and therefore it is important to design and implement them carefully to find the right trade-off between deoptimization, memory overhead, and (partly) execution speed. In this paper, we perform an empirical analysis of runtime properties of guards. This is used to guide the design of guards in the RPython tracing JIT.},
  isbn = {978-1-4503-1633-0},
  file = {C:\Users\caner\Zotero\storage\2PWWZY7I\Schneider and Bolz - 2012 - The efficient handling of guards in the design of RPython's tracing JIT.pdf}
}

@inproceedings{self-hosted-tachyon,
  title = {Bootstrapping a Self-Hosted Research Virtual Machine for {{JavaScript}}: An Experience Report},
  shorttitle = {Bootstrapping a Self-Hosted Research Virtual Machine for {{JavaScript}}},
  booktitle = {Proceedings of the 7th Symposium on {{Dynamic}} Languages},
  author = {{Chevalier-Boisvert}, Maxime and Lavoie, Erick and Feeley, Marc and Dufour, Bruno},
  year = {2011},
  month = oct,
  series = {{{DLS}} '11},
  pages = {61--72},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2047849.2047858},
  urldate = {2025-07-14},
  abstract = {JavaScript is one of the most widely used dynamic languages. The performance of existing JavaScript VMs, however, is lower than that of VMs for static languages. There is a need for a research VM to easily explore new implementation approaches. This paper presents the Tachyon JavaScript VM which was designed to be flexible and to allow experimenting with new approaches for the execution of JavaScript. The Tachyon VM is itself implemented in JavaScript and currently supports a subset of the full language that is sufficient to bootstrap itself. The paper discusses the architecture of the system and in particular the bootstrapping of a self-hosted VM. Preliminary performance results indicate that our VM, with few optimizations, can already execute code faster than a commercial JavaScript interpreter on some benchmarks.},
  isbn = {978-1-4503-0939-4},
  file = {C:\Users\caner\Zotero\storage\CPZBTXFR\Chevalier-Boisvert et al. - 2011 - Bootstrapping a self-hosted research virtual machine for JavaScript an experience report.pdf}
}

@article{self-maps:89,
  title = {An Efficient Implementation of {{SELF}} a Dynamically-Typed Object-Oriented Language Based on Prototypes},
  author = {Chambers, C. and Ungar, D. and Lee, E.},
  year = {1989},
  month = sep,
  journal = {SIGPLAN Not.},
  volume = {24},
  number = {10},
  pages = {49--70},
  issn = {0362-1340},
  doi = {10.1145/74878.74884},
  urldate = {2025-07-16},
  abstract = {We have developed and implemented techniques that double the performance of dynamically-typed object-oriented languages. Our SELF implementation runs twice as fast as the fastest Smalltalk implementation, despite SELF's lack of classes and explicit variables.To compensate for the absence of classes, our system uses implementation-level maps to transparently group objects cloned from the same prototype, providing data type information and eliminating the apparent space overhead for prototype-based systems. To compensate for dynamic typing, user-defined control structures, and the lack of explicit variables, our system dynamically compiles multiple versions of a source method, each customized according to its receiver's map. Within each version the type of the receiver is fixed, and thus the compiler can statically bind and inline all messages sent to self. Message splitting and type prediction extract and preserve even more static type information, allowing the compiler to inline many other messages. Inlining dramatically improves performance and eliminates the need to hard-wire low-level methods such as +,==, and ifTrue:.Despite inlining and other optimizations, our system still supports interactive programming environments. The system traverses internal dependency lists to invalidate all compiled methods affected by a programming change. The debugger reconstructs inlined stack frames from compiler-generated debugging information, making inlining invisible to the SELF programmer.},
  file = {C:\Users\caner\Zotero\storage\NNQDP37F\Chambers et al. - 1989 - An efficient implementation of SELF a dynamically-typed object-oriented language based on prototypes.pdf}
}

@book{sicpbook,
  title = {Structure and {{Interpretation}} of {{Computer Programs}}},
  author = {Abelson, Harold and Sussman, Gerald Jay},
  year = {1996},
  month = jul,
  publisher = {The MIT Press},
  urldate = {2025-07-11},
  abstract = {Structure and Interpretation of Computer Programs has had a dramatic impact on computer science curricula over the past decade. This long-awaited revision contains changes throughout the text. There are new implementations of most of the major programming systems in the book, including the interpreters and compilers, and the authors have incorporated many small changes that reflect their experience teaching the course at MIT since the first edition was published. A new theme has been introduced that emphasizes the central role played by different approaches to dealing with time in computational models: objects with state, concurrent programming, functional programming and lazy evaluation, and nondeterministic programming. There are new example sections on higher-order procedures in graphics and on applications of stream processing in numerical programming, and many new exercises. In addition, all the programs have been reworked to run in any Scheme implementation that adheres to the IEEE standard.},
  isbn = {978-0-262-51087-5 978-0-262-31091-8},
  langid = {english},
  keywords = {thema EDItEUR::U Computing and Information Technology::UY Computer science},
  annotation = {Accepted: 2019-01-17 23:55},
  file = {C:\Users\caner\Zotero\storage\AVYTX98K\Abelson and Sussman - 1996 - Structure and Interpretation of Computer Programs.pdf}
}

@article{spurJIT,
  title = {{{SPUR}}: A Trace-Based {{JIT}} Compiler for {{CIL}}},
  author = {Bebenita, Michael and Brandner, Florian and Fahndrich, Manuel and Logozzo, Francesco and Schulte, Wolfram and Tillmann, Nikolai and Venter, Herman},
  pages = {18},
  abstract = {Tracing just-in-time compilers (TJITs) determine frequently executed traces (hot paths and loops) in running programs and focus their optimization effort by emitting optimized machine code specialized to these traces. Prior work has established this strategy to be especially beneficial for dynamic languages such as JavaScript, where the TJIT interfaces with the interpreter and produces machine code from the JavaScript trace.},
  langid = {english},
  file = {C:\Users\caner\Zotero\storage\73UNH3Y4\Bebenita et al. - SPUR a trace-based JIT compiler for CIL.pdf}
}

@inproceedings{squeak_smalltalk_vm,
  title = {Back to the Future: The Story of {{Squeak}}, a Practical {{Smalltalk}} Written in Itself},
  shorttitle = {Back to the Future},
  booktitle = {Proceedings of the 12th {{ACM SIGPLAN}} Conference on {{Object-oriented}} Programming, Systems, Languages, and Applications},
  author = {Ingalls, Dan and Kaehler, Ted and Maloney, John and Wallace, Scott and Kay, Alan},
  year = {1997},
  month = oct,
  series = {{{OOPSLA}} '97},
  pages = {318--326},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/263698.263754},
  urldate = {2025-07-03},
  abstract = {Squeak is an open, highly-portable Smalltalk implementation whose virtual machine is written entirely in Smalltalk, making it easy to. debug, analyze, and change. To achieve practical performance, a translator produces an equivalent C program whose performance is comparable to commercial Smalltalks.Other noteworthy aspects of Squeak include: a compact object format that typically requires only a single word of overhead per object; a simple yet efficient incremental garbage collector for 32-bit direct pointers; efficient bulk-mutation of objects; extensions of BitBlt to handle color of any depth and anti-aliased image rotation and scaling; and real-time sound and music synthesis written entirely in Smalltalk.},
  isbn = {978-0-89791-908-1},
  file = {C:\Users\caner\Zotero\storage\EL25R8Z5\Ingalls et al. - 1997 - Back to the future the story of Squeak, a practical Smalltalk written in itself.pdf}
}

@article{stJITdyn:12,
  title = {On the Benefits and Pitfalls of Extending a Statically Typed Language {{JIT}} Compiler for Dynamic Scripting Languages},
  author = {Castanos, Jose and Edelsohn, David and Ishizaki, Kazuaki and Nagpurkar, Priya and Nakatani, Toshio and Ogasawara, Takeshi and Wu, Peng},
  year = {2012},
  month = oct,
  journal = {SIGPLAN Not.},
  volume = {47},
  number = {10},
  pages = {195--212},
  issn = {0362-1340},
  doi = {10.1145/2398857.2384631},
  urldate = {2025-07-16},
  abstract = {Whenever the need to compile a new dynamically typed language arises, an appealing option is to repurpose an existing statically typed language Just-In-Time (JIT) compiler (repurposed JIT compiler). Existing repurposed JIT compilers (RJIT compilers), however, have not yet delivered the hoped-for performance boosts. The performance of JVM languages, for instance, often lags behind standard interpreter implementations. Even more customized solutions that extend the internals of a JIT compiler for the target language compete poorly with those designed specifically for dynamically typed languages. Our own Fiorano JIT compiler is an example of this problem. As a state-of-the-art, RJIT compiler for Python, the Fiorano JIT compiler outperforms two other RJIT compilers (Unladen Swallow and Jython), but still shows a noticeable performance gap compared to PyPy, today's best performing Python JIT compiler. In this paper, we discuss techniques that have proved effective in the Fiorano JIT compiler as well as limitations of our current implementation. More importantly, this work offers the first in-depth look at benefits and limitations of the repurposed JIT compiler approach. We believe the most common pitfall of existing RJIT compilers is not focusing sufficiently on specialization, an abundant optimization opportunity unique to dynamically typed languages. Unfortunately, the lack of specialization cannot be overcome by applying traditional optimizations.},
  file = {C:\Users\caner\Zotero\storage\KB4QLV55\Castanos et al. - 2012 - On the benefits and pitfalls of extending a statically typed language JIT compiler for dynamic scrip.pdf}
}

@article{survey:05,
  title = {A {{Survey}} of {{Adaptive Optimization}} in {{Virtual Machines}}},
  author = {Arnold, M. and Fink, S. J. and Grove, D. and Hind, M. and Sweeney, P. F.},
  year = {2005},
  month = feb,
  journal = {Proceedings of the IEEE},
  volume = {93},
  number = {2},
  pages = {449--466},
  issn = {0018-9219},
  doi = {10.1109/JPROC.2004.840305},
  abstract = {Virtual machines face significant performance challenges beyond those confronted by traditional static optimizers. First, portable program representations and dynamic language features, such as dynamic class loading, force the deferral of most optimizations until runtime, inducing runtime optimization overhead. Second, modular program representations preclude many forms of whole-program interprocedural optimization. Third, virtual machines incur additional costs for runtime services such as security guarantees and automatic memory management. To address these challenges, vendors have invested considerable resources into adaptive optimization systems in production virtual machines. Today, mainstream virtual machine implementations include substantial infrastructure for online monitoring and profiling, runtime compilation, and feedback-directed optimization. As a result, adaptive optimization has begun to mature as a widespread production-level technology. This paper surveys the evolution and current state of adaptive optimization technology in virtual machines.},
  keywords = {Adaptive optimization,adaptive optimization systems,Adaptive systems,automatic memory management,Condition monitoring,Costs,dynamic optimization,feedback directed optimization,feedback-directed optimization (FDO),Memory management,modular program representations,online monitoring,online profiling,optimisation,optimising compilers,optimization,Optimized production technology,production level technology,Production systems,Runtime,runtime compilation,Security,software performance evaluation,static optimizers,Virtual machine monitors,virtual machines,Virtual machining},
  file = {C\:\\Users\\caner\\Zotero\\storage\\DVWJXGSE\\Arnold et al. - 2005 - A Survey of Adaptive Optimization in Virtual Machi.pdf;C\:\\Users\\caner\\Zotero\\storage\\I5PPYRYM\\1386662.html}
}

@inproceedings{tamarin,
  title = {Tracing for Web 3.0: Trace Compilation for the next Generation Web Applications},
  shorttitle = {Tracing for Web 3.0},
  booktitle = {Proceedings of the 2009 {{ACM SIGPLAN}}/{{SIGOPS}} International Conference on {{Virtual}} Execution Environments},
  author = {Chang, Mason and Smith, Edwin and Reitmaier, Rick and Bebenita, Michael and Gal, Andreas and Wimmer, Christian and Eich, Brendan and Franz, Michael},
  year = {2009},
  month = mar,
  series = {{{VEE}} '09},
  pages = {71--80},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1508293.1508304},
  urldate = {2025-07-16},
  abstract = {Today's web applications are pushing the limits of modern web browsers. The emergence of the browser as the platform of choice for rich client-side applications has shifted the use of in-browser JavaScript from small scripting programs to large computationally intensive application logic. For many web applications, JavaScript performance has become one of the bottlenecks preventing the development of even more interactive client side applications. While traditional just-in-time compilation is successful for statically typed virtual machine based languages like Java, compiling JavaScript turns out to be a challenging task. Many JavaScript programs and scripts are short-lived, and users expect a responsive browser during page loading. This leaves little time for compilation of JavaScript to generate machine code.We present a trace-based just-in-time compiler for JavaScript that uses run-time profiling to identify frequently executed code paths, which are compiled to executable machine code. Our approach increases execution performance by up to 116\% by decomposing complex JavaScript instructions into a simple Forth-based representation, and then recording the actually executed code path through this low-level IR. Giving developers more computational horsepower enables a new generation of innovative web applications.},
  isbn = {978-1-60558-375-4},
  file = {C:\Users\caner\Zotero\storage\PLWFJCPU\Chang et al. - 2009 - Tracing for web 3.0 trace compilation for the next generation web applications.pdf}
}

@inproceedings{tdpe:99,
  title = {A {{Semantic Account}} of {{Type-Directed Partial Evaluation}}},
  booktitle = {Proceedings of the {{International Conference PPDP}}'99 on {{Principles}} and {{Practice}} of {{Declarative Programming}}},
  author = {Filinski, Andrzej},
  year = {1999},
  month = sep,
  series = {{{PPDP}} '99},
  pages = {378--395},
  publisher = {Springer-Verlag},
  address = {Berlin, Heidelberg},
  urldate = {2025-07-16},
  isbn = {978-3-540-66540-3}
}

@inproceedings{trace-vs-PE,
  title = {Tracing vs. Partial Evaluation: Comparing Meta-Compilation Approaches for Self-Optimizing Interpreters},
  shorttitle = {Tracing vs. Partial Evaluation},
  booktitle = {Proceedings of the 2015 {{ACM SIGPLAN International Conference}} on {{Object-Oriented Programming}}, {{Systems}}, {{Languages}}, and {{Applications}} - {{OOPSLA}} 2015},
  author = {Marr, Stefan and Ducasse, St{\'e}phane},
  year = {2015},
  pages = {821--839},
  publisher = {ACM Press},
  address = {Pittsburgh, PA, USA},
  doi = {10.1145/2814270.2814275},
  urldate = {2019-03-28},
  isbn = {978-1-4503-3689-5},
  langid = {english},
  file = {C:\Users\caner\Zotero\storage\U6CTI5AY\Marr and Ducasse - 2015 - Tracing vs. partial evaluation comparing meta-com.pdf}
}

@inproceedings{traceMonkey,
  title = {Trace-Based {{Just-in-time Type Specialization}} for {{Dynamic Languages}}},
  booktitle = {Proceedings of the 30th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Gal, Andreas and Eich, Brendan and Shaver, Mike and Anderson, David and Mandelin, David and Haghighat, Mohammad R. and Kaplan, Blake and Hoare, Graydon and Zbarsky, Boris and Orendorff, Jason and Ruderman, Jesse and Smith, Edwin W. and Reitmaier, Rick and Bebenita, Michael and Chang, Mason and Franz, Michael},
  year = {2009},
  series = {{{PLDI}} '09},
  pages = {465--478},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/1542476.1542528},
  urldate = {2019-11-28},
  abstract = {Dynamic languages such as JavaScript are more difficult to compile than statically typed ones. Since no concrete type information is available, traditional compilers need to emit generic code that can handle all possible type combinations at runtime. We present an alternative compilation technique for dynamically-typed languages that identifies frequently executed loop traces at run-time and then generates machine code on the fly that is specialized for the actual dynamic types occurring on each path through the loop. Our method provides cheap inter-procedural type specialization, and an elegant and efficient way of incrementally compiling lazily discovered alternative paths through nested loops. We have implemented a dynamic compiler for JavaScript based on our technique and we have measured speedups of 10x and more for certain benchmark programs.},
  isbn = {978-1-60558-392-1},
  keywords = {dynamically typed languages,trace-based compilation},
  file = {C:\Users\caner\Zotero\storage\FWYDACR5\Gal et al. - 2009 - Trace-based Just-in-time Type Specialization for D.pdf}
}

@inproceedings{truffle-graal,
  title = {One {{VM}} to Rule Them All},
  booktitle = {Proceedings of the 2013 {{ACM}} International Symposium on {{New}} Ideas, New Paradigms, and Reflections on Programming \& Software - {{Onward}}! '13},
  author = {W{\"u}rthinger, Thomas and Wimmer, Christian and W{\"o}{\ss}, Andreas and Stadler, Lukas and Duboscq, Gilles and Humer, Christian and Richards, Gregor and Simon, Doug and Wolczko, Mario},
  year = {2013},
  pages = {187--204},
  publisher = {ACM Press},
  address = {Indianapolis, Indiana, USA},
  doi = {10.1145/2509578.2509581},
  urldate = {2019-11-25},
  abstract = {Building high-performance virtual machines is a complex and expensive undertaking; many popular languages still have low-performance implementations. We describe a new approach to virtual machine (VM) construction that amortizes much of the effort in initial construction by allowing new languages to be implemented with modest additional effort. The approach relies on abstract syntax tree (AST) interpretation where a node can rewrite itself to a more specialized or more general node, together with an optimizing compiler that exploits the structure of the interpreter. The compiler uses speculative assumptions and deoptimization in order to produce efficient machine code. Our initial experience suggests that high performance is attainable while preserving a modular and layered architecture, and that new highperformance language implementations can be obtained by writing little more than a stylized interpreter.},
  isbn = {978-1-4503-2472-4},
  langid = {english},
  file = {C:\Users\caner\Zotero\storage\7JJNARJ3\Würthinger et al. - 2013 - One VM to rule them all.pdf}
}

@article{typeSpecial:2009,
  title = {Trace-Based Just-in-Time Type Specialization for Dynamic Languages},
  author = {Gal, Andreas and Eich, Brendan and Shaver, Mike and Anderson, David and Mandelin, David and Haghighat, Mohammad R. and Kaplan, Blake and Hoare, Graydon and Zbarsky, Boris and Orendorff, Jason and Ruderman, Jesse and Smith, Edwin W. and Reitmaier, Rick and Bebenita, Michael and Chang, Mason and Franz, Michael},
  year = {2009},
  month = jun,
  journal = {SIGPLAN Not.},
  volume = {44},
  number = {6},
  pages = {465--478},
  issn = {0362-1340},
  doi = {10.1145/1543135.1542528},
  urldate = {2025-06-23},
  abstract = {Dynamic languages such as JavaScript are more difficult to compile than statically typed ones. Since no concrete type information is available, traditional compilers need to emit generic code that can handle all possible type combinations at runtime. We present an alternative compilation technique for dynamically-typed languages that identifies frequently executed loop traces at run-time and then generates machine code on the fly that is specialized for the actual dynamic types occurring on each path through the loop. Our method provides cheap inter-procedural type specialization, and an elegant and efficient way of incrementally compiling lazily discovered alternative paths through nested loops. We have implemented a dynamic compiler for JavaScript based on our technique and we have measured speedups of 10x and more for certain benchmark programs.},
  file = {C:\Users\caner\Zotero\storage\T59L5B5Z\Gal et al. - 2009 - Trace-based just-in-time type specialization for dynamic languages.pdf}
}

@article{vandercammenEssenceMetaTracingJIT,
  title = {The {{Essence}} of {{Meta-Tracing JIT Compilers}}},
  author = {Vandercammen, Maarten},
  langid = {english},
  file = {C:\Users\caner\Zotero\storage\9Q858YLX\Vandercammen - The Essence of Meta-Tracing JIT Compilers.pdf}
}

@article{whatever:19,
  title = {Compiling with {{Continuations}}, or {{Without}}? {{Whatever}}.},
  shorttitle = {Compiling with {{Continuations}}, or {{Without}}?},
  author = {Cong, Youyou and Osvald, Leo and Essertel, Gr{\'e}gory M. and Rompf, Tiark},
  year = {2019},
  month = jul,
  journal = {Proc. ACM Program. Lang.},
  volume = {3},
  number = {ICFP},
  pages = {79:1--79:28},
  issn = {2475-1421},
  doi = {10.1145/3341643},
  urldate = {2019-11-29},
  abstract = {What makes a good compiler IR? In the context of functional languages, there has been an extensive debate on the advantages and disadvantages of continuation-passing-style (CPS). The consensus seems to be that some form of explicit continuations is necessary to model jumps in a functional style, but that they should have a 2nd-class status, separate from regular functions, to ensure efficient code generation. Building on this observation, a recent study from PLDI 2017 proposed a direct-style IR with explicit join points, which essentially represent local continuations, i.e., functions that do not return or escape. While this IR can work well in practice, as evidenced by the implementation of join points in the Glasgow Haskell Compiler (GHC), there still seems to be room for improvement, especially with regard to the way continuations are handled in the course of optimization.  In this paper, we contribute to the CPS debate by developing a novel IR with the following features. First, we integrate a control operator that resembles Felleisen's C, eliminating certain redundant rewrites observed in the previous study. Second, we treat the non-returning and non-escaping aspects of continuations separately, allowing efficient compilation of well-behaved functions defined by the user. Third, we define a selective CPS translation of our IR, which erases control operators while preserving the meaning and typing of programs. These features enable optimizations in both direct style and full CPS, as well as in any intermediate style with selectively exposed continuations. Thus, we change the spectrum of available options from ``CPS yes or no'' to ``as much or as little CPS as you want, when you want it''.},
  keywords = {compiler intermediate languages,control operators,CPS translation},
  file = {C:\Users\caner\Zotero\storage\67PLU4YQ\Cong et al. - 2019 - Compiling with Continuations, or Without Whatever.pdf}
}
