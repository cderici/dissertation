@book{appelCompilingContinuations2007,
  title = {Compiling with {{Continuations}}},
  author = {Appel, Andrew W.},
  year = {2007},
  month = jan,
  publisher = {Cambridge University Press},
  address = {USA},
  abstract = {This book shows how continuation-passing style is used as an intermediate representation to perform optimizations and program transformations. Continuations can be used to compile most programming languages. The method is illustrated in a compiler for the programming language Standard ML. Prior knowledge of ML, however, is not necessary, as the author carefully explains each concept as it arises. This is the first book to show how concepts from the theory of programming languages can be applied to the production of practical optimizing compilers for modern languages like ML. All the details of compiling are covered, including the interface to a runtime system and garbage collector.},
  isbn = {978-0-521-03311-4}
}

@article{bolzMetatracingMakesFast2014,
  title = {Meta-Tracing Makes a Fast {{Racket}}},
  author = {Bolz, Carl Friedrich and Pape, Tobias and Siek, Jeremy and {Tobin-Hochstadt}, Sam},
  year = {2014},
  pages = {7},
  abstract = {Tracing just-in-time (JIT) compilers record and optimize the instruction sequences they observe at runtime. With some modifications, a tracing JIT can perform well even when the executed program is itself an interpreter, an approach called meta-tracing. The advantage of meta-tracing is that it separates the concern of JIT compilation from language implementation, enabling the same JIT compiler to be used with many different languages. The RPython meta-tracing JIT compiler has enabled the efficient interpretation of several dynamic languages including Python (PyPy), Prolog, and Smalltalk. In this paper we present initial findings in applying the RPython JIT to Racket. Racket comes from the Scheme family of programming languages for which there are mature static optimizing compilers. We present the result of spending just a couple person-months implementing and tuning an implementation of Racket written in RPython. The results are promising, with a geometric mean equal to Racket's performance and within a factor of 2 slower than Gambit and Larceny on a collection of standard Scheme benchmarks. The results on individual benchmarks vary widely. On the positive side, our interpreter is sometimes up to two to six times faster than Gambit, an order of magnitude faster than Larceny, and two orders of magnitude faster than the Racket JIT compiler when making heavy use of continuations. On the negative side, our interpreter is sometimes three times slower than Racket, nine times slower than Gambit, and five times slower than Larceny.},
  langid = {english},
  file = {C:\Users\caner\Zotero\storage\2RGFN3VC\Bolz et al. - Meta-tracing makes a fast Racket.pdf}
}

@article{bolzPhDThesis,
  title = {{Meta-Tracing Just-in-Time Compilation for RPython}},
  author = {Bolz, Carl Friedrich},
  langid = {ngerman},
  file = {C:\Users\caner\Zotero\storage\57J8CJLC\Bolz - Meta-Tracing Just-in-Time Compilation for RPython.pdf}
}

@inproceedings{clasp_llvm,
  title = {Clasp {{Common Lisp Implementation}} and {{Optimization}}},
  booktitle = {Proceedings of the 11th {{European Lisp Symposium}} on {{European Lisp Symposium}}},
  author = {Schafmeister, Christian A. and Wood, Alex},
  year = {2018},
  month = apr,
  series = {{{ELS2018}}},
  pages = {59--64},
  publisher = {European Lisp Scientific Activities Association},
  address = {Marbella, Spain},
  urldate = {2025-06-26},
  abstract = {We describe implementation strategies and updates made in the last two years to clasp,[1, 2] a new Common Lisp implementation that interoperates with C++, uses the cleavir compiler, and uses the LLVM backend[4]. Improvements in clasp have been made in many areas. The most important changes are: (1) Tagged pointers and immediate values have been incorporated. (2) A fast generic function dispatch approach has been implemented that allows clasp to carry out generic function dispatch as fast as SBCL, a highly optimized free implementation of Common Lisp. The generic function dispatch memoization approach was developed by Robert Strandh[8] and demonstrates a 20x improvement in performance of generic function dispatch. (3) The new LLVM feature ``Thin Link Time Optimization'' has been added, which speeds up generated code by removing call overhead throughout the system. (4) Type inference has been added to the cleavir compiler, which is part of clasp. Type inference removes redundant run-time type checks and dead code paths. Type inference currently provides about a 30\% speedup in microbenchmarks.[9] (5) Constants and literals have been moved close to the code and ``instruction pointer addressing'' has been incorporated to speed access to literals and constants. (6) Pre-emptive multithreading has been added to clasp, based on pthreads, supporting the Bordeaux threads library. (7) The overall build time for clasp has been reduced from five to eight hours over two years ago to approximately one hour at present.},
  isbn = {978-2-9557474-2-1}
}

@inproceedings{clos_overview_mop,
  title = {The {{Common Lisp Object System}}: {{An Overview}}},
  shorttitle = {The {{Common Lisp Object System}}},
  booktitle = {Proceedings of the {{European Conference}} on {{Object-Oriented Programming}}},
  author = {DeMichiel, Linda G. and Gabriel, Richard P.},
  year = {1987},
  month = jun,
  series = {{{ECOOP}} '87},
  pages = {151--170},
  publisher = {Springer-Verlag},
  address = {Berlin, Heidelberg},
  urldate = {2025-07-03},
  abstract = {The Common Lisp Object System is an object-oriented system that is based on the concepts of generic functions, multiple inheritance, and method combination. All objects in the Object System are instances of classes that form an extension to the Common Lisp type system. The Common Lisp Object System is based on a meta-object protocol that renders it possible to alter the fundamental structure of the Object System itself. The Common Lisp Object System has been proposed as a standard for ANSI Common Lisp and has been tentatively endorsed by X3J13.},
  isbn = {978-3-540-18353-2}
}

@inproceedings{danvy:93,
  title = {Separating Stages in the Continuation-Passing Style Transformation},
  booktitle = {Proceedings of the 20th {{ACM SIGPLAN-SIGACT}} Symposium on {{Principles}} of Programming Languages},
  author = {Lawall, Julia L. and Danvy, Olivier},
  year = {1993},
  month = mar,
  series = {{{POPL}} '93},
  pages = {124--136},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/158511.158613},
  urldate = {2025-06-23},
  abstract = {The continuation-passing style (CPS) transformation is powerful but complex. Our thesis is that this transformation is in fact compound, and we set out to stage it. We factor the CPS transformation into several steps, separating aspects in each step: (1) Intermediate values are named; (2) Continuations are introduced; (3) Sequencing order is decided and administrative reductions are performed.Step 1 determines the evaluation order (e.g., call-by-name or call-by-value). Step 2 isolates the introduction of continuations and is expressed with local, structure-preserving rewrite rules --- a novel aspect standing in sharp contrast with the usual CPS transformations. Step 3 determines the ordering of continuations (e.g., left-to-right or right-to-left evaluation) and leads to the familiar-looking continuation-passing terms.Step 2 is completely reversible and Steps 1 and 3 form Galois connections. Together they lead to the direct style (DS) transformation of our earlier work (including first-class continuations): (1) Intermediate continuations are named and sequencing order is abstracted; (2) Second-class continuations are eliminated; (3) Administrative reductions are performed.A subset of these transformations can leverage program manipulation systems: CPS-based compilers can modify sequencing to improve e.g., register allocation; static program analyzers can yield more precise results; and overspecified CPS programs can be rescheduled. Separating aspects of the CPS transformation also enables a new programming style, with applications to nondeterministic programming. As a byproduct, our work also suggests a new continuation semantics for unspecified sequencing orders in programming languages (e.g., Scheme).},
  isbn = {978-0-89791-560-1},
  file = {C:\Users\caner\Zotero\storage\5LRL7IK2\Lawall and Danvy - 1993 - Separating stages in the continuation-passing style transformation.pdf}
}

@article{dynamo,
  title = {Dynamo: {{A Transparent Dynamic Optimization System}}},
  author = {Bala, Vasanth and Duesterwald, Evelyn and Banerjia, Sanjeev},
  pages = {12},
  abstract = {We describe the design and implementation of Dynamo, a software dynamic optimization system that is capable of transparently improving the performance of a native instruction stream as it executes on the processor. The input native instruction stream to Dynamo can be dynamically generated (by a JIT for example), or it can come from the execution of a statically compiled native binary. This paper evaluates the Dynamo system in the latter, more challenging situation, in order to emphasize the limits, rather than the potential, of the system. Our experiments demonstrate that even statically optimized native binaries can be accelerated Dynamo, and often by a significant degree. For example, the average performance of --O optimized SpecInt95 benchmark binaries created by the HP product C compiler is improved to a level comparable to their --O4 optimized version running without Dynamo. Dynamo achieves this by focusing its efforts on optimization opportunities that tend to manifest only at runtime, and hence opportunities that might be difficult for a static compiler to exploit. Dynamo's operation is transparent in the sense that it does not depend on any user annotations or binary instrumentation, and does not require multiple runs, or any special compiler, operating system or hardware support. The Dynamo prototype presented here is a realistic implementation running on an HP PA-8000 workstation under the HPUX 10.20 operating system.},
  langid = {english}
}

@inproceedings{enginesOriginal,
  title = {Engines Build Process Abstractions},
  booktitle = {Proceedings of the 1984 {{ACM Symposium}} on {{LISP}} and Functional Programming},
  author = {Haynes, Christopher T. and Friedman, Daniel P.},
  year = {1984},
  month = aug,
  series = {{{LFP}} '84},
  pages = {18--24},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/800055.802018},
  urldate = {2025-07-04},
  abstract = {Engines are a new programming language abstraction for timed preemption. In conjunction with first class continuations, engines allow the language to be extended with a time-sharing implementation of process abstraction facilities. To illustrate engine programming techniques, we implement a round-robin process scheduler. The importance of simple but powerful primitives such as engines is discussed.},
  isbn = {978-0-89791-142-9},
  file = {C:\Users\caner\Zotero\storage\WCJ4LMLR\Haynes and Friedman - 1984 - Engines build process abstractions.pdf}
}

@inproceedings{erlang_otp_hpc,
  title = {High-Performance Technical Computing with Erlang},
  booktitle = {Proceedings of the 7th {{ACM SIGPLAN}} Workshop on {{ERLANG}}},
  author = {Scalas, Alceste and Casu, Giovanni and Pili, Piero},
  year = {2008},
  month = sep,
  series = {{{ERLANG}} '08},
  pages = {49--60},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1411273.1411281},
  urldate = {2025-07-03},
  abstract = {High-performance Technical Computing (HPTC) is a branch of HPC (High-performance Computing) that deals with scientific applications, such as physics simulations. Due to its numerical nature, it has been traditionally based on low-level or mathematically-oriented languages (C, C++, Fortran), extended with libraries that implement remote execution and inter-process communication (like MPI and PVM).But those libraries just provide what Erlang does out-of-the-box: networking, process distribution, concurrency, interprocess communication and fault tolerance. So, is it possible to use Erlang as a foundation for developing HPTC applications?This paper shows our experiences in using Erlang for distributed number-crunching systems. We introduce two extensions: a simple and efficient foreign function interface (FFI), and an Erlang binding for numerical libraries. We use them as a basis for developing a simple mathematically-oriented programming language (in the style of Matlab™) compiled into Core Erlang. These tools are later used for creating a HPTC framework (based on message-passing) and an IDE for distributed applications.The results of this research and development show that Erlang/OTP can be used as a platform for developing large and scalable numerical applications.},
  isbn = {978-1-60558-065-4},
  file = {C:\Users\caner\Zotero\storage\PWQBJDAV\Scalas et al. - 2008 - High-performance technical computing with erlang.pdf}
}

@inproceedings{felleisen87,
  title = {Control Operators, the {{SECD-machine}}, and the {$\lambda$}-Calculus},
  booktitle = {Formal {{Description}} of {{Programming Concepts}}},
  author = {Felleisen, Matthias and Friedman, Daniel P.},
  year = {1987},
  keywords = {Lambda calculus,SECD machine}
}

@misc{findler-felleisen:2002,
  title = {Contracts for Higher-Order Functions {\textbar} {{Proceedings}} of the Seventh {{ACM SIGPLAN}} International Conference on {{Functional}} Programming},
  urldate = {2025-06-23},
  howpublished = {https://dl-acm-org.proxyiub.uits.iu.edu/doi/10.1145/581478.581484},
  file = {C:\Users\caner\Zotero\storage\DJUW4VN7\581478.html}
}

@article{flanagan:93,
  title = {The Essence of Compiling with Continuations},
  author = {Flanagan, Cormac and Sabry, Amr and Duba, Bruce F. and Felleisen, Matthias},
  year = {1993},
  month = jun,
  journal = {SIGPLAN Not.},
  volume = {28},
  number = {6},
  pages = {237--247},
  issn = {0362-1340},
  doi = {10.1145/173262.155113},
  urldate = {2025-06-23},
  abstract = {In order to simplify the compilation process, many compilers for higher-order languages use the continuation-passing style (CPS) transformation in a first phase to generate an intermediate representation of the source program. The salient aspect of this intermediate form is that all procedures take an argument that represents the rest of the computation (the ``continuation''). Since the nai{\textasciidieresis}ve CPS transformation considerably increases the size of programs, CPS compilers perform reductions to produce a more compact intermediate representation. Although often implemented as a part of the CPS transformation, this step is conceptually a second phase. Finally, code generators for typical CPS compilers treat continuations specially in order to optimize the interpretation of continuation parameters.A thorough analysis of the abstract machine for CPS terms show that the actions of the code generator invert the nai{\textasciidieresis}ve CPS translation step. Put differently, the combined effect of the three phases is equivalent to a source-to-source transformation that simulates the compaction phase. Thus, fully developed CPS compilers do not need to employ the CPS transformation but can achieve the same results with a simple source-level transformation.},
  file = {C:\Users\caner\Zotero\storage\VWHIVX6I\Flanagan et al. - 1993 - The essence of compiling with continuations.pdf}
}

@article{flatt:2002,
  title = {Composable and Compilable Macros: You Want It When?},
  shorttitle = {Composable and Compilable Macros},
  author = {Flatt, Matthew},
  year = {2002},
  month = sep,
  journal = {SIGPLAN Not.},
  volume = {37},
  number = {9},
  pages = {72--83},
  issn = {0362-1340},
  doi = {10.1145/583852.581486},
  urldate = {2025-06-23},
  abstract = {Many macro systems, especially for Lisp and Scheme, allow macro transformers to perform general computation. Moreover, the language for implementing compile-time macro transformers is usually the same as the language for implementing run-time functions. As a side effect of this sharing, implementations tend to allow the mingling of compile-time values and run-time values, as well as values from separate compilations. Such mingling breaks programming tools that must parse code without executing it. Macro implementors avoid harmful mingling by obeying certain macro-definition protocols and by inserting phase-distinguishing annotations into the code. However, the annotations are fragile, the protocols are not enforced, and programmers can only reason about the result in terms of the compiler's implementation. MzScheme---the language of the PLT Scheme tool suite---addresses the problem through a macro system that separates compilation without sacrificing the expressiveness of macros.},
  file = {C:\Users\caner\Zotero\storage\55CGSQJ9\Flatt - 2002 - Composable and compilable macros you want it when.pdf}
}

@article{flattMacrosThatWork2012,
  title = {Macros That {{Work Together}}: {{Compile-time}} Bindings, Partial Expansion, and Definition Contexts},
  shorttitle = {Macros That {{Work Together}}},
  author = {Flatt, Matthew and Culpepper, Ryan and Darais, David and Findler, Robert Bruce},
  year = {2012},
  month = mar,
  journal = {Journal of Functional Programming},
  volume = {22},
  number = {2},
  pages = {181--216},
  issn = {1469-7653, 0956-7968},
  doi = {10.1017/S0956796812000093},
  urldate = {2020-01-30},
  abstract = {Racket is a large language that is built mostly within itself. Unlike the usual approach taken by non-Lisp languages, the self-hosting of Racket is not a matter of bootstrapping one implementation through a previous implementation, but instead a matter of building a tower of languages and libraries via macros. The upper layers of the tower include a class system, a component system, pedagogic variants of Scheme, a statically typed dialect of Scheme, and more. The demands of this language-construction effort require a macro system that is substantially more expressive than previous macro systems. In particular, while conventional Scheme macro systems handle stand-alone syntactic forms adequately, they provide weak support for macros that share information or macros that use existing syntactic forms in new contexts. This paper describes and models features of the Racket macro system, including support for general compile-time bindings, sub-form expansion and analysis, and environment management. The presentation assumes a basic familiarity with Lisp-style macros, and it takes for granted the need for macros that respect lexical scope. The model, however, strips away the pattern and template system that is normally associated with Scheme macros, isolating a core that is simpler, can support pattern and template forms themselves as macros, and generalizes naturally to Racket's other extensions.},
  langid = {english},
  file = {C\:\\Users\\caner\\Zotero\\storage\\KJ3MZ6SZ\\Flatt et al. - 2012 - Macros that Work Together Compile-time bindings, .pdf;C\:\\Users\\caner\\Zotero\\storage\\9IWC2NZF\\375043C6746405B22014D235FA4C90C3.html}
}

@misc{gal:2006,
  title = {Incremental {{Dynamic Code Generation}} with {{Trace Trees}} {\textbar} {{Request PDF}}},
  journal = {ResearchGate},
  urldate = {2025-06-24},
  abstract = {Request PDF {\textbar} Incremental Dynamic Code Generation with Trace Trees {\textbar} The unit of compilation for traditional just-in-time compilers is the method. We have explored trace-based compilation, in which the unit of... {\textbar} Find, read and cite all the research you need on ResearchGate},
  howpublished = {https://www.researchgate.net/publication/213877755\_Incremental\_Dynamic\_Code\_Generation\_with\_Trace\_Trees},
  langid = {english},
  file = {C:\Users\caner\Zotero\storage\I8RVLKRY\213877755_Incremental_Dynamic_Code_Generation_with_Trace_Trees.html}
}

@article{ghc_llvm_backend,
  title = {An {{llVM}} Backend for {{GHC}}},
  author = {Terei, David A. and Chakravarty, Manuel M.T.},
  year = {2010},
  month = sep,
  journal = {SIGPLAN Not.},
  volume = {45},
  number = {11},
  pages = {109--120},
  issn = {0362-1340},
  doi = {10.1145/2088456.1863538},
  urldate = {2025-06-26},
  abstract = {In the presence of ever-changing computer architectures, high-quality optimising compiler backends are moving targets that require specialist knowledge and sophisticated algorithms. In this paper, we explore a new backend for the Glasgow Haskell Compiler (GHC) that leverages the Low Level Virtual Machine (LLVM), a new breed of compiler written explicitly for use by other compiler writers, not high-level programmers, that promises to enable outsourcing of low-level and architecture-dependent aspects of code generation. We discuss the conceptual challenges and our backend design. We also provide an extensive quantitative evaluation of the performance of the backend and of the code it produces.},
  file = {C:\Users\caner\Zotero\storage\VRKITGY6\Terei and Chakravarty - 2010 - An llVM backend for GHC.pdf}
}

@article{hayashizakiImprovingPerformanceTracebased2011,
  title = {Improving the Performance of Trace-Based Systems by False Loop Filtering},
  author = {Hayashizaki, Hiroshige and Wu, Peng and Inoue, Hiroshi and Serrano, Mauricio J. and Nakatani, Toshio},
  year = {2011},
  month = mar,
  journal = {ACM SIGPLAN Notices},
  volume = {46},
  number = {3},
  pages = {405--418},
  issn = {0362-1340, 1558-1160},
  doi = {10.1145/1961296.1950412},
  urldate = {2025-03-27},
  abstract = {Trace-based compilation is a promising technique for language compilers and binary translators. It offers the potential to expand the compilation scopes that have traditionally been limited by method boundaries.             Detecting repeating cyclic execution paths and capturing the detected repetitions into traces is a key requirement for trace selection algorithms to achieve good optimization and performance with small amounts of code. One important class of repetition detection is cyclic-path-based repetition detection, where a cyclic execution path (a path that starts and ends at the same instruction address) is detected as a repeating cyclic execution path.             However, we found many cyclic paths that are not repeating cyclic execution paths, which we call false loops. A common class of false loops occurs when a method is invoked from multiple call-sites. A cycle is formed between two invocations of the method from different call-sites, but which does not represent loops or recursion. False loops can result in shorter traces and smaller compilation scopes, and degrade the performance.             We propose false loop filtering, an approach to reject false loops in the repetition detection step of trace selection, and a technique called false loop filtering by call-stack-comparison, which rejects a cyclic path as a false loop if the call stacks at the beginning and the end of the cycle are different.             We applied false loop filtering to our trace-based Java™ JIT compiler that is based on IBM's J9 JVM. We found that false loop filtering achieved an average improvement of 16\% and 10\% for the DaCapo benchmark when applied to two baseline trace selection algorithms, respectively, with up to 37\% improvement for individual benchmarks. In the end, with false loop filtering, our trace-based JIT achieves a performance comparable to that of the method-based J9 JVM/JIT using the corresponding optimization level.},
  langid = {english},
  file = {C:\Users\caner\Zotero\storage\QQGFA4RB\Hayashizaki et al. - 2011 - Improving the performance of trace-based systems by false loop filtering.pdf}
}

@inproceedings{hotpath:06,
  title = {{{HotpathVM}}: {{An Effective JIT Compiler}} for {{Resource-constrained Devices}}},
  shorttitle = {{{HotpathVM}}},
  booktitle = {Proceedings of the {{2Nd International Conference}} on {{Virtual Execution Environments}}},
  author = {Gal, Andreas and Probst, Christian W. and Franz, Michael},
  year = {2006},
  series = {{{VEE}} '06},
  pages = {144--153},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/1134760.1134780},
  urldate = {2019-11-16},
  abstract = {We present a just-in-time compiler for a Java VM that is small enough to fit on resource-constrained devices, yet is surprisingly effective. Our system dynamically identifies traces of frequently executed bytecode instructions (which may span several basic blocks across several methods) and compiles them via Static Single Assignment (SSA) construction. Our novel use of SSA form in this context allows to hoist instructions across trace side-exits without necessitating expensive compensation code in off-trace paths. The overall memory consumption (code and data) of our system is only 150 kBytes, yet benchmarks show a speedup that in some cases rivals heavy-weight just-in-time compilers.},
  isbn = {978-1-59593-332-4},
  keywords = {dynamic compilation,embedded and resource-constrained systems,mixed-mode interpretive compiled systems,software trace scheduling,static single assignment form,virtual machines},
  file = {C:\Users\caner\Zotero\storage\MUT26PBM\Gal et al. - 2006 - HotpathVM An Effective JIT Compiler for Resource-.pdf}
}

@article{icfp2019,
  title = {Rebuilding {{Racket}} on {{Chez Scheme}} ({{Experience Report}})},
  author = {Flatt, Matthew and Derici, Caner and Dybvig, R. Kent and Keep, Andrew W. and Massaccesi, Gustavo E. and Spall, Sarah and {Tobin-Hochstadt}, Sam and Zeppieri, Jon},
  year = {2019},
  month = jul,
  journal = {Proc. ACM Program. Lang.},
  volume = {3},
  number = {ICFP},
  pages = {78:1--78:15},
  issn = {2475-1421},
  doi = {10.1145/3341642},
  urldate = {2019-11-12},
  abstract = {We rebuilt Racket on Chez Scheme, and it works well\&mdash;as long as we're allowed a few patches to Chez Scheme. DrRacket runs, the Racket distribution can build itself, and nearly all of the core Racket test suite passes. Maintainability and performance of the resulting implementation are good, although some work remains to improve end-to-end performance. The least predictable part of our effort was how big the differences between Racket and Chez Scheme would turn out to be and how we would manage those differences. We expect Racket on Chez Scheme to become the main Racket implementation, and we encourage other language implementers to consider Chez Scheme as a target virtual machine.},
  keywords = {Racket,Scheme},
  file = {C:\Users\caner\Zotero\storage\76JHJJYP\Flatt et al. - 2019 - Rebuilding Racket on Chez Scheme (Experience Repor.pdf}
}

@article{ir_cacm,
  title = {Intermediate Representation},
  author = {Chow, Fred},
  year = {2013},
  month = dec,
  journal = {Commun. ACM},
  volume = {56},
  number = {12},
  pages = {57--62},
  issn = {0001-0782},
  doi = {10.1145/2534706.2534720},
  urldate = {2025-06-26},
  abstract = {The increasing significance of intermediate representations in compilers.},
  file = {C:\Users\caner\Zotero\storage\FMNRNXNJ\Chow - 2013 - Intermediate representation.pdf}
}

@inproceedings{izawaAmalgamatingDifferentJIT2020b,
  title = {Amalgamating Different {{JIT}} Compilations in a Meta-Tracing {{JIT}} Compiler Framework},
  booktitle = {Proceedings of the 16th {{ACM SIGPLAN International Symposium}} on {{Dynamic Languages}}},
  author = {Izawa, Yusuke and Masuhara, Hidehiko},
  year = {2020},
  month = nov,
  pages = {1--15},
  publisher = {ACM},
  address = {Virtual USA},
  doi = {10.1145/3426422.3426977},
  urldate = {2025-06-13},
  isbn = {978-1-4503-8175-8},
  langid = {english},
  file = {C:\Users\caner\Zotero\storage\WAKJ6FNQ\Izawa and Masuhara - 2020 - Amalgamating different JIT compilations in a meta-tracing JIT compiler framework.pdf}
}

@misc{izawaLightweightMethodGenerating2025,
  title = {A {{Lightweight Method}} for {{Generating Multi-Tier JIT Compilation Virtual Machine}} in a {{Meta-Tracing Compiler Framework}}},
  author = {Izawa, Yusuke and Masuhara, Hidehiko and {Bolz-Tereick}, Carl Friedrich},
  year = {2025},
  month = apr,
  number = {arXiv:2504.17460},
  eprint = {2504.17460},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.17460},
  urldate = {2025-05-28},
  abstract = {Meta-compiler frameworks, such as RPython and Graal/Truffle, generate high-performance virtual machines (VMs) from interpreter definitions. Although they generate VMs with high-quality just-in-time (JIT) compilers, they still lack an important feature that dedicated VMs (i.e., VMs that are developed for specific languages) have, namely {\textbackslash}emph\{multi-tier compilation\}. Multi-tier compilation uses light-weight compilers at early stages and highly-optimizing compilers at later stages in order to balance between compilation overheads and code quality. We propose a novel approach to enabling multi-tier compilation in the VMs generated by a meta-compiler framework. Instead of extending the JIT compiler backend of the framework, our approach drives an existing (heavyweight) compiler backend in the framework to quickly generate unoptimized native code by merely embedding directives and compile-time operations into interpreter definitions. As a validation of the approach, we developed 2SOM, a Simple Object Machine with a two-tier JIT compiler based on RPython. 2SOM first applies the tier-1 threaded code generator that is generated by our proposed technique, then, to the loops that exceed a threshold, applies the tier-2 tracing JIT compiler that is generated by the original RPython framework. Our performance evaluation that runs a program with a realistic workload showed that 2SOM improved, when compared against an RPython-based VM, warm-up performance by 15{\textbackslash}\%, with merely a 5{\textbackslash}\% reduction in peak performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Programming Languages},
  file = {C\:\\Users\\caner\\Zotero\\storage\\SX29ZQUE\\Izawa et al. - 2025 - A Lightweight Method for Generating Multi-Tier JIT Compilation Virtual Machine in a Meta-Tracing Com.pdf;C\:\\Users\\caner\\Zotero\\storage\\7WFHXFWI\\2504.html}
}

@article{jit-history:03,
  title = {A {{Brief History}} of {{Just-in-time}}},
  author = {Aycock, John},
  year = {2003},
  month = jun,
  journal = {ACM Comput. Surv.},
  volume = {35},
  number = {2},
  pages = {97--113},
  issn = {0360-0300},
  doi = {10.1145/857076.857077},
  urldate = {2019-12-01},
  abstract = {Software systems have been using "just-in-time" compilation (JIT) techniques since the 1960s. Broadly, JIT compilation includes any translation performed dynamically, after a program has started execution. We examine the motivation behind JIT compilation and constraints imposed on JIT compilation systems, and present a classification scheme for such systems. This classification emerges as we survey forty years of JIT work, from 1960--2000.},
  keywords = {dynamic compilation,Just-in-time compilation},
  file = {C:\Users\caner\Zotero\storage\48RY7S5V\Aycock - 2003 - A Brief History of Just-in-time.pdf}
}

@inproceedings{llvm_verification,
  title = {Leveraging {{Compiler Intermediate Representation}} for {{Multi-}} and {{Cross-Language Verification}}},
  booktitle = {Verification, {{Model Checking}}, and {{Abstract Interpretation}}: 21st {{International Conference}}, {{VMCAI}} 2020, {{New Orleans}}, {{LA}}, {{USA}}, {{January}} 16--21, 2020, {{Proceedings}}},
  author = {Garzella, Jack J. and Baranowski, Marek and He, Shaobo and Rakamari{\'c}, Zvonimir},
  year = {2020},
  month = jan,
  pages = {90--111},
  publisher = {Springer-Verlag},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-030-39322-9_5},
  urldate = {2025-06-26},
  abstract = {Developers nowadays regularly use numerous programming languages with different characteristics and trade-offs. Unfortunately, implementing a software verifier for a new language from scratch is a large and tedious undertaking, requiring expert knowledge in multiple domains, such as compilers, verification, and constraint solving. Hence, only a tiny fraction of the used languages has readily available software verifiers to aid in the development of correct programs. In the past decade, there has been a trend of leveraging popular compiler intermediate representations (IRs), such as LLVM IR, when implementing software verifiers. Processing IR promises out-of-the-box multi- and cross-language verification since, at least in theory, a verifier ought to be able to handle a program in any programming language (and their combination) that can be compiled into the IR. In practice though, to the best of our knowledge, nobody has explored the feasibility and ease of such integration of new languages. In this paper, we provide a procedure for adding support for a new language into an IR-based verification toolflow. Using our procedure, we extend the SMACK verifier with prototypical support for 6 additional languages. We assess the quality of our extensions through several case studies, and we describe our experience in detail to guide future efforts in this area.},
  isbn = {978-3-030-39321-2}
}

@inproceedings{loop-aware:12,
  title = {Loop-Aware {{Optimizations}} in {{PyPy}}'s {{Tracing JIT}}},
  booktitle = {Proceedings of the 8th {{Symposium}} on {{Dynamic Languages}}},
  author = {Ard{\"o}, H{\aa}kan and Bolz, Carl Friedrich and Fija{\l}kowski, Maciej},
  year = {2012},
  series = {{{DLS}} '12},
  pages = {63--72},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/2384577.2384586},
  urldate = {2019-03-22},
  abstract = {One of the nice properties of a tracing just-in-time compiler (JIT) is that many of its optimizations are simple, requiring one forward pass only. This is not true for loop-invariant code motion which is a very important optimization for code with tight kernels. Especially for dynamic languages that typically perform quite a lot of loop invariant type checking, boxed value unwrapping and virtual method lookups. In this paper we explain a scheme pioneered within the context of the LuaJIT project for making basic optimizations loop-aware by using a simple pre-processing step on the trace without changing the optimizations themselves. We have implemented the scheme in RPython's tracing JIT compiler. PyPy's Python JIT executing simple numerical kernels can become up to two times faster, bringing the performance into the ballpark of static language compilers.},
  isbn = {978-1-4503-1564-7},
  keywords = {loop-invariant code motion,optimization,tracing JIT},
  file = {C:\Users\caner\Zotero\storage\QSMR5HNR\Ardö et al. - 2012 - Loop-aware Optimizations in PyPy's Tracing JIT.pdf}
}

@inproceedings{malloc-removal:11,
  title = {Allocation {{Removal}} by {{Partial Evaluation}} in a {{Tracing JIT}}},
  booktitle = {Proceedings of the 20th {{ACM SIGPLAN Workshop}} on {{Partial Evaluation}} and {{Program Manipulation}}},
  author = {Bolz, Carl Friedrich and Cuni, Antonio and FijaBkowski, Maciej and Leuschel, Michael and Pedroni, Samuele and Rigo, Armin},
  year = {2011},
  series = {{{PEPM}} '11},
  pages = {43--52},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/1929501.1929508},
  urldate = {2019-11-16},
  abstract = {The performance of many dynamic language implementations suffers from high allocation rates and runtime type checks. This makes dynamic languages less applicable to purely algorithmic problems, despite their growing popularity. In this paper we present a simple compiler optimization based on online partial evaluation to remove object allocations and runtime type checks in the context of a tracing JIT. We evaluate the optimization using a Python VM and find that it gives good results for all our (real-life) benchmarks.},
  isbn = {978-1-4503-0485-6},
  keywords = {optimization,partial evaluation,tracing jit},
  file = {C:\Users\caner\Zotero\storage\7GRXDUIE\Bolz et al. - 2011 - Allocation Removal by Partial Evaluation in a Trac.pdf}
}

@misc{mozblog,
  title = {You Lose More When Slow than You Gain When Fast -- {{Nicholas Nethercote}}},
  urldate = {2025-06-12},
  howpublished = {https://blog.mozilla.org/nnethercote/2011/05/31/you-lose-more-when-slow-than-you-gain-when-fast/},
  file = {C:\Users\caner\Zotero\storage\RG2D8I54\you-lose-more-when-slow-than-you-gain-when-fast.html}
}

@inproceedings{practical-partial,
  title = {Practical Partial Evaluation for High-Performance Dynamic Language Runtimes},
  booktitle = {Proceedings of the 38th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {W{\"u}rthinger, Thomas and Wimmer, Christian and Humer, Christian and W{\"o}{\ss}, Andreas and Stadler, Lukas and Seaton, Chris and Duboscq, Gilles and Simon, Doug and Grimmer, Matthias},
  year = {2017},
  month = jun,
  pages = {662--676},
  publisher = {ACM},
  address = {Barcelona Spain},
  doi = {10.1145/3062341.3062381},
  urldate = {2025-06-17},
  abstract = {Most high-performance dynamic language virtual machines duplicate language semantics in the interpreter, compiler, and runtime system. This violates the principle to not repeat yourself. In contrast, we define languages solely by writing an interpreter. The interpreter performs specializations, e.g., augments the interpreted program with type information and profiling information. Compiled code is derived automatically using partial evaluation while incorporating these specializations. This makes partial evaluation practical in the context of dynamic languages: It reduces the size of the compiled code while still compiling all parts of an operation that are relevant for a particular program. When a speculation fails, execution transfers back to the interpreter, the program re-specializes in the interpreter, and later partial evaluation again transforms the new state of the interpreter to compiled code. We evaluate our approach by comparing our implementations of JavaScript, Ruby, and R with best-inclass specialized production implementations. Our generalpurpose compilation system is competitive with production systems even when they have been heavily optimized for the one language they support. For our set of benchmarks, our speedup relative to the V8 JavaScript VM is 0.83x, relative to JRuby is 3.8x, and relative to GNU R is 5x.},
  isbn = {978-1-4503-4988-8},
  langid = {english},
  file = {C:\Users\caner\Zotero\storage\8V8IYVQT\Würthinger et al. - 2017 - Practical partial evaluation for high-performance dynamic language runtimes.pdf}
}

@inproceedings{pycketmain,
  title = {Pycket: {{A Tracing JIT}} for a {{Functional Language}}},
  shorttitle = {Pycket},
  booktitle = {Proceedings of the 20th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Bauman, Spenser and Bolz, Carl Friedrich and Hirschfeld, Robert and Kirilichev, Vasily and Pape, Tobias and Siek, Jeremy G. and {Tobin-Hochstadt}, Sam},
  year = {2015},
  series = {{{ICFP}} 2015},
  pages = {22--34},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/2784731.2784740},
  urldate = {2019-03-22},
  abstract = {We present Pycket, a high-performance tracing JIT compiler for Racket. Pycket supports a wide variety of the sophisticated features in Racket such as contracts, continuations, classes, structures, dynamic binding, and more. On average, over a standard suite of benchmarks, Pycket outperforms existing compilers, both Racket's JIT and other highly-optimizing Scheme compilers. Further, Pycket provides much better performance for Racket proxies than existing systems, dramatically reducing the overhead of contracts and gradual typing. We validate this claim with performance evaluation on multiple existing benchmark suites. The Pycket implementation is of independent interest as an application of the RPython meta-tracing framework (originally created for PyPy), which automatically generates tracing JIT compilers from interpreters. Prior work on meta-tracing focuses on bytecode interpreters, whereas Pycket is a high-level interpreter based on the CEK abstract machine and operates directly on abstract syntax trees. Pycket supports proper tail calls and first-class continuations. In the setting of a functional language, where recursion and higher-order functions are more prevalent than explicit loops, the most significant performance challenge for a tracing JIT is identifying which control flows constitute a loop---we discuss two strategies for identifying loops and measure their impact.},
  isbn = {978-1-4503-3669-7},
  keywords = {contracts,functional languages,JIT compilers,Racket,tracing},
  file = {C:\Users\caner\Zotero\storage\2JP3BUTY\Bauman et al. - 2015 - Pycket A Tracing JIT for a Functional Language.pdf}
}

@article{pycketmain2,
  title = {Sound Gradual Typing: Only Mostly Dead},
  shorttitle = {Sound Gradual Typing},
  author = {Bauman, Spenser and {Bolz-Tereick}, Carl Friedrich and Siek, Jeremy and {Tobin-Hochstadt}, Sam},
  year = {2017},
  month = oct,
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {1},
  number = {OOPSLA},
  pages = {1--24},
  issn = {24751421},
  doi = {10.1145/3133878},
  urldate = {2019-11-10},
  langid = {english},
  file = {C:\Users\caner\Zotero\storage\DSSSEG9Z\Bauman et al. - 2017 - Sound gradual typing only mostly dead.pdf}
}

@inproceedings{pypy-main,
  title = {Tracing the {{Meta-level}}: {{PyPy}}'s {{Tracing JIT Compiler}}},
  shorttitle = {Tracing the {{Meta-level}}},
  booktitle = {Proceedings of the 4th {{Workshop}} on the {{Implementation}}, {{Compilation}}, {{Optimization}} of {{Object-Oriented Languages}} and {{Programming Systems}}},
  author = {Bolz, Carl Friedrich and Cuni, Antonio and Fijalkowski, Maciej and Rigo, Armin},
  year = {2009},
  series = {{{ICOOOLPS}} '09},
  pages = {18--25},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/1565824.1565827},
  urldate = {2019-03-22},
  abstract = {We attempt to apply the technique of Tracing JIT Compilers in the context of the PyPy project, i.e., to programs that are interpreters for some dynamic languages, including Python. Tracing JIT compilers can greatly speed up programs that spend most of their time in loops in which they take similar code paths. However, applying an unmodified tracing JIT to a program that is itself a bytecode interpreter results in very limited or no speedup. In this paper we show how to guide tracing JIT compilers to greatly improve the speed of bytecode interpreters. One crucial point is to unroll the bytecode dispatch loop, based on two kinds of hints provided by the implementer of the bytecode interpreter. We evaluate our technique by applying it to two PyPy interpreters: one is a small example, and the other one is the full Python interpreter.},
  isbn = {978-1-60558-541-3},
  file = {C:\Users\caner\Zotero\storage\ZW64A5W8\Bolz et al. - 2009 - Tracing the Meta-level PyPy's Tracing JIT Compile.pdf}
}

@inproceedings{pypy06,
  title = {{{PyPy}}'s {{Approach}} to {{Virtual Machine Construction}}},
  booktitle = {Companion to the 21st {{ACM SIGPLAN Symposium}} on {{Object-oriented Programming Systems}}, {{Languages}}, and {{Applications}}},
  author = {Rigo, Armin and Pedroni, Samuele},
  year = {2006},
  series = {{{OOPSLA}} '06},
  pages = {944--953},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/1176617.1176753},
  urldate = {2019-11-12},
  abstract = {The PyPy project seeks to prove both on a research and a practical level the feasibility of constructing a virtual machine (VM) for a dynamic language in a dynamic language - in this case, Python. The aim is to translate (i.e. compile) the VM to arbitrary target environments, ranging in level from C/Posix to Smalltalk/Squeak via Java and CLI/.NET, while still being of reasonable efficiency within these environments.A key tool to achieve this goal is the systematic reuse of the Python language as a system programming language at various levels of our architecture and translation process. For each level, we design a corresponding type system and apply a generic type inference engine - for example, the garbage collector is written in a style that manipulates simulated pointer and address objects, and when translated to C these operations become C-level pointer and address instructions.},
  isbn = {978-1-59593-491-8},
  keywords = {metacircularity,Python,retargettable code generation,type inference,virtual machine},
  file = {C:\Users\caner\Zotero\storage\ALGTU6SG\Rigo and Pedroni - 2006 - PyPy's Approach to Virtual Machine Construction.pdf}
}

@misc{pypy08,
  title = {The {{Architecture}} of {{Open Source Applications}} ({{Volume}} 2): {{PyPy}}},
  urldate = {2019-11-12},
  howpublished = {https://www.aosabook.org/en/pypy.html},
  file = {C:\Users\caner\Zotero\storage\B2QBSUWA\pypy.html}
}

@article{racket:create-langs,
  title = {Creating Languages in {{Racket}}},
  author = {Flatt, Matthew},
  year = {2012},
  month = jan,
  journal = {Commun. ACM},
  volume = {55},
  number = {1},
  pages = {48--56},
  issn = {0001-0782},
  doi = {10.1145/2063176.2063195},
  urldate = {2025-07-08},
  abstract = {Sometimes you just have to make a better mousetrap.},
  file = {C:\Users\caner\Zotero\storage\ND6ZFPJ2\Flatt - 2012 - Creating languages in Racket.pdf}
}

@article{randomizedTesting,
  title = {Random Testing for Higher-Order, Stateful Programs},
  author = {Klein, Casey and Flatt, Matthew and Findler, Robert Bruce},
  year = {2010},
  month = oct,
  journal = {SIGPLAN Not.},
  volume = {45},
  number = {10},
  pages = {555--566},
  issn = {0362-1340},
  doi = {10.1145/1932682.1869505},
  urldate = {2025-06-28},
  abstract = {Testing is among the most effective tools available for finding bugs. Still, we know of no automatic technique for generating test cases that expose bugs involving a combination of mutable state and callbacks, even though objects and method overriding set up exactly that combination. For such cases, a test generator must create callbacks or subclasses that aggressively exercise side-effecting operations using combinations of generated objects.This paper presents a new algorithm for randomly testing programs that use state and callbacks. Our algorithm exploits a combination of contracts and environment bindings to guide the test-case generator toward interesting inputs. Our prototype implementation for Racket (formerly PLT Scheme) - which has a Java-like class system, but with first-class classes as well as gbeta-like augmentable methods - uncovered dozens of bugs in a well-tested and widely used text-editor library.We describe our approach in a precise, formal notation, borrowing the techniques used to describe operational semantics and type systems. The formalism enables us to provide a compact and self-contained explanation of the core of our technique without the ambiguity usually present in pseudo-code descriptions.},
  file = {C:\Users\caner\Zotero\storage\Q29ZZ87P\Klein et al. - 2010 - Random testing for higher-order, stateful programs.pdf}
}

@book{redexBook,
  title = {Semantics {{Engineering}} with {{PLT Redex}}},
  author = {Felleisen, Matthias and Findler, Robert Bruce and Flatt, Matthew},
  year = {2009},
  month = jul,
  edition = {1st},
  publisher = {The MIT Press},
  abstract = {This text is the first comprehensive presentation of reduction semantics in one volume; it also introduces the first reliable and easy-to-use tool set for such forms of semantics. Software engineers have long known that automatic tool support is critical for rapid prototyping and modeling, and this book is addressed to the working semantics engineer (graduate student or professional language designer). The book comes with a prototyping tool suite to develop, explore, test, debug, and publish semantic models of programming languages. With PLT Redex, semanticists can formulate models as grammars and reduction models on their computers with the ease of paper and pencil. The text first presents a framework for the formulation of language models, focusing on equational calculi and abstract machines, then introduces PLT Redex, a suite of software tools for expressing these models as PLT Redex models. Finally, experts describe a range of models formulated in Redex. PLT Redex comes with the PLT Scheme implementation, available free at http://www.plt-scheme.org/. Readers can download the software and experiment with Redex as they work their way through the book. For more information (including the preface, a sample syllabus, and a quick introduction to Redex), see the Redex website at http://redex.plt-scheme.org/. Matthias Felleisen, Robert Bruce Findler, and Matthew Flatt are the authors (with Shiram Krishnamurthi) of How to Design Programs: An Introduction to Programming and Computing, also published by the MIT Press.},
  isbn = {978-0-262-06275-6}
}

@inproceedings{rpython07,
  title = {{{RPython}}: {{A Step Towards Reconciling Dynamically}} and {{Statically Typed OO Languages}}},
  shorttitle = {{{RPython}}},
  booktitle = {Proceedings of the 2007 {{Symposium}} on {{Dynamic Languages}}},
  author = {Ancona, Davide and Ancona, Massimo and Cuni, Antonio and Matsakis, Nicholas D.},
  year = {2007},
  series = {{{DLS}} '07},
  pages = {53--64},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/1297081.1297091},
  urldate = {2019-11-12},
  abstract = {Although the C-based interpreter of Python is reasonably fast, implementations on the CLI or the JVM platforms offers some advantages in terms of robustness and interoperability. Unfortunately, because the CLI and JVM are primarily designed to execute statically typed, object-oriented languages, most dynamic language implementations cannot use the native bytecodes for common operations like method calls and exception handling; as a result, they are not able to take full advantage of the power offered by the CLI and JVM. We describe a different approach that attempts to preserve the flexibility of Python, while still allowing for efficient execution. This is achieved by limiting the use of the more dynamic features of Python to an initial, bootstrapping phase. This phase is used to construct a final RPython (Restricted Python) program that is actually executed. RPython is a proper subset of Python, is statically typed, and does not allow dynamic modification of class or method definitions; however, it can still take advantage of Python features such as mixins and first-class methods and classes. This paper presents an overview of RPython, including its design and its translation to both CLI and JVM bytecode. We show how the bootstrapping phase can be used to implement advanced features, like extensible classes and generative programming. We also discuss what work remains before RPython is truly ready for general use, and compare the performance of RPython with that of other approaches.},
  isbn = {978-1-59593-868-8},
  keywords = {.NET,JVM,Python},
  file = {C:\Users\caner\Zotero\storage\MVZCMYBI\Ancona et al. - 2007 - RPython A Step Towards Reconciling Dynamically an.pdf}
}

@inproceedings{rpython09,
  title = {{{PyGirl}}: {{Generating Whole-System VMs}} from High-Level Models Using {{PyPy}}},
  shorttitle = {{{PyGirl}}},
  author = {Bruni, Camillo and Verwaest, Toon},
  year = {2009},
  month = jun,
  volume = {33},
  pages = {328--347},
  doi = {10.1007/978-3-642-02571-6_19},
  abstract = {Virtual machines (VMs) emulating hardware devices are generally implemented in low-level languages for performance reasons. This results in unmaintainable systems that are difficult to understand. In this paper we report on our experience using the PyPy toolchain to improve the portability and reduce the complexity of whole-system VM implementations. As a case study we implement a VM prototype for a Nintendo Game Boy, called PyGirl, in which the high-level model is separated from low-level VM implementation issues. We shed light on the process of refactoring from a low-level VM implementation in Java to a high-level model in RPython. We show that our whole-system VM written with PyPy is significantly less complex than standard implementations, without substantial loss in performance.},
  file = {C:\Users\caner\Zotero\storage\ESEZHPD5\Bruni and Verwaest - 2009 - PyGirl Generating Whole-System VMs from high-leve.pdf}
}

@article{samth:11,
  title = {Languages as Libraries},
  author = {{Tobin-Hochstadt}, Sam and {St-Amour}, Vincent and Culpepper, Ryan and Flatt, Matthew and Felleisen, Matthias},
  pages = {10},
  abstract = {Programming language design benefits from constructs for extending the syntax and semantics of a host language. While C's stringbased macros empower programmers to introduce notational shorthands, the parser-level macros of Lisp encourage experimentation with domain-specific languages. The Scheme programming language improves on Lisp with macros that respect lexical scope.},
  langid = {english},
  file = {C:\Users\caner\Zotero\storage\Y47HQ2G4\Tobin-Hochstadt et al. - Languages as libraries.pdf}
}

@inproceedings{schneiderEfficientHandlingGuards2012,
  title = {The Efficient Handling of Guards in the Design of {{RPython}}'s Tracing {{JIT}}},
  booktitle = {Proceedings of the Sixth {{ACM}} Workshop on {{Virtual}} Machines and Intermediate Languages},
  author = {Schneider, David and Bolz, Carl Friedrich},
  year = {2012},
  month = oct,
  pages = {3--12},
  publisher = {ACM},
  address = {Tucson Arizona USA},
  doi = {10.1145/2414740.2414743},
  urldate = {2025-06-20},
  isbn = {978-1-4503-1633-0},
  langid = {english},
  file = {C:\Users\caner\Zotero\storage\EWXZMD8Y\Schneider and Bolz - 2012 - The efficient handling of guards in the design of RPython's tracing JIT.pdf}
}

@inproceedings{schneiderEfficientHandlingGuards2012a,
  title = {The Efficient Handling of Guards in the Design of {{RPython}}'s Tracing {{JIT}}},
  booktitle = {Proceedings of the Sixth {{ACM}} Workshop on {{Virtual}} Machines and Intermediate Languages},
  author = {Schneider, David and Bolz, Carl Friedrich},
  year = {2012},
  month = oct,
  series = {{{VMIL}} '12},
  pages = {3--12},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2414740.2414743},
  urldate = {2025-06-20},
  abstract = {Tracing just-in-time (JIT) compilers record linear control flow paths, inserting operations called guards at points of possible divergence. These operations occur frequently in generated traces and therefore it is important to design and implement them carefully to find the right trade-off between deoptimization, memory overhead, and (partly) execution speed. In this paper, we perform an empirical analysis of runtime properties of guards. This is used to guide the design of guards in the RPython tracing JIT.},
  isbn = {978-1-4503-1633-0},
  file = {C:\Users\caner\Zotero\storage\2PWWZY7I\Schneider and Bolz - 2012 - The efficient handling of guards in the design of RPython's tracing JIT.pdf}
}

@inproceedings{squeak_smalltalk_vm,
  title = {Back to the Future: The Story of {{Squeak}}, a Practical {{Smalltalk}} Written in Itself},
  shorttitle = {Back to the Future},
  booktitle = {Proceedings of the 12th {{ACM SIGPLAN}} Conference on {{Object-oriented}} Programming, Systems, Languages, and Applications},
  author = {Ingalls, Dan and Kaehler, Ted and Maloney, John and Wallace, Scott and Kay, Alan},
  year = {1997},
  month = oct,
  series = {{{OOPSLA}} '97},
  pages = {318--326},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/263698.263754},
  urldate = {2025-07-03},
  abstract = {Squeak is an open, highly-portable Smalltalk implementation whose virtual machine is written entirely in Smalltalk, making it easy to. debug, analyze, and change. To achieve practical performance, a translator produces an equivalent C program whose performance is comparable to commercial Smalltalks.Other noteworthy aspects of Squeak include: a compact object format that typically requires only a single word of overhead per object; a simple yet efficient incremental garbage collector for 32-bit direct pointers; efficient bulk-mutation of objects; extensions of BitBlt to handle color of any depth and anti-aliased image rotation and scaling; and real-time sound and music synthesis written entirely in Smalltalk.},
  isbn = {978-0-89791-908-1},
  file = {C:\Users\caner\Zotero\storage\EL25R8Z5\Ingalls et al. - 1997 - Back to the future the story of Squeak, a practical Smalltalk written in itself.pdf}
}

@article{survey:05,
  title = {A {{Survey}} of {{Adaptive Optimization}} in {{Virtual Machines}}},
  author = {Arnold, M. and Fink, S. J. and Grove, D. and Hind, M. and Sweeney, P. F.},
  year = {2005},
  month = feb,
  journal = {Proceedings of the IEEE},
  volume = {93},
  number = {2},
  pages = {449--466},
  issn = {0018-9219},
  doi = {10.1109/JPROC.2004.840305},
  abstract = {Virtual machines face significant performance challenges beyond those confronted by traditional static optimizers. First, portable program representations and dynamic language features, such as dynamic class loading, force the deferral of most optimizations until runtime, inducing runtime optimization overhead. Second, modular program representations preclude many forms of whole-program interprocedural optimization. Third, virtual machines incur additional costs for runtime services such as security guarantees and automatic memory management. To address these challenges, vendors have invested considerable resources into adaptive optimization systems in production virtual machines. Today, mainstream virtual machine implementations include substantial infrastructure for online monitoring and profiling, runtime compilation, and feedback-directed optimization. As a result, adaptive optimization has begun to mature as a widespread production-level technology. This paper surveys the evolution and current state of adaptive optimization technology in virtual machines.},
  keywords = {Adaptive optimization,adaptive optimization systems,Adaptive systems,automatic memory management,Condition monitoring,Costs,dynamic optimization,feedback directed optimization,feedback-directed optimization (FDO),Memory management,modular program representations,online monitoring,online profiling,optimisation,optimising compilers,optimization,Optimized production technology,production level technology,Production systems,Runtime,runtime compilation,Security,software performance evaluation,static optimizers,Virtual machine monitors,virtual machines,Virtual machining},
  file = {C\:\\Users\\caner\\Zotero\\storage\\DVWJXGSE\\Arnold et al. - 2005 - A Survey of Adaptive Optimization in Virtual Machi.pdf;C\:\\Users\\caner\\Zotero\\storage\\I5PPYRYM\\1386662.html}
}

@inproceedings{trace-vs-PE,
  title = {Tracing vs. Partial Evaluation: Comparing Meta-Compilation Approaches for Self-Optimizing Interpreters},
  shorttitle = {Tracing vs. Partial Evaluation},
  booktitle = {Proceedings of the 2015 {{ACM SIGPLAN International Conference}} on {{Object-Oriented Programming}}, {{Systems}}, {{Languages}}, and {{Applications}} - {{OOPSLA}} 2015},
  author = {Marr, Stefan and Ducasse, St{\'e}phane},
  year = {2015},
  pages = {821--839},
  publisher = {ACM Press},
  address = {Pittsburgh, PA, USA},
  doi = {10.1145/2814270.2814275},
  urldate = {2019-03-28},
  isbn = {978-1-4503-3689-5},
  langid = {english},
  file = {C:\Users\caner\Zotero\storage\U6CTI5AY\Marr and Ducasse - 2015 - Tracing vs. partial evaluation comparing meta-com.pdf}
}

@inproceedings{traceMonkey,
  title = {Trace-Based {{Just-in-time Type Specialization}} for {{Dynamic Languages}}},
  booktitle = {Proceedings of the 30th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Gal, Andreas and Eich, Brendan and Shaver, Mike and Anderson, David and Mandelin, David and Haghighat, Mohammad R. and Kaplan, Blake and Hoare, Graydon and Zbarsky, Boris and Orendorff, Jason and Ruderman, Jesse and Smith, Edwin W. and Reitmaier, Rick and Bebenita, Michael and Chang, Mason and Franz, Michael},
  year = {2009},
  series = {{{PLDI}} '09},
  pages = {465--478},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/1542476.1542528},
  urldate = {2019-11-28},
  abstract = {Dynamic languages such as JavaScript are more difficult to compile than statically typed ones. Since no concrete type information is available, traditional compilers need to emit generic code that can handle all possible type combinations at runtime. We present an alternative compilation technique for dynamically-typed languages that identifies frequently executed loop traces at run-time and then generates machine code on the fly that is specialized for the actual dynamic types occurring on each path through the loop. Our method provides cheap inter-procedural type specialization, and an elegant and efficient way of incrementally compiling lazily discovered alternative paths through nested loops. We have implemented a dynamic compiler for JavaScript based on our technique and we have measured speedups of 10x and more for certain benchmark programs.},
  isbn = {978-1-60558-392-1},
  keywords = {dynamically typed languages,trace-based compilation},
  file = {C:\Users\caner\Zotero\storage\FWYDACR5\Gal et al. - 2009 - Trace-based Just-in-time Type Specialization for D.pdf}
}

@inproceedings{truffle-graal,
  title = {One {{VM}} to Rule Them All},
  booktitle = {Proceedings of the 2013 {{ACM}} International Symposium on {{New}} Ideas, New Paradigms, and Reflections on Programming \& Software - {{Onward}}! '13},
  author = {W{\"u}rthinger, Thomas and Wimmer, Christian and W{\"o}{\ss}, Andreas and Stadler, Lukas and Duboscq, Gilles and Humer, Christian and Richards, Gregor and Simon, Doug and Wolczko, Mario},
  year = {2013},
  pages = {187--204},
  publisher = {ACM Press},
  address = {Indianapolis, Indiana, USA},
  doi = {10.1145/2509578.2509581},
  urldate = {2019-11-25},
  abstract = {Building high-performance virtual machines is a complex and expensive undertaking; many popular languages still have low-performance implementations. We describe a new approach to virtual machine (VM) construction that amortizes much of the effort in initial construction by allowing new languages to be implemented with modest additional effort. The approach relies on abstract syntax tree (AST) interpretation where a node can rewrite itself to a more specialized or more general node, together with an optimizing compiler that exploits the structure of the interpreter. The compiler uses speculative assumptions and deoptimization in order to produce efficient machine code. Our initial experience suggests that high performance is attainable while preserving a modular and layered architecture, and that new highperformance language implementations can be obtained by writing little more than a stylized interpreter.},
  isbn = {978-1-4503-2472-4},
  langid = {english},
  file = {C:\Users\caner\Zotero\storage\7JJNARJ3\Würthinger et al. - 2013 - One VM to rule them all.pdf}
}

@article{typeSpecial:2009,
  title = {Trace-Based Just-in-Time Type Specialization for Dynamic Languages},
  author = {Gal, Andreas and Eich, Brendan and Shaver, Mike and Anderson, David and Mandelin, David and Haghighat, Mohammad R. and Kaplan, Blake and Hoare, Graydon and Zbarsky, Boris and Orendorff, Jason and Ruderman, Jesse and Smith, Edwin W. and Reitmaier, Rick and Bebenita, Michael and Chang, Mason and Franz, Michael},
  year = {2009},
  month = jun,
  journal = {SIGPLAN Not.},
  volume = {44},
  number = {6},
  pages = {465--478},
  issn = {0362-1340},
  doi = {10.1145/1543135.1542528},
  urldate = {2025-06-23},
  abstract = {Dynamic languages such as JavaScript are more difficult to compile than statically typed ones. Since no concrete type information is available, traditional compilers need to emit generic code that can handle all possible type combinations at runtime. We present an alternative compilation technique for dynamically-typed languages that identifies frequently executed loop traces at run-time and then generates machine code on the fly that is specialized for the actual dynamic types occurring on each path through the loop. Our method provides cheap inter-procedural type specialization, and an elegant and efficient way of incrementally compiling lazily discovered alternative paths through nested loops. We have implemented a dynamic compiler for JavaScript based on our technique and we have measured speedups of 10x and more for certain benchmark programs.},
  file = {C:\Users\caner\Zotero\storage\T59L5B5Z\Gal et al. - 2009 - Trace-based just-in-time type specialization for dynamic languages.pdf}
}

@article{vandercammenEssenceMetaTracingJIT,
  title = {The {{Essence}} of {{Meta-Tracing JIT Compilers}}},
  author = {Vandercammen, Maarten},
  langid = {english},
  file = {C:\Users\caner\Zotero\storage\9Q858YLX\Vandercammen - The Essence of Meta-Tracing JIT Compilers.pdf}
}
